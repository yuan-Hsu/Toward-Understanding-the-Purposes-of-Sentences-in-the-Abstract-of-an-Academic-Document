Id,Abstract,Task 1
D01106,"We introduce ""Unspeech"" embeddings, which are based on unsupervised learning of context feature representations for spoken language.$$$The embeddings were trained on up to 9500 hours of crawled English speech data without transcriptions or speaker information, by using a straightforward learning objective based on context and non-context discrimination with negative sampling.$$$We use a Siamese convolutional neural network architecture to train Unspeech embeddings and evaluate them on speaker comparison, utterance clustering and as a context feature in TDNN-HMM acoustic models trained on TED-LIUM, comparing it to i-vector baselines.$$$Particularly decoding out-of-domain speech data from the recently released Common Voice corpus shows consistent WER reductions.$$$We release our source code and pre-trained Unspeech models under a permissive open source license.",OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS OTHERS
D05520,"Shifting to a lexicalized grammar reduces the number of parsing errors and improves application results.$$$However, such an operation affects a syntactic parser in all its aspects.$$$One of our research objectives is to design a realistic model for grammar lexicalization.$$$We carried out experiments for which we used a grammar with a very simple content and formalism, and a very informative syntactic lexicon, the lexicon-grammar of French elaborated by the LADL.$$$Lexicalization was performed by applying the parameterized-graph approach.$$$Our results tend to show that most information in the lexicon-grammar can be transferred into a grammar and exploited successfully for the syntactic parsing of sentences.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D02397,"Segmentation of retinal vessels from retinal fundus images is the key step in the automatic retinal image analysis.$$$In this paper, we propose a new unsupervised automatic method to segment the retinal vessels from retinal fundus images.$$$Contrast enhancement and illumination correction are carried out through a series of image processing steps followed by adaptive histogram equalization and anisotropic diffusion filtering.$$$This image is then converted to a gray scale using weighted scaling.$$$The vessel edges are enhanced by boosting the detail curvelet coefficients.$$$Optic disk pixels are removed before applying fuzzy C-mean classification to avoid the misclassification.$$$Morphological operations and connected component analysis are applied to obtain the segmented retinal vessels.$$$The performance of the proposed method is evaluated using DRIVE database to be able to compare with other state-of-art supervised and unsupervised methods.$$$The overall segmentation accuracy of the proposed method is 95.18% which outperforms the other algorithms.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D01945,"Mechanical learning is a computing system that is based on a set of simple and fixed rules, and can learn from incoming data.$$$A learning machine is a system that realizes mechanical learning.$$$Importantly, we emphasis that it is based on a set of simple and fixed rules, contrasting to often called machine learning that is sophisticated software based on very complicated mathematical theory, and often needs human intervene for software fine tune and manual adjustments.$$$Here, we discuss some basic facts and principles of such system, and try to lay down a framework for further study.$$$We propose 2 directions to approach mechanical learning, just like Church-Turing pair: one is trying to realize a learning machine, another is trying to well describe the mechanical learning.",BACKGROUND OBJECTIVES/METHODS/RESULTS BACKGROUND/OBJECTIVES/METHODS OBJECTIVES/CONCLUSIONS OBJECTIVES/METHODS/RESULTS/CONCLUSIONS
D04292,"Recently, a chaotic image encryption algorithm based on perceptron model was proposed.$$$The present paper analyzes security of the algorithm and finds that the equivalent secret key can be reconstructed with only one pair of known-plaintext/ciphertext, which is supported by both mathematical proof and experiment results.$$$In addition, some other security defects are also reported.",BACKGROUND OBJECTIVES OTHERS
D05671,"This paper explores the problem of sockpuppet detection in deceptive opinion spam using authorship attribution and verification approaches.$$$Two methods are explored.$$$The first is a feature subsampling scheme that uses the KL-Divergence on stylistic language models of an author to find discriminative features.$$$The second is a transduction scheme, spy induction that leverages the diversity of authors in the unlabeled test set by sending a set of spies (positive samples) from the training set to retrieve hidden samples in the unlabeled test set using nearest and farthest neighbors.$$$Experiments using ground truth sockpuppet data show the effectiveness of the proposed schemes.",OBJECTIVES OBJECTIVES METHODS METHODS CONCLUSIONS
D05744,"Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the ""curse of dimensionality"".$$$This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs.$$$To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function.$$$Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost.$$$This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.",BACKGROUND OTHERS METHODS RESULTS CONCLUSIONS
D06484,"Automatic melody generation has been a long-time aspiration for both AI researchers and musicians.$$$However, learning to generate euphonious melodies has turned out to be highly challenging.$$$This paper introduces 1) a new variant of variational autoencoder (VAE), where the model structure is designed in a modularized manner in order to model polyphonic and dynamic music with domain knowledge, and 2) a hierarchical encoding/decoding strategy, which explicitly models the dependency between melodic features.$$$The proposed framework is capable of generating distinct melodies that sounds natural, and the experiments for evaluating generated music clips show that the proposed model outperforms the baselines in human evaluation.",BACKGROUND BACKGROUND/OBJECTIVES METHODS RESULTS/CONCLUSIONS
D04719,"Mobile automated video surveillance system involves application of real-time image and video processing algorithms which require a vast quantity of computing and storage resources.$$$To support the execution of mobile automated video surveillance system, a mobile ad hoc cloud computing and networking infrastructure is proposed in which multiple mobile devices interconnected through a mobile ad hoc network are combined to create a virtual supercomputing node.$$$An energy efficient resource allocation scheme has also been proposed for allocation of realtime automated video surveillance tasks.$$$To enable communication between mobile devices, a Wi-Fi Direct based mobile ad hoc cloud networking infrastructure has been developed.$$$More specifically, a routing layer has been developed to support communication between Wi-Fi Direct devices in a group and multi-hop communication between devices across the group.$$$The proposed system has been implemented on a group of Wi-Fi Direct-enabled Samsung mobile devices.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS
D02695,"Recent developments in the field of Networking have provided opportunities for networks to efficiently cater application specific needs of a user.$$$In this context, a routing path is not only dependent upon the network states but also is calculated in the best interest of an application using the network.$$$These advanced routing algorithms can exploit application state data to enhance advanced network services such as anycast, edge cloud computing and cyber physical systems (CPS).$$$In this work, we aim to design such a routing algorithm where the router decisions are based upon convex optimization techniques.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS
D02703,"In most classification tasks there are observations that are ambiguous and therefore difficult to correctly label.$$$Set-valued classifiers output sets of plausible labels rather than a single label, thereby giving a more appropriate and informative treatment to the labeling of ambiguous instances.$$$We introduce a framework for multiclass set-valued classification, where the classifiers guarantee user-defined levels of coverage or confidence (the probability that the true label is contained in the set) while minimizing the ambiguity (the expected size of the output).$$$We first derive oracle classifiers assuming the true distribution to be known.$$$We show that the oracle classifiers are obtained from level sets of the functions that define the conditional probability of each class.$$$Then we develop estimators with good asymptotic and finite sample properties.$$$The proposed estimators build on existing single-label classifiers.$$$The optimal classifier can sometimes output the empty set, but we provide two solutions to fix this issue that are suitable for various practical needs.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS RESULTS
D00143,"Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets.$$$In this paper, we offer contributions in both these areas to enable similar progress in audio modeling.$$$First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform.$$$Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets.$$$Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline.$$$Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05310,This paper explains genetic algorithm for novice in this field.$$$Basic philosophy of genetic algorithm and its flowchart are described.$$$Step by step numerical computation of genetic algorithm for solving simple mathematical equality problem will be briefly explained,BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS/RESULTS/CONCLUSIONS
D02410,"The densification and expansion of wireless network pose new challenges on interference management and reducing energy consumption.$$$This paper studies energy-efficient resource management in heterogeneous networks by jointly optimizing cell activation, user association and multicell multiuser channel assignment, according to the long-term average traffic and channel conditions.$$$The proposed framework is built on characterizing the interference coupling by pre-defined interference patterns, and performing resource allocation among these patterns.$$$In this way, the interference fluctuation caused by (de)activating cells is explicitly taken into account when calculating the user achievable rates.$$$A tailored algorithm is developed to solve the formulated problem in the dual domain by exploiting the problem structure, which gives a significant complexity saving.$$$Numerical results show a huge improvement in energy saving achieved by the proposed scheme.$$$The user association derived from the proposed joint resource optimization is mapped to standard-compliant cell selection biasing.$$$This mapping reveals that the cell-specific biasing for energy saving is quite different from that for load balancing investigated in the literature.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS METHODS RESULTS
D00986,"In this paper we discuss some reasons why temporal logic might not be suitable to model real life norms.$$$To show this, we present a novel deontic logic contrary-to-duty/derived permission paradox based on the interaction of obligations, permissions and contrary-to-duty obligations.$$$The paradox is inspired by real life norms.",OBJECTIVES/RESULTS/CONCLUSIONS METHODS/RESULTS OTHERS
D03456,"This paper presents a simple, robust and (almost) unsupervised dictionary-based method, qwn-ppv (Q-WordNet as Personalized PageRanking Vector) to automatically generate polarity lexicons.$$$We show that qwn-ppv outperforms other automatically generated lexicons for the four extrinsic evaluations presented here.$$$It also shows very competitive and robust results with respect to manually annotated ones.$$$Results suggest that no single lexicon is best for every task and dataset and that the intrinsic evaluation of polarity lexicons is not a good performance indicator on a Sentiment Analysis task.$$$The qwn-ppv method allows to easily create quality polarity lexicons whenever no domain-based annotated corpora are available for a given language.",OBJECTIVES/METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D04426,"We present some of the experiments we have performed to best test our design for a library for MathScheme, the mechanized mathematics software system we are building.$$$We wish for our library design to use and reflect, as much as possible, the mathematical structure present in the objects which populate the library.",OBJECTIVES OBJECTIVES
D04472,"The vision and requirements of the sixth generation (6G) mobile communication systems are expected to adopt free-space optical communication (FSO) and wireless power transfer (WPT).$$$The laser-based WPT or wireless information transfer (WIT) usually faces the challenges of mobility and safety.$$$We present here a mobile and safe resonant beam communication (RBCom) system, which can realize high-rate simultaneous wireless information and power transfer (SWIPT).$$$We propose the analytical model to depict its SWIPT procedure.$$$The numerical results show that RBCom can achieve 7.5 Gbit/s with 200 mW received optical power, which seems to connect the transmitter and the receiver with a mobile ""wireless optical fiber"".",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS
D06788,"A number of differences have emerged between modern and classic approaches to constituency parsing in recent years, with structural components like grammars and feature-rich lexicons becoming less central while recurrent neural network representations rise in popularity.$$$The goal of this work is to analyze the extent to which information provided directly by the model structure in classical systems is still being captured by neural methods.$$$To this end, we propose a high-performance neural model (92.08 F1 on PTB) that is representative of recent work and perform a series of investigative experiments.$$$We find that our model implicitly learns to encode much of the same information that was explicitly provided by grammars and lexicons in the past, indicating that this scaffolding can largely be subsumed by powerful general-purpose neural machinery.",BACKGROUND OBJECTIVES METHODS CONCLUSIONS
D01422,Univariate fractions can be transformed to mixed fractions in the equational theory of meadows of characteristic zero.,CONCLUSIONS
D06513,"In last decade, data analytics have rapidly progressed from traditional disk-based processing to modern in-memory processing.$$$However, little effort has been devoted at enhancing performance at micro-architecture level.$$$This paper characterizes the performance of in-memory data analytics using Apache Spark framework.$$$We use a single node NUMA machine and identify the bottlenecks hampering the scalability of workloads.$$$We also quantify the inefficiencies at micro-architecture level for various data analysis workloads.$$$Through empirical evaluation, we show that spark workloads do not scale linearly beyond twelve threads, due to work time inflation and thread level load imbalance.$$$Further, at the micro-architecture level, we observe memory bound latency to be the major cause of work time inflation.",BACKGROUND BACKGROUND OBJECTIVES METHODS OBJECTIVES RESULTS RESULTS/CONCLUSIONS
D05154,"We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks.$$$Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer.$$$The key observation of our approach is that the problem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM.$$$We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem.$$$This allows to design an optimization algorithm with an optimal learning rate that does not require any tuning.$$$Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.",OBJECTIVES METHODS METHODS METHODS CONCLUSIONS RESULTS
D01619,"Fault based testing is a technique in which test cases are chosen to reveal certain classes of faults.$$$At present, testing professionals use their personal experience to select testing methods for fault classes considered the most likely to be present.$$$However, there is little empirical evidence available in the open literature to support these intuitions.$$$By examining the source code changes when faults were fixed in seven open source software artifacts, we have classified bug fix patterns into fault classes, and recorded the relative frequencies of the identified fault classes.$$$This paper reports our findings related to ""if-conditional"" fixes.$$$We have classified the ""if-conditional"" fixes into fourteen fault classes and calculated their frequencies.$$$We found the most common fault class related to changes within a single ""atom"".$$$The next most common fault was the omission of an ""atom"".$$$We analysed these results in the context of Boolean specification testing.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES OBJECTIVES/METHODS OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D06432,"We propose an efficient Stereographic Projection Neural Network (SPNet) for learning representations of 3D objects.$$$We first transform a 3D input volume into a 2D planar image using stereographic projection.$$$We then present a shallow 2D convolutional neural network (CNN) to estimate the object category followed by view ensemble, which combines the responses from multiple views of the object to further enhance the predictions.$$$Specifically, the proposed approach consists of four stages: (1) Stereographic projection of a 3D object, (2) view-specific feature learning, (3) view selection and (4) view ensemble.$$$The proposed approach performs comparably to the state-of-the-art methods while having substantially lower GPU memory as well as network parameters.$$$Despite its lightness, the experiments on 3D object classification and shape retrievals demonstrate the high performance of the proposed method.",OBJECTIVES/METHODS BACKGROUND/METHODS BACKGROUND/METHODS METHODS RESULTS RESULTS
D03327,"The recently advancement in Wireless Sensor Network (WSN) technology has brought new distributed sensing applications such as water quality monitoring.$$$With sensing capabilities and using parameters like pH, conductivity and temperature, the quality of water can be known.$$$This paper proposes a novel design based on IEEE 802.15.4 (Zig-Bee protocol) and solar energy called Autonomous Water Quality Monitoring Prototype (AWQMP).$$$The prototype is designed to use ECHERP routing protocol and Adruino Mega 2560, an open-source electronic prototyping platform for data acquisition.$$$AWQMP is expected to give real time data acquirement and to reduce the cost of manual water quality monitoring due to its autonomous characteristic.$$$Moreover, the proposed prototype will help to study the behavior of aquatic animals in deployed water bodies.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS
D06672,"Inspired by Christopher Alexanders conception of the world - space is not lifeless or neutral but a living structure involving far more small things than large ones a topological representation has been previously developed to characterize the living structure or the wholeness of geographic space.$$$This paper further develops the topological representation and living structure for predicting human activities in geographic space.$$$Based on millions of street nodes of the United Kingdom extracted from OpenStreetMap, we established living structures at different levels of scale in a nested manner.$$$We found that tweet locations at different levels of scale, such as country and city, can be well predicted by the underlying living structure.$$$The high predictability demonstrates that the living structure and the topological representation are efficient and effective for better understanding geographic forms.$$$Based on this major finding, we argue that the topological representation is a truly multi-scale representation, and point out that existing geographic representations are essentially single scale, so they bear many scale problems such as modifiable areal unit problem, the conundrum of length, and the ecological fallacy.$$$We further discuss on why the living structure is an efficient and effective instrument for structuring geospatial big data, and why Alexanders organic worldview constitutes the third view of space.$$$Keywords: Organic worldview, topological representation, tweet locations, natural cities, scaling of geographic space",BACKGROUND OBJECTIVES BACKGROUND RESULTS RESULTS CONCLUSIONS CONCLUSIONS OTHERS
D03995,"External or internal domain-specific languages (DSLs) or (fluent) APIs?$$$Whoever you are -- a developer or a user of a DSL -- you usually have to choose your side; you should not!$$$What about metamorphic DSLs that change their shape according to your needs?$$$We report on our 4-years journey of providing the ""right"" support (in the domain of feature modeling), leading us to develop an external DSL, different shapes of an internal API, and maintain all these languages.$$$A key insight is that there is no one-size-fits-all solution or no clear superiority of a solution compared to another.$$$On the contrary, we found that it does make sense to continue the maintenance of an external and internal DSL.$$$The vision that we foresee for the future of software languages is their ability to be self-adaptable to the most appropriate shape (including the corresponding integrated development environment) according to a particular usage or task.$$$We call metamorphic DSL such a language, able to change from one shape to another shape.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D01098,"We introduce an exact reformulation of a broad class of neighborhood filters, among which the bilateral filters, in terms of two functional rearrangements: the decreasing and the relative rearrangements.$$$Independently of the image spatial dimension (one-dimensional signal, image, volume of images, etc.$$$), we reformulate these filters as integral operators defined in a one-dimensional space corresponding to the level sets measures.$$$We prove the equivalence between the usual pixel-based version and the rearranged version of the filter.$$$When restricted to the discrete setting, our reformulation of bilateral filters extends previous results for the so-called fast bilateral filtering.$$$We, in addition, prove that the solution of the discrete setting, understood as constant-wise interpolators, converges to the solution of the continuous setting.$$$Finally, we numerically illustrate computational aspects concerning quality approximation and execution time provided by the rearranged formulation.",BACKGROUND METHODS METHODS RESULTS BACKGROUND RESULTS RESULTS
D06126,"Learning controllers for bipedal robots is a challenging problem, often requiring expert knowledge and extensive tuning of parameters that vary in different situations.$$$Recently, deep reinforcement learning has shown promise at automatically learning controllers for complex systems in simulation.$$$This has been followed by a push towards learning controllers that can be transferred between simulation and hardware, primarily with the use of domain randomization.$$$However, domain randomization can make the problem of finding stable controllers even more challenging, especially for underactuated bipedal robots.$$$In this work, we explore whether policies learned in simulation can be transferred to hardware with the use of high-fidelity simulators and structured controllers.$$$We learn a neural network policy which is a part of a more structured controller.$$$While the neural network is learned in simulation, the rest of the controller stays fixed, and can be tuned by the expert as needed.$$$We show that using this approach can greatly speed up the rate of learning in simulation, as well as enable transfer of policies between simulation and hardware.$$$We present our results on an ATRIAS robot and explore the effect of action spaces and cost functions on the rate of transfer between simulation and hardware.$$$Our results show that structured policies can indeed be learned in simulation and implemented on hardware successfully.$$$This has several advantages, as the structure preserves the intuitive nature of the policy, and the neural network improves the performance of the hand-designed policy.$$$In this way, we propose a way of using neural networks to improve expert designed controllers, while maintaining ease of understanding.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D02700,"Compressive sensing is a method to recover the original image from undersampled measurements.$$$In order to overcome the ill-posedness of this inverse problem, image priors are used such as sparsity in the wavelet domain, minimum total-variation, or self-similarity.$$$Recently, deep learning based compressive image recovery methods have been proposed and have yielded state-of-the-art performances.$$$They used deep learning based data-driven approaches instead of hand-crafted image priors to solve the ill-posed inverse problem with undersampled data.$$$Ironically, training deep neural networks for them requires ""clean"" ground truth images, but obtaining the best quality images from undersampled data requires well-trained deep neural networks.$$$To resolve this dilemma, we propose novel methods based on two well-grounded theories: denoiser-approximate message passing and Stein's unbiased risk estimator.$$$Our proposed methods were able to train deep learning based image denoisers from undersampled measurements without ground truth images and without image priors, and to recover images with state-of-the-art qualities from undersampled data.$$$We evaluated our methods for various compressive sensing recovery problems with Gaussian random, coded diffraction pattern, and compressive sensing MRI measurement matrices.$$$Our methods yielded state-of-the-art performances for all cases without ground truth images and without image priors.$$$They also yielded comparable performances to the methods with ground truth data.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS RESULTS/CONCLUSIONS CONCLUSIONS
D00317,In this contribution to the 3rd CHiME Speech Separation and Recognition Challenge (CHiME-3) we extend the acoustic front-end of the CHiME-3 baseline speech recognition system by a coherence-based Wiener filter which is applied to the output signal of the baseline beamformer.$$$To compute the time- and frequency-dependent postfilter gains the ratio between direct and diffuse signal components at the output of the baseline beamformer is estimated and used as approximation of the short-time signal-to-noise ratio.$$$The proposed spectral enhancement technique is evaluated with respect to word error rates of the CHiME-3 challenge baseline speech recognition system using real speech recorded in public environments.$$$Results confirm the effectiveness of the coherence-based postfilter when integrated into the front-end signal enhancement.,BACKGROUND/OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D01622,"Over the years the Artificial Intelligence (AI) community has produced several datasets which have given the machine learning algorithms the opportunity to learn various skills across various domains.$$$However, a subclass of these machine learning algorithms that aimed at learning logic programs, namely the Inductive Logic Programming algorithms, have often failed at the task due to the vastness of these datasets.$$$This has impacted the usability of knowledge representation and reasoning techniques in the development of AI systems.$$$In this research, we try to address this scalability issue for the algorithms that learn answer set programs.$$$We present a sound and complete algorithm which takes the input in a slightly different manner and performs an efficient and more user controlled search for a solution.$$$We show via experiments that our algorithm can learn from two popular datasets from machine learning community, namely bAbl (a question answering dataset) and MNIST (a dataset for handwritten digit recognition), which to the best of our knowledge was not previously possible.$$$The system is publicly available at https://goo.gl/KdWAcV.$$$This paper is under consideration for acceptance in TPLP.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS/CONCLUSIONS RESULTS/CONCLUSIONS OTHERS OTHERS
D04748,"Accurate information of inertial parameters is critical to motion planning and control of space robots.$$$Before the launch, only a rudimentary estimate of the inertial parameters is available from experiments and computer-aided design (CAD) models.$$$After the launch, on-orbit operations substantially alter the value of inertial parameters.$$$In this work, we propose a new momentum model-based method for identifying the minimal parameters of a space robot while on orbit.$$$Minimal parameters are combinations of the inertial parameters of the links and uniquely define the momentum and dynamic models.$$$Consequently, they are sufficient for motion planning and control of both the satellite and robotic arms mounted on it.$$$The key to the proposed framework is the unique formulation of momentum model in the linear form of minimal parameters.$$$Further, to estimate the minimal parameters, we propose a novel joint trajectory planning and optimization technique based on direction combinations of joints' velocity.$$$The efficacy of the identification framework is demonstrated on a 12 degrees-of-freedom, spatial, dual-arm space robot.$$$The methodology is developed for tree-type space robots, requires just the pose and twist data, and scalable with increasing number of joints.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS RESULTS CONCLUSIONS
D03105,"The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost.$$$In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy.$$$This is achieved by enforcing channel-level sparsity in the network in a simple but effective way.$$$Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models.$$$We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy.$$$We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets.$$$For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D02051,"This paper claims that a new field of empirical software engineering research and practice is emerging: data mining using/used-by optimizers for empirical studies, or DUO.$$$For example, data miners can generate the models that are explored by optimizers.Also, optimizers can advise how to best adjust the control parameters of a data miner.$$$This combined approach acts like an agent leaning over the shoulder of an analyst that advises ""ask this question next"" or ""ignore that problem, it is not relevant to your goals"".$$$Further, those agents can help us build ""better"" predictive models, where ""better"" can be either greater predictive accuracy, or faster modeling time (which, in turn, enables the exploration of a wider range of options).$$$We also caution that the era of papers that just use data miners is coming to an end.$$$Results obtained from an unoptimized data miner can be quickly refuted, just by applying an optimizer to produce a different (and better performing) model.$$$Our conclusion, hence, is that for software analytics it is possible, useful and necessary to combine data mining and optimization using DUO.",OBJECTIVES OTHERS METHODS METHODS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D00976,"A pulse shape analysis framework is described, which was developed for n_TOF-Phase3, the third phase in the operation of the n_TOF facility at CERN.$$$The most notable feature of this new framework is the adoption of generic pulse shape analysis routines, characterized by a minimal number of explicit assumptions about the nature of pulses.$$$The aim of these routines is to be applicable to a wide variety of detectors, thus facilitating the introduction of the new detectors or types of detectors into the analysis framework.$$$The operational details of the routines are suited to the specific requirements of particular detectors by adjusting the set of external input parameters.$$$Pulse recognition, baseline calculation and the pulse shape fitting procedure are described.$$$Special emphasis is put on their computational efficiency, since the most basic implementations of these conceptually simple methods are often computationally inefficient.",BACKGROUND/RESULTS BACKGROUND/METHODS BACKGROUND/METHODS METHODS METHODS METHODS
D01506,"This paper presents a system based on a Two-Way Particle-Tracking Model to analyze possible crash positions of flight MH370.$$$The particle simulator includes a simple flow simulation of the debris based on a Lagrangian approach and a module to extract appropriated ocean current data from netCDF files.$$$The influence of wind, waves, immersion depth and hydrodynamic behavior are not considered in the simulation.",BACKGROUND METHODS METHODS
D01878,"In this paper, we develop a distributed intermittent communication and task planning framework for mobile robot teams.$$$The goal of the robots is to accomplish complex tasks, captured by local Linear Temporal Logic formulas, and share the collected information with all other robots and possibly also with a user.$$$Specifically, we consider situations where the robot communication capabilities are not sufficient to form reliable and connected networks while the robots move to accomplish their tasks.$$$In this case, intermittent communication protocols are necessary that allow the robots to temporarily disconnect from the network in order to accomplish their tasks free of communication constraints.$$$We assume that the robots can only communicate with each other when they meet at common locations in space.$$$Our distributed control framework jointly determines local plans that allow all robots fulfill their assigned temporal tasks, sequences of communication events that guarantee information exchange infinitely often, and optimal communication locations that minimize a desired distance metric.$$$Simulation results verify the efficacy of the proposed controllers.",OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS/RESULTS RESULTS/CONCLUSIONS
D05557,"The Internet of Things (IoT) represents a comprehensive environment that consists of a large number of smart devices interconnecting heterogeneous physical objects to the Internet.$$$Many domains such as logistics, manufacturing, agriculture, urban computing, home automation, ambient assisted living and various ubiquitous computing applications have utilised IoT technologies.$$$Meanwhile, Business Process Management Systems (BPMS) have become a successful and efficient solution for coordinated management and optimised utilisation of resources/entities.$$$However, past BPMS have not considered many issues they will face in managing large scale connected heterogeneous IoT entities.$$$Without fully understanding the behaviour, capability and state of the IoT entities, the BPMS can fail to manage the IoT integrated information systems.$$$In this paper, we analyse existing BPMS for IoT and identify the limitations and their drawbacks based on Mobile Cloud Computing perspective.$$$Later, we discuss a number of open challenges in BPMS for IoT.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES
D06691,"In this paper, an online adaptation algorithm for bipedal walking on uneven surfaces with height uncertainty is proposed.$$$In order to generate walking patterns on flat terrains, the trajectories in the task space are planned to satisfy the dynamic balance and slippage avoidance constraints, and also to guarantee smooth landing of the swing foot.$$$To ensure smooth landing of the swing foot on surfaces with height uncertainty, the preplanned trajectories in the task space should be adapted.$$$The proposed adaptation algorithm consists of two stages.$$$In the first stage, once the swing foot reaches its maximum height, the supervisory control is initiated until the touch is detected.$$$After the detection, the trajectories in the task space are modified to guarantee smooth landing.$$$In the second stage, this modification is preserved during the Double Support Phase (DSP), and released in the next Single Support Phase (SSP).$$$Effectiveness of the proposed online adaptation algorithm is experimentally verified through realization of the walking patterns on the SURENA III humanoid robot, designed and fabricated at CAST.$$$The walking is tested on a surface with various flat obstacles, where the swing foot is prone to either land on the ground soon or late.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D05313,"We present a uniform method for translating an arbitrary nondeterministic finite automaton (NFA) into a deterministic mass action input/output chemical reaction network (I/O CRN) that simulates it.$$$The I/O CRN receives its input as a continuous time signal consisting of concentrations of chemical species that vary to represent the NFA's input string in a natural way.$$$The I/O CRN exploits the inherent parallelism of chemical kinetics to simulate the NFA in real time with a number of chemical species that is linear in the size of the NFA.$$$We prove that the simulation is correct and that it is robust with respect to perturbations of the input signal, the initial concentrations of species, the output (decision), and the rate constants of the reactions of the I/O CRN.",RESULTS RESULTS METHODS RESULTS
D06877,"Convolutional LDPC ensembles, introduced by Felstrom and Zigangirov, have excellent thresholds and these thresholds are rapidly increasing as a function of the average degree.$$$Several variations on the basic theme have been proposed to date, all of which share the good performance characteristics of convolutional LDPC ensembles.$$$We describe the fundamental mechanism which explains why ""convolutional-like"" or ""spatially coupled"" codes perform so well.$$$In essence, the spatial coupling of the individual code structure has the effect of increasing the belief-propagation (BP) threshold of the new ensemble to its maximum possible value, namely the maximum-a-posteriori (MAP) threshold of the underlying ensemble.$$$For this reason we call this phenomenon ""threshold saturation.""$$$This gives an entirely new way of approaching capacity.$$$One significant advantage of such a construction is that one can create capacity-approaching ensembles with an error correcting radius which is increasing in the blocklength.$$$Our proof makes use of the area theorem of the BP-EXIT curve and the connection between the MAP and BP threshold recently pointed out by Measson, Montanari, Richardson, and Urbanke.$$$Although we prove the connection between the MAP and the BP threshold only for a very specific ensemble and only for the binary erasure channel, empirically a threshold saturation phenomenon occurs for a wide class of ensembles and channels.$$$More generally, we conjecture that for a large range of graphical systems a similar saturation of the ""dynamical"" threshold occurs once individual components are coupled sufficiently strongly.$$$This might give rise to improved algorithms as well as to new techniques for analysis.",BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS RESULTS RESULTS/CONCLUSIONS RESULTS METHODS RESULTS/CONCLUSIONS CONCLUSIONS CONCLUSIONS
D06692,"This short paper provides a description of an architecture to acquisition and use of knowledge by intelligent agents over a restricted domain of the Internet Infrastructure.$$$The proposed architecture is added to an intelligent agent deployment model over a very useful server for Internet Autonomous System administrators.$$$Such servers, which are heavily dependent on arbitrary and eventual updates of human beings, become unreliable.$$$This is a position paper that proposes three research questions that are still in progress.",OBJECTIVES RESULTS BACKGROUND OBJECTIVES
D01844,"An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they ""focus only on the information that can be obtained from the image alone"" (Hodosh et al., 2013, p. 859).$$$This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset.$$$Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.",BACKGROUND RESULTS RESULTS
D06667,"Prior efforts to create an autonomous computer system capable of predicting what a human being is thinking or feeling from facial expression data have been largely based on outdated, inaccurate models of how emotions work that rely on many scientifically questionable assumptions.$$$In our research, we are creating an empathetic system that incorporates the latest provable scientific understanding of emotions: that they are constructs of the human mind, rather than universal expressions of distinct internal states.$$$Thus, our system uses a user-dependent method of analysis and relies heavily on contextual information to make predictions about what subjects are experiencing.$$$Our system's accuracy and therefore usefulness are built on provable ground truths that prohibit the drawing of inaccurate conclusions that other systems could too easily make.",BACKGROUND OBJECTIVES METHODS CONCLUSIONS
D00794,"Evolutionary clustering aims at capturing the temporal evolution of clusters.$$$This issue is particularly important in the context of social media data that are naturally temporally driven.$$$In this paper, we propose a new probabilistic model-based evolutionary clustering technique.$$$The Temporal Multinomial Mixture (TMM) is an extension of classical mixture model that optimizes feature co-occurrences in the trade-off with temporal smoothness.$$$Our model is evaluated for two recent case studies on opinion aggregation over time.$$$We compare four different probabilistic clustering models and we show the superiority of our proposal in the task of instance-oriented clustering.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D02527,"Unsupervised learning permits the development of algorithms that are able to adapt to a variety of different data sets using the same underlying rules thanks to the autonomous discovery of discriminating features during training.$$$Recently, a new class of Hebbian-like and local unsupervised learning rules for neural networks have been developed that minimise a similarity matching cost-function.$$$These have been shown to perform sparse representation learning.$$$This study tests the effectiveness of one such learning rule for learning features from images.$$$The rule implemented is derived from a nonnegative classical multidimensional scaling cost-function, and is applied to both single and multi-layer architectures.$$$The features learned by the algorithm are then used as input to an SVM to test their effectiveness in classification on the established CIFAR-10 image dataset.$$$The algorithm performs well in comparison to other unsupervised learning algorithms and multi-layer networks, thus suggesting its validity in the design of a new class of compact, online learning networks.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D06936,"This paper presents a wp-style calculus for obtaining expectations on the outcomes of (mutually) recursive probabilistic programs.$$$We provide several proof rules to derive one-- and two--sided bounds for such expectations, and show the soundness of our wp-calculus with respect to a probabilistic pushdown automaton semantics.$$$We also give a wp-style calculus for obtaining bounds on the expected runtime of recursive programs that can be used to determine the (possibly infinite) time until termination of such programs.",OBJECTIVES/METHODS/RESULTS RESULTS METHODS/RESULTS
D02601,"Automatic annotation of images with descriptive words is a challenging problem with vast applications in the areas of image search and retrieval.$$$This problem can be viewed as a label-assignment problem by a classifier dealing with a very large set of labels, i.e., the vocabulary set.$$$We propose a novel annotation method that employs two layers of sparse coding and performs coarse-to-fine labeling.$$$Themes extracted from the training data are treated as coarse labels.$$$Each theme is a set of training images that share a common subject in their visual and textual contents.$$$Our system extracts coarse labels for training and test images without requiring any prior knowledge.$$$Vocabulary words are the fine labels to be associated with images.$$$Most of the annotation methods achieve low recall due to the large number of available fine labels, i.e., vocabulary words.$$$These systems also tend to achieve high precision for highly frequent words only while relatively rare words are more important for search and retrieval purposes.$$$Our system not only outperforms various previously proposed annotation systems, but also achieves symmetric response in terms of precision and recall.$$$Our system scores and maintains high precision for words with a wide range of frequencies.$$$Such behavior is achieved by intelligently reducing the number of available fine labels or words for each image based on coarse labels assigned to it.",BACKGROUND METHODS METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D00526,"It is known that both quantum and classical cellular automata (CA) exist that are computationally universal in the sense that they can simulate, after appropriate initialization, any quantum or classical computation, respectively.$$$Here we introduce a different notion of universality: a CA is called physically universal if every transformation on any finite region can be (approximately) implemented by the autonomous time evolution of the system after the complement of the region has been initialized in an appropriate way.$$$We pose the question of whether physically universal CAs exist.$$$Such CAs would provide a model of the world where the boundary between a physical system and its controller can be consistently shifted, in analogy to the Heisenberg cut for the quantum measurement problem.$$$We propose to study the thermodynamic cost of computation and control within such a model because implementing a cyclic process on a microsystem may require a non-cyclic process for its controller, whereas implementing a cyclic process on system and controller may require the implementation of a non-cyclic process on a ""meta""-controller, and so on.$$$Physically universal CAs avoid this infinite hierarchy of controllers and the cost of implementing cycles on a subsystem can be described by mixing properties of the CA dynamics.$$$We define a physical prior on the CA configurations by applying the dynamics to an initial state where half of the CA is in the maximum entropy state and half of it is in the all-zero state (thus reflecting the fact that life requires non-equilibrium states like the boundary between a hold and a cold reservoir).$$$As opposed to Solomonoff's prior, our prior does not only account for the Kolmogorov complexity but also for the cost of isolating the system during the state preparation if the preparation process is not robust.",BACKGROUND OBJECTIVES/RESULTS OBJECTIVES BACKGROUND OBJECTIVES BACKGROUND RESULTS RESULTS
D03883,"Genetic algorithms are considered as an original way to solve problems, probably because of their generality and of their ""blind"" nature.$$$But GAs are also unusual since the features of many implementations (among all that could be thought of) are principally led by the biological metaphor, while efficiency measurements intervene only afterwards.$$$We propose here to examine the relevance of these biomimetic aspects, by pointing out some fundamental similarities and divergences between GAs and the genome of living beings shaped by natural selection.$$$One of the main differences comes from the fact that GAs rely principally on the so-called implicit parallelism, while giving to the mutation/selection mechanism the second role.$$$Such differences could suggest new ways of employing GAs on complex problems, using complex codings and starting from nearly homogeneous populations.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES CONCLUSIONS
D02347,"Due to the ubiquity of batch data processing in cloud computing, the related problem of scheduling malleable batch tasks and its extensions have received significant attention recently.$$$In this paper, we consider a fundamental model where a set of n tasks is to be processed on C identical machines and each task is specified by a value, a workload, a deadline and a parallelism bound.$$$Within the parallelism bound, the number of machines assigned to a task can vary over time without affecting its workload.$$$For this model, we obtain two core results: a sufficient and necessary condition such that a set of tasks can be finished by their deadlines on C machines, and an algorithm to produce such a schedule.$$$These core results provide a conceptual tool and an optimal scheduling algorithm that enable proposing new algorithmic analysis and design and improving existing algorithms under various objectives.",BACKGROUND/OBJECTIVES OBJECTIVES OBJECTIVES RESULTS OBJECTIVES/RESULTS/CONCLUSIONS
D05562,"Distributed parameter estimation for large-scale systems is an active research problem.$$$The goal is to derive a distributed algorithm in which each agent obtains a local estimate of its own subset of the global parameter vector, based on local measurements as well as information received from its neighbours.$$$A recent algorithm has been proposed, which yields the optimal solution (i.e., the one that would be obtained using a centralized method) in finite time, provided the communication network forms an acyclic graph.$$$If instead, the graph is cyclic, the only available alternative algorithm, which is based on iterative matrix inversion, achieving the optimal solution, does so asymptotically.$$$However, it is also known that, in the cyclic case, the algorithm designed for acyclic graphs produces a solution which, although non optimal, is highly accurate.$$$In this paper we do a theoretical study of the accuracy of this algorithm, in communication networks forming cyclic graphs.$$$To this end, we provide bounds for the sub-optimality of the estimation error and the estimation error covariance, for a class of systems whose topological sparsity and signal-to-noise ratio satisfy certain condition.$$$Our results show that, at each node, the accuracy improves exponentially with the so-called loop-free depth.$$$Also, although the algorithm no longer converges in finite time in the case of cyclic graphs, simulation results show that the convergence is significantly faster than that of methods based on iterative matrix inversion.$$$Our results suggest that, depending on the loop-free depth, the studied algorithm may be the preferred option even in applications with cyclic communication graphs.",BACKGROUND OBJECTIVES BACKGROUND BACKGROUND RESULTS OBJECTIVES METHODS RESULTS RESULTS RESULTS
D06955,"Event-based state estimation can achieve estimation quality comparable to traditional time-triggered methods, but with a significantly lower number of samples.$$$In networked estimation problems, this reduction in sampling instants does, however, not necessarily translate into better usage of the shared communication resource.$$$Because typical event-based approaches decide instantaneously whether communication is needed or not, free slots cannot be reallocated immediately, and hence remain unused.$$$In this paper, novel predictive and self triggering protocols are proposed, which give the communication system time to adapt and reallocate freed resources.$$$From a unified Bayesian decision framework, two schemes are developed: self-triggers that predict, at the current triggering instant, the next one; and predictive triggers that indicate, at every time step, whether communication will be needed at a given prediction horizon.$$$The effectiveness of the proposed triggers in trading off estimation quality for communication reduction is compared in numerical simulations.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D04319,"Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation.$$$Most bibliometric analyses have in common their data sources: Thomson Reuters' Web of Science (WoS) and Elsevier's Scopus.$$$This research compares the journal coverage of both databases in terms of fields, countries and languages, using Ulrich's extensive periodical directory as a base for comparison.$$$Results indicate that the use of either WoS or Scopus for research evaluation may introduces biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities.$$$Similarly, English-language journals are overrepresented to the detriment of other languages.$$$While both databases share these biases, their coverage differs substantially.$$$As a consequence, the results of bibliometric analyses may vary depending on the database used.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS CONCLUSIONS
D02104,"We present an algorithm that incorporates a tabu search procedure into the framework of path relinking to tackle the job shop scheduling problem (JSP).$$$This tabu search/path relinking (TS/PR) algorithm comprises several distinguishing features, such as a specific relinking procedure and a reference solution determination method.$$$To test the performance of TS/PR, we apply it to tackle almost all of the benchmark JSP instances available in the literature.$$$The test results show that TS/PR obtains competitive results compared with state-of-the-art algorithms for JSP in the literature, demonstrating its efficacy in terms of both solution quality and computational efficiency.$$$In particular, TS/PR is able to improve the upper bounds for 49 out of the 205 tested instances and it solves a challenging instance that has remained unsolved for over 20 years.",BACKGROUND METHODS RESULTS CONCLUSIONS RESULTS
D02360,"In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process ""grounding"").$$$The approach is particularly well-suited for extracting large numbers of concepts from text.$$$We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text.$$$Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary.$$$Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.",OBJECTIVES/CONCLUSIONS OTHERS METHODS METHODS RESULTS
D05941,"Chirality plays an important role in physics, chemistry, biology, and other fields.$$$It describes an essential symmetry in structure.$$$However, chirality invariants are usually complicated in expression or difficult to evaluate.$$$In this paper, we present five general three-dimensional chirality invariants based on the generating functions.$$$And the five chiral invariants have four characteristics:(1) They play an important role in the detection of symmetry, especially in the treatment of 'false zero' problem.$$$(2) Three of the five chiral invariants decode an universal chirality index.$$$(3) Three of them are proposed for the first time.$$$(4) The five chiral invariants have low order no bigger than 4, brief expression, low time complexity O(n) and can act as descriptors of three-dimensional objects in shape analysis.$$$The five chiral invariants give a geometric view to study the chiral invariants.$$$And the experiments show that the five chirality invariants are effective and efficient, they can be used as a tool for symmetry detection or features in shape analysis.",BACKGROUND BACKGROUND BACKGROUND RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00597,"Inspired by CapsNet's routing-by-agreement mechanism, with its ability to learn object properties, and by center-of-mass calculations from physics, we propose a CapsNet architecture with object coordinate atoms and an LSTM network for evaluation.$$$The first is based on CapsNet but uses a new routing algorithm to find the objects' approximate positions in the image coordinate system, and the second is a parameterized affine transformation network that can predict future positions from past positions by learning the translation transformation from 2D object coordinates generated from the first network.$$$We demonstrate the learned translation transformation is transferable to another dataset without the need to train the transformation network again.$$$Only the CapsNet needs training on the new dataset.$$$As a result, our work shows that object recognition and motion prediction can be separated, and that motion prediction can be transferred to another dataset with different object types.",BACKGROUND METHODS RESULTS CONCLUSIONS CONCLUSIONS
D00165,"This paper investigates and evaluates support vector machine active learning algorithms for use with imbalanced datasets, which commonly arise in many applications such as information extraction applications.$$$Algorithms based on closest-to-hyperplane selection and query-by-committee selection are combined with methods for addressing imbalance such as positive amplification based on prevalence statistics from initial random samples.$$$Three algorithms (ClosestPA, QBagPA, and QBoostPA) are presented and carefully evaluated on datasets for text classification and relation extraction.$$$The ClosestPA algorithm is shown to consistently outperform the other two in a variety of ways and insights are provided as to why this is the case.",BACKGROUND/OBJECTIVES/METHODS METHODS OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D01693,"To overcome the tradeoff of the conventional normalized least mean square (NLMS) algorithm between fast convergence rate and low steady-state misalignment, this paper proposes a variable step size (VSS) NLMS algorithm by devising a new strategy to update the step size.$$$In this strategy, the input signal power and the cross-correlation between the input signal and the error signal are used to estimate the true tracking error power, reducing the effect of the system noise on the algorithm performance.$$$Moreover, the steady-state performances of the algorithm are provided for Gaussian white input signal and are verified by simulations.$$$Finally, simulation results in the context of the system identification and acoustic echo cancellation (AEC) have demonstrated that the proposed algorithm has lower steady-state misalignment than other VSS algorithms.",BACKGROUND METHODS OBJECTIVES RESULTS
D06450,"In this paper, we propose a novel fully convolutional two-stream fusion network (FCTSFN) for interactive image segmentation.$$$The proposed network includes two sub-networks: a two-stream late fusion network (TSLFN) that predicts the foreground at a reduced resolution, and a multi-scale refining network (MSRN) that refines the foreground at full resolution.$$$The TSLFN includes two distinct deep streams followed by a fusion network.$$$The intuition is that, since user interactions are more direct information on foreground/background than the image itself, the two-stream structure of the TSLFN reduces the number of layers between the pure user interaction features and the network output, allowing the user interactions to have a more direct impact on the segmentation result.$$$The MSRN fuses the features from different layers of TSLFN with different scales, in order to seek the local to global information on the foreground to refine the segmentation result at full resolution.$$$We conduct comprehensive experiments on four benchmark datasets.$$$The results show that the proposed network achieves competitive performance compared to current state-of-the-art interactive image segmentation methods",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS
D02845,"In this paper, the problem of secure transmission of sensitive contents over the public network Internet is addressed by proposing a novel data hiding method in encrypted images with dual-level security.$$$The secret information is divided into three blocks using a specific pattern, followed by an encryption mechanism based on the three-level encryption algorithm (TLEA).$$$The input image is scrambled using a secret key, and the encrypted sub-message blocks are then embedded in the scrambled image by cyclic18 least significant bit (LSB) substitution method, utilizing LSBs and intermediate LSB planes.$$$Furthermore, the cover image and its planes are rotated at different angles using a secret key prior to embedding, deceiving the attacker during data extraction.$$$The usage of message blocks division, TLEA, image scrambling, and the cyclic18 LSB method results in an advanced security system, maintaining the visual transparency of resultant images and increasing the security of embedded data.$$$In addition, employing various secret keys for image scrambling, data encryption, and data hiding using the cyclic18 LSB method makes the data recovery comparatively more challenging for attackers.$$$Experimental results not only validate the effectiveness of the proposed framework in terms of visual quality and security compared to other state-of-the-art methods, but also suggest its feasibility for secure transmission of diagnostically important keyframes to healthcare centers and gastroenterologists during wireless capsule endoscopy.",OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D05906,"Prediction-based transformation is applied to control-affine systems with distributed input delays.$$$Transformed system state is calculated as a prediction of the system's future response to the past input with future input set to zero.$$$Stabilization of the new system leads to Lyapunov-Krasovskii proven stabilization of the original one.$$$Conditions on the original system are: smooth linearly bounded open-loop vector field and smooth uniformly bounded input vectors.$$$About the transformed system which turns out to be affine in the undelayed input but with input vectors dependent on the input history and system state, we assume existence of a linearly bounded stabilizing feedback and quadratically bounded control-Lyapunov function.$$$If all assumptions hold globally, then achieved exponential stability is global, otherwise local.$$$Analytical and numerical control design examples are provided.",METHODS METHODS RESULTS BACKGROUND BACKGROUND RESULTS RESULTS
D04268,"Researchers spend a great deal of time reading research papers.$$$Keshav (2012) provides a three-pass method to researchers to improve their reading skills.$$$This article extends Keshav's method for reading a research compendium.$$$Research compendia are an increasingly used form of publication, which packages not only the research paper's text and figures, but also all data and software for better reproducibility.$$$We introduce the existing conventions for research compendia and suggest how to utilise their shared properties in a structured reading process.$$$Unlike the original, this article is not build upon a long history but intends to provide guidance at the outset of an emerging practice.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS/RESULTS CONCLUSIONS
D02112,"Truck Factor (TF) is a metric proposed by the agile community as a tool to identify concentration of knowledge in software development environments.$$$It states the minimal number of developers that have to be hit by a truck (or quit) before a project is incapacitated.$$$In other words, TF helps to measure how prepared is a project to deal with developer turnover.$$$Despite its clear relevance, few studies explore this metric.$$$Altogether there is no consensus about how to calculate it, and no supporting evidence backing estimates for systems in the wild.$$$To mitigate both issues, we propose a novel (and automated) approach for estimating TF-values, which we execute against a corpus of 133 popular project in GitHub.$$$We later survey developers as a means to assess the reliability of our results.$$$Among others, we find that the majority of our target systems (65%) have TF <= 2.$$$Surveying developers from 67 target systems provides confidence towards our estimates; in 84% of the valid answers we collect, developers agree or partially agree that the TF's authors are the main authors of their systems; in 53% we receive a positive or partially positive answer regarding our estimated truck factors.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D06321,"The continuing expansion of Internet media consumption has increased traffic volumes, and hence congestion, on access links.$$$In response, both mobile and wireline ISPs must either increase capacity or perform traffic engineering over existing resources.$$$Unfortunately, provisioning timescales are long, the process is costly, and single-homing means operators cannot balance across the last mile.$$$Inspired by energy and transport networks, we propose demand-side management of users to reduce the impact caused by consumption patterns out-pacing that of edge network provision.$$$By directly affecting user behaviour through a range of incentives, our techniques enable resource management over shorter timescales than is possible in conventional networks.$$$Using survey data from 100 participants we explore the feasibility of introducing the principles of demand-side management in today's networks.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS METHODS/RESULTS
D04912,"We present a framework for learning efficient holistic representation for handwritten word images.$$$The proposed method uses a deep convolutional neural network with traditional classification loss.$$$The major strengths of our work lie in: (i) the efficient usage of synthetic data to pre-train a deep network, (ii) an adapted version of ResNet-34 architecture with region of interest pooling (referred as HWNet v2) which learns discriminative features with variable sized word images, and (iii) realistic augmentation of training data with multiple scales and elastic distortion which mimics the natural process of handwriting.$$$We further investigate the process of fine-tuning at various layers to reduce the domain gap between synthetic and real domain and also analyze the in-variances learned at different layers using recent visualization techniques proposed in literature.$$$Our representation leads to state of the art word spotting performance on standard handwritten datasets and historical manuscripts in different languages with minimal representation size.$$$On the challenging IAM dataset, our method is first to report an mAP above 0.90 for word spotting with a representation size of just 32 dimensions.$$$Further more, we also present results on printed document datasets in English and Indic scripts which validates the generic nature of the proposed framework for learning word image representation.",BACKGROUND METHODS METHODS METHODS RESULTS RESULTS RESULTS
D04349,"Regular languages (RL) are the simplest family in Chomsky's hierarchy.$$$Thanks to their simplicity they enjoy various nice algebraic and logic properties that have been successfully exploited in many application fields.$$$Practically all of their related problems are decidable, so that they support automatic verification algorithms.$$$Also, they can be recognized in real-time.$$$Context-free languages (CFL) are another major family well-suited to formalize programming, natural, and many other classes of languages; their increased generative power w.r.t.$$$RL, however, causes the loss of several closure properties and of the decidability of important problems; furthermore they need complex parsing algorithms.$$$Thus, various subclasses thereof have been defined with different goals, spanning from efficient, deterministic parsing to closure properties, logic characterization and automatic verification techniques.$$$Among CFL subclasses, so-called structured ones, i.e., those where the typical tree-structure is visible in the sentences, exhibit many of the algebraic and logic properties of RL, whereas deterministic CFL have been thoroughly exploited in compiler construction and other application fields.$$$After surveying and comparing the main properties of those various language families, we go back to operator precedence languages (OPL), an old family through which R. Floyd pioneered deterministic parsing, and we show that they offer unexpected properties in two fields so far investigated in totally independent ways: they enable parsing parallelization in a more effective way than traditional sequential parsers, and exhibit the same algebraic and logic properties so far obtained only for less expressive language families.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES/RESULTS/CONCLUSIONS
D00718,"User Interfaces (UIs) intensively rely on event-driven programming: widgets send UI events, which capture users' interactions, to dedicated objects called controllers.$$$Controllers use several UI listeners that handle these events to produce UI commands.$$$First, we reveal the presence of design smells in the code that describes and controls UIs.$$$Second, we demonstrate that specific code analyses are necessary to analyze and refactor UI code, because of its coupling with the rest of the code.$$$We conducted an empirical study on four large Java Swing and SWT open-source software systems.$$$We study to what extent the number of UI commands that a UI listener can produce has an impact on the change- and fault-proneness of the UI listener code.$$$We develop a static code analysis for detecting UI commands in the code.$$$We identify a new type of design smell, called Blob Listener that characterizes UI listeners that can produce more than two UI commands.$$$We propose a systematic static code analysis procedure that searches for Blob Listeners that we implement in InspectorGuidget.$$$We conducted experiments on the four software systems for which we manually identified 53 instances of Blob Listener.$$$InspectorGuidget successfully detected 52 Blob Listeners out of 53.$$$The results exhibit a precision of 81.25% and a recall of 98.11%.$$$We then developed a semi-automatically and behavior-preserving refactoring process to remove Blob Listeners.$$$49.06% of the 53 Blob Listeners were automatically refactored.$$$Patches for JabRef, and FreeCol have been accepted and merged.$$$Discussions with developers of the four software systems assess the relevance of the Blob Listener.$$$This work shows that UI code also suffers from design smells that have to be identified and characterized.$$$We argue that studies have to be conducted to find other UI design smells and tools that analyze UI code must be developed.",BACKGROUND BACKGROUND OBJECTIVES/RESULTS OBJECTIVES METHODS METHODS METHODS RESULTS METHODS RESULTS RESULTS RESULTS METHODS RESULTS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D02588,"We propose a new algorithm to the problem of polygonal curve approximation based on a multiresolution approach.$$$This algorithm is suboptimal but still maintains some optimality between successive levels of resolution using dynamic programming.$$$We show theoretically and experimentally that this algorithm has a linear complexity in time and space.$$$We experimentally compare the outcomes of our algorithm to the optimal ""full search"" dynamic programming solution and finally to classical merge and split approaches.$$$The experimental evaluations confirm the theoretical derivations and show that the proposed approach evaluated on 2D coastal maps either show a lower time complexity or provide polygonal approximations closer to the input discrete curves.",RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D04722,"In this paper we demonstrate the use of intelligent optimization methodologies on the visualization optimization of virtual / simulated environments.$$$The problem of automatic selection of an optimized set of views, which better describes an on-going simulation over a virtual environment is addressed in the context of the RoboCup Rescue Simulation domain.$$$A generic architecture for optimization is proposed and described.$$$We outline the possible extensions of this architecture and argue on how several problems within the fields of Interactive Rendering and Visualization can benefit from it.",BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES/RESULTS OTHERS
D01947,"Human face analysis is an important task in computer vision.$$$According to cognitive-psychological studies, facial dynamics could provide crucial cues for face analysis.$$$The motion of a facial local region in facial expression is related to the motion of other facial local regions.$$$In this paper, a novel deep learning approach, named facial dynamics interpreter network, has been proposed to interpret the important relations between local dynamics for estimating facial traits from expression sequence.$$$The facial dynamics interpreter network is designed to be able to encode a relational importance, which is used for interpreting the relation between facial local dynamics and estimating facial traits.$$$By comparative experiments, the effectiveness of the proposed method has been verified.$$$The important relations between facial local dynamics are investigated by the proposed facial dynamics interpreter network in gender classification and age estimation.$$$Moreover, experimental results show that the proposed method outperforms the state-of-the-art methods in gender classification and age estimation.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D01995,"We discuss the scheduling of a set of networked control systems implemented over a shared communication network.$$$Each control loop is described by a linear-time-invariant (LTI) system with an event-triggered implementation.$$$We assume the network can be used by at most one control loop at any time instant and after each controller update, a pre-defined channel occupancy time elapses before the network is available.$$$In our framework we offer the scheduler two options to avoid conflicts: using the event-triggering mechanism, where the scheduler can choose the triggering coefficient; or forcing controller updates at an earlier pre-defined time.$$$Our objective is avoiding communication conflict while guaranteeing stability of all control loops.$$$We formulate the original scheduling problem as a control synthesis problem over a network of timed game automata (NTGA) with a safety objective.$$$The NTGA is obtained by taking the parallel composition of the timed game automata (TGA) associated with the network and with all control loops.$$$The construction of TGA associated with control loops leverages recent results on the abstraction of timing models of event-triggered LTI systems.$$$In our problem, the safety objective is to avoid that update requests from a control loop happen while the network is in use by another task.$$$We showcase the results in some examples.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D05586,"Salient object detection is a fundamental problem and has been received a great deal of attentions in computer vision.$$$Recently deep learning model became a powerful tool for image feature extraction.$$$In this paper, we propose a multi-scale deep neural network (MSDNN) for salient object detection.$$$The proposed model first extracts global high-level features and context information over the whole source image with recurrent convolutional neural network (RCNN).$$$Then several stacked deconvolutional layers are adopted to get the multi-scale feature representation and obtain a series of saliency maps.$$$Finally, we investigate a fusion convolution module (FCM) to build a final pixel level saliency map.$$$The proposed model is extensively evaluated on four salient object detection benchmark datasets.$$$Results show that our deep model significantly outperforms other 12 state-of-the-art approaches.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D05484,"Contemporary Deep Neural Network (DNN) contains millions of synaptic connections with tens to hundreds of layers.$$$The large computation and memory requirements pose a challenge to the hardware design.$$$In this work, we leverage the intrinsic activation sparsity of DNN to substantially reduce the execution cycles and the energy consumption.$$$An end-to-end training algorithm is proposed to develop a lightweight run-time predictor for the output activation sparsity on the fly.$$$From our experimental results, the computation overhead of the prediction phase can be reduced to less than 5% of the original feedforward phase with negligible accuracy loss.$$$Furthermore, an energy-efficient hardware architecture, SparseNN, is proposed to exploit both the input and output sparsity.$$$SparseNN is a scalable architecture with distributed memories and processing elements connected through a dedicated on-chip network.$$$Compared with the state-of-the-art accelerators which only exploit the input sparsity, SparseNN can achieve a 10%-70% improvement in throughput and a power reduction of around 50%.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS OBJECTIVES METHODS CONCLUSIONS
D06893,"Handwritten recognition (HWR) is the ability of a computer to receive and interpret intelligible handwritten input from source such as paper documents, photographs, touch-screens and other devices.$$$In this paper we will using three (3) classification t o re cognize the handwritten which is SVM, KNN and Neural Network.",BACKGROUND METHODS
D01961,"We propose a novel distributed inference algorithm for continuous graphical models, by extending Stein variational gradient descent (SVGD) to leverage the Markov dependency structure of the distribution of interest.$$$Our approach combines SVGD with a set of structured local kernel functions defined on the Markov blanket of each node, which alleviates the curse of high dimensionality and simultaneously yields a distributed algorithm for decentralized inference tasks.$$$We justify our method with theoretical analysis and show that the use of local kernels can be viewed as a new type of localized approximation that matches the target distribution on the conditional distributions of each node over its Markov blanket.$$$Our empirical results show that our method outperforms a variety of baselines including standard MCMC and particle message passing methods.",OBJECTIVES METHODS RESULTS RESULTS
D01866,"Traffic Matrix estimation has always caught attention from researchers for better network management and future planning.$$$With the advent of high traffic loads due to Cloud Computing platforms and Software Defined Networking based tunable routing and traffic management algorithms on the Internet, it is more necessary as ever to be able to predict current and future traffic volumes on the network.$$$For large networks such origin-destination traffic prediction problem takes the form of a large under-constrained and under-determined system of equations with a dynamic measurement matrix.$$$In this work, we present our Compressed Sensing with Dynamic Model Estimation (CS-DME) architecture suitable for modern software defined networks.$$$Our main contributions are: (1) we formulate an approach in which measurement matrix in the compressed sensing scheme can be accurately and dynamically estimated through a reformulation of the problem based on traffic demands.$$$(2) We show that the problem formulation using a dynamic measurement matrix based on instantaneous traffic demands may be used instead of a stationary binary routing matrix which is more suitable to modern Software Defined Networks that are constantly evolving in terms of routing by inspection of its Eigen Spectrum using two real world datasets.$$$(3) We also show that linking this compressed measurement matrix dynamically with the measured parameters can lead to acceptable estimation of Origin Destination (OD) Traffic flows with marginally poor results with other state-of-art schemes relying on fixed measurement matrices.$$$(4) Furthermore, using this compressed reformulated problem, a new strategy for selection of vantage points for most efficient traffic matrix estimation is also presented through a secondary compression technique based on subset of link measurements.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS/RESULTS METHODS/RESULTS METHODS
D00920,"Sampling efficiency in a highly constrained environment has long been a major challenge for sampling-based planners.$$$In this work, we propose Rapidly-exploring Random disjointed-Trees* (RRdT*), an incremental optimal multi-query planner.$$$RRdT* uses multiple disjointed-trees to exploit local-connectivity of spaces via Markov Chain random sampling, which utilises neighbourhood information derived from previous successful and failed samples.$$$To balance local exploitation, RRdT* actively explore unseen global spaces when local-connectivity exploitation is unsuccessful.$$$The active trade-off between local exploitation and global exploration is formulated as a multi-armed bandit problem.$$$We argue that the active balancing of global exploration and local exploitation is the key to improving sample efficient in sampling-based motion planners.$$$We provide rigorous proofs of completeness and optimal convergence for this novel approach.$$$Furthermore, we demonstrate experimentally the effectiveness of RRdT*'s locally exploring trees in granting improved visibility for planning.$$$Consequently, RRdT* outperforms existing state-of-the-art incremental planners, especially in highly constrained environments.",BACKGROUND OBJECTIVES METHODS METHODS METHODS OBJECTIVES/CONCLUSIONS METHODS/RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00048,"Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting.$$$However, vanilla online variants are on-policy only and not able to take advantage of off-policy data.$$$In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer.$$$This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values.$$$This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates.$$$We refer to the new technique as 'PGQL', for policy gradient and Q-learning.$$$We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms.$$$We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL.$$$In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS OBJECTIVES RESULTS RESULTS
D06074,"Hosting platforms for software projects can form collaborative social networks and a prime example of this is GitHub which is arguably the most popular platform of this kind.$$$An open source project recommendation system could be a major feature for a platform like GitHub, enabling its users to find relevant projects in a fast and simple manner.$$$We perform network analysis on a constructed graph based on GitHub data and present a recommendation system that uses link prediction.",BACKGROUND OBJECTIVES METHODS
D06775,"Differential privacy is a promising approach to privacy preserving data analysis with a well-developed theory for functions.$$$Despite recent work on implementing systems that aim to provide differential privacy, the problem of formally verifying that these systems have differential privacy has not been adequately addressed.$$$This paper presents the first results towards automated verification of source code for differentially private interactive systems.$$$We develop a formal probabilistic automaton model of differential privacy for systems by adapting prior work on differential privacy for functions.$$$The main technical result of the paper is a sound proof technique based on a form of probabilistic bisimulation relation for proving that a system modeled as a probabilistic automaton satisfies differential privacy.$$$The novelty lies in the way we track quantitative privacy leakage bounds using a relation family instead of a single relation.$$$We illustrate the proof technique on a representative automaton motivated by PINQ, an implemented system that is intended to provide differential privacy.$$$To make our proof technique easier to apply to realistic systems, we prove a form of refinement theorem and apply it to show that a refinement of the abstract PINQ automaton also satisfies our differential privacy definition.$$$Finally, we begin the process of automating our proof technique by providing an algorithm for mechanically checking a restricted class of relations from the proof technique.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS METHODS METHODS RESULTS METHODS
D03550,"Publications in the life sciences are characterized by a large technical vocabulary, with many lexical and semantic variations for expressing the same concept.$$$Towards addressing the problem of relevance in biomedical literature search, we introduce a deep learning model for the relevance of a document's text to a keyword style query.$$$Limited by a relatively small amount of training data, the model uses pre-trained word embeddings.$$$With these, the model first computes a variable-length Delta matrix between the query and document, representing a difference between the two texts, which is then passed through a deep convolution stage followed by a deep feed-forward network to compute a relevance score.$$$This results in a fast model suitable for use in an online search engine.$$$The model is robust and outperforms comparable state-of-the-art deep learning approaches.",BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D06858,"In general the problem of finding a miminum spanning tree for a weighted directed graph is difficult but solvable.$$$There are a lot of differences between problems for directed and undirected graphs, therefore the algorithms for undirected graphs cannot usually be applied to the directed case.$$$In this paper we examine the kind of weights such that the problems are equivalent and a minimum spanning tree of a directed graph may be found by a simple algorithm for an undirected graph.",BACKGROUND METHODS RESULTS
D00648,"Deep neural networks have been shown to achieve state-of-the-art performance in several machine learning tasks.$$$Stochastic Gradient Descent (SGD) is the preferred optimization algorithm for training these networks and asynchronous SGD (ASGD) has been widely adopted for accelerating the training of large-scale deep networks in a distributed computing environment.$$$However, in practice it is quite challenging to tune the training hyperparameters (such as learning rate) when using ASGD so as achieve convergence and linear speedup, since the stability of the optimization algorithm is strongly influenced by the asynchronous nature of parameter updates.$$$In this paper, we propose a variant of the ASGD algorithm in which the learning rate is modulated according to the gradient staleness and provide theoretical guarantees for convergence of this algorithm.$$$Experimental verification is performed on commonly-used image classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and the conventional ASGD algorithm.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS
D03657,"In this article we show how power transformations can be used as a common framework for the derivation of local term weights.$$$We found that under some parametric conditions, BM25 and inverse regression produce equivalent results.$$$As a special case of inverse regression, we show that the largest increment in term weight occurs when a term is mentioned for the second time.$$$A model based on inverse regression (BM25IR) is presented.$$$Simulations suggest that BM25IR works fairly well for different BM25 parametric conditions and document lengths.",BACKGROUND/OBJECTIVES/CONCLUSIONS METHODS/RESULTS RESULTS OBJECTIVES/CONCLUSIONS CONCLUSIONS
D00162,"We show how faceted search using a combination of traditional classification systems and mixed-membership topic models can go beyond keyword search to inform resource discovery, hypothesis formulation, and argument extraction for interdisciplinary research.$$$Our test domain is the history and philosophy of scientific work on animal mind and cognition.$$$The methods can be generalized to other research areas and ultimately support a system for semi-automatic identification of argument structures.$$$We provide a case study for the application of the methods to the problem of identifying and extracting arguments about anthropomorphism during a critical period in the development of comparative psychology.$$$We show how a combination of classification systems and mixed-membership models trained over large digital libraries can inform resource discovery in this domain.$$$Through a novel approach of ""drill-down"" topic modeling---simultaneously reducing both the size of the corpus and the unit of analysis---we are able to reduce a large collection of fulltext volumes to a much smaller set of pages within six focal volumes containing arguments of interest to historians and philosophers of comparative psychology.$$$The volumes identified in this way did not appear among the first ten results of the keyword search in the HathiTrust digital library and the pages bear the kind of ""close reading"" needed to generate original interpretations that is the heart of scholarly work in the humanities.$$$Zooming back out, we provide a way to place the books onto a map of science originally constructed from very different data and for different purposes.$$$The multilevel approach advances understanding of the intellectual and societal contexts in which writings are interpreted.",OBJECTIVES/METHODS BACKGROUND CONCLUSIONS OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS RESULTS RESULTS CONCLUSIONS
D03819,"The modeling of speech can be used for speech synthesis and speech recognition.$$$We present a speech analysis method based on pole-zero modeling of speech with mixed block sparse and Gaussian excitation.$$$By using a pole-zero model, instead of the all-pole model, a better spectral fitting can be expected.$$$Moreover, motivated by the block sparse glottal flow excitation during voiced speech and the white noise excitation for unvoiced speech, we model the excitation sequence as a combination of block sparse signals and white noise.$$$A variational EM (VEM) method is proposed for estimating the posterior PDFs of the block sparse residuals and point estimates of mod- elling parameters within a sparse Bayesian learning framework.$$$Compared to conventional pole-zero and all-pole based methods, experimental results show that the proposed method has lower spectral distortion and good performance in reconstructing of the block sparse excitation.",BACKGROUND METHODS OBJECTIVES METHODS METHODS RESULTS
D01040,"Many deep models have been recently proposed for anomaly detection.$$$This paper presents comparison of selected generative deep models and classical anomaly detection methods on an extensive number of non--image benchmark datasets.$$$We provide statistical comparison of the selected models, in many configurations, architectures and hyperparamaters.$$$We arrive to conclusion that performance of the generative models is determined by the process of selection of their hyperparameters.$$$Specifically, performance of the deep generative models deteriorates with decreasing amount of anomalous samples used in hyperparameter selection.$$$In practical scenarios of anomaly detection, none of the deep generative models systematically outperforms the kNN.",BACKGROUND METHODS METHODS CONCLUSIONS CONCLUSIONS RESULTS
D02472,"Existing region-based object detectors are limited to regions with fixed box geometry to represent objects, even if those are highly non-rectangular.$$$In this paper we introduce DP-FCN, a deep model for object detection which explicitly adapts to shapes of objects with deformable parts.$$$Without additional annotations, it learns to focus on discriminative elements and to align them, and simultaneously brings more invariance for classification and geometric information to refine localization.$$$DP-FCN is composed of three main modules: a Fully Convolutional Network to efficiently maintain spatial resolution, a deformable part-based RoI pooling layer to optimize positions of parts and build invariance, and a deformation-aware localization module explicitly exploiting displacements of parts to improve accuracy of bounding box regression.$$$We experimentally validate our model and show significant gains.$$$DP-FCN achieves state-of-the-art performances of 83.1% and 80.9% on PASCAL VOC 2007 and 2012 with VOC data only.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS
D04554,"Human action recognition in 3D skeleton sequences has attracted a lot of research attention.$$$Recently, Long Short-Term Memory (LSTM) networks have shown promising performance in this task due to their strengths in modeling the dependencies and dynamics in sequential data.$$$As not all skeletal joints are informative for action recognition, and the irrelevant joints often bring noise which can degrade the performance, we need to pay more attention to the informative ones.$$$However, the original LSTM network does not have explicit attention ability.$$$In this paper, we propose a new class of LSTM network, Global Context-Aware Attention LSTM (GCA-LSTM), for skeleton based action recognition.$$$This network is capable of selectively focusing on the informative joints in each frame of each skeleton sequence by using a global context memory cell.$$$To further improve the attention capability of our network, we also introduce a recurrent attention mechanism, with which the attention performance of the network can be enhanced progressively.$$$Moreover, we propose a stepwise training scheme in order to train our network effectively.$$$Our approach achieves state-of-the-art performance on five challenging benchmark datasets for skeleton based action recognition.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS
D05306,"Clone-and-own approach is a natural way of source code reuse for software developers.$$$To assess how known bugs and security vulnerabilities of a cloned component affect an application, developers and security analysts need to identify an original version of the component and understand how the cloned component is different from the original one.$$$Although developers may record the original version information in a version control system and/or directory names, such information is often either unavailable or incomplete.$$$In this research, we propose a code search method that takes as input a set of source files and extracts all the components including similar files from a software ecosystem (i.e., a collection of existing versions of software packages).$$$Our method employs an efficient file similarity computation using b-bit minwise hashing technique.$$$We use an aggregated file similarity for ranking components.$$$To evaluate the effectiveness of this tool, we analyzed 75 cloned components in Firefox and Android source code.$$$The tool took about two hours to report the original components from 10 million files in Debian GNU/Linux packages.$$$Recall of the top-five components in the extracted lists is 0.907, while recall of a baseline using SHA-1 file hash is 0.773, according to the ground truth recorded in the source code repositories.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS
D06948,"Image Forensics has already achieved great results for the source camera identification task on images.$$$Standard approaches for data coming from Social Network Platforms cannot be applied due to different processes involved (e.g., scaling, compression, etc.).$$$Over 1 billion images are shared each day on the Internet and obtaining information about their history from the moment they were acquired could be exploited for investigation purposes.$$$In this paper, a classification engine for the reconstruction of the history of an image, is presented.$$$Specifically, exploiting K-NN and decision trees classifiers and a-priori knowledge acquired through image analysis, we propose an automatic approach that can understand which Social Network Platform has processed an image and the software application used to perform the image upload.$$$The engine makes use of proper alterations introduced by each platform as features.$$$Results, in terms of global accuracy on a dataset of 2720 images, confirm the effectiveness of the proposed strategy.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D06900,"Regular Path Queries (RPQs) are a type of graph query where answers are pairs of nodes connected by a sequence of edges matching a regular expression.$$$We study the techniques to process such queries on a distributed graph of data.$$$While many techniques assume the location of each data element (node or edge) is known, when the components of the distributed system are autonomous, the data will be arbitrarily distributed.$$$As the different query processing strategies are equivalently costly in the worst case, we isolate query-dependent cost factors and present a method to choose between strategies, using new query cost estimation techniques.$$$We evaluate our techniques using meaningful queries on biomedical data.",BACKGROUND OBJECTIVES OBJECTIVES RESULTS OTHERS
D05835,"The serious privacy and security problems related to online social networks (OSNs) are what fueled two complementary studies as part of this thesis.$$$In the first study, we developed a general algorithm for the mining of data of targeted organizations by using Facebook (currently the most popular OSN) and socialbots.$$$By friending employees in a targeted organization, our active socialbots were able to find new employees and informal organizational links that we could not find by crawling with passive socialbots.$$$We evaluated our method on the Facebook OSN and were able to reconstruct the social networks of employees in three distinct, actual organizations.$$$Furthermore, in the crawling process with our active socialbots we discovered up to 13.55% more employees and 22.27% more informal organizational links in contrast to the crawling process that was performed by passive socialbots with no company associations as friends.$$$In our second study, we developed a general algorithm for reaching specific OSN users who declared themselves to be employees of targeted organizations, using the topologies of organizational social networks and utilizing socialbots.$$$We evaluated the proposed method on targeted users from three actual organizations on Facebook, and two actual organizations on the Xing OSN (another popular OSN platform).$$$Eventually, our socialbots were able to reach specific users with a success rate of up to 70% on Facebook, and up to 60% on Xing.",BACKGROUND OBJECTIVES METHODS RESULTS RESULTS METHODS RESULTS RESULTS
D06730,"We describe a protocol for quantum information splitting (QIS) of a restricted class of three-qubit states among three parties Alice, Bob and Charlie, using a pair of GHZ states as the quantum channel.$$$There are two different forms of this three-qubit state that is used for QIS depending on the distribution of the particles among the three parties.$$$There is also a special type of four-qubit state that can be used for QIS using the above channel.$$$We explicitly construct the quantum channel, Alice's measurement basis and the analytic form of the unitary operations required by the receiver for such a purpose.",OBJECTIVES BACKGROUND OBJECTIVES METHODS
D05767,"Tagging is a popular feature that supports several collaborative tasks, including search, as tags produced by one user can help others finding relevant content.$$$However, task performance depends on the existence of 'good' tags.$$$A first step towards creating incentives for users to produce 'good' tags is the quantification of their value in the first place.$$$This work fills this gap by combining qualitative and quantitative research methods.$$$In particular, using contextual interviews, we first determine aspects that influence users' perception of tags' value for exploratory search.$$$Next, we formalize some of the identified aspects and propose an information-theoretical method with provable properties that quantifies the two most important aspects (according to the qualitative analysis) that influence the perception of tag value: the ability of a tag to reduce the search space while retrieving relevant items to the user.$$$The evaluation on real data shows that our method is accurate: tags that users consider more important have higher value than tags users have not expressed interest.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D06909,"This paper proposes a double-layered framework (or form of network) to integrate two mechanisms, termed consensus and conservation, achieving distributed solution of a linear equation.$$$The multi-agent framework considered in the paper is composed of clusters (which serve as a form of aggregating agent) and each cluster consists of a sub-network of agents.$$$By achieving consensus and conservation through agent-agent communications in the same cluster and cluster-cluster communications, distributed algorithms are devised for agents to cooperatively achieve a solution to the overall linear equation.$$$These algorithms outperform existing consensus-based algorithms, including but not limited to the following aspects: first, each agent does not have to know as much as a complete row or column of the overall equation; second, each agent only needs to control as few as two scalar states when the number of clusters and the number of agents are sufficiently large; third, the dimensions of agents' states in the proposed algorithms do not have to be the same (while in contrast, algorithms based on the idea of standard consensus inherently require all agents' states to be of the same dimension).$$$Both analytical proof and simulation results are provided to validate exponential convergence of the proposed distributed algorithms in solving linear equations.",OBJECTIVES METHODS METHODS BACKGROUND/RESULTS OTHERS
D04664,"Network based on distributed caching of content is a new architecture to alleviate the ongoing explosive demands for rate of multi-media traffic.$$$In caching networks, coded caching is a recently proposed technique that achieves significant performance gains compared to uncoded caching schemes.$$$In this paper, we derive a lower bound on the average rate with a memory constraint for a family of caching allocation placement and a family of XOR cooperative delivery.$$$The lower bound inspires us how placement and delivery affect the rate memory tradeoff.$$$Based on the clues, we design a new placement and two new delivery algorithms.$$$On one hand, the new placement scheme can allocate the cache more flexibly compared to grouping scheme.$$$On the other hand, the new delivery can exploit more cooperative opportunities compared to the known schemes.$$$The simulations validate our idea.",BACKGROUND BACKGROUND RESULTS METHODS METHODS CONCLUSIONS CONCLUSIONS OTHERS
D04689,"Recent studies on face attribute transfer have achieved great success.$$$A lot of models are able to transfer face attributes with an input image.$$$However, they suffer from three limitations: (1) incapability of generating image by exemplars; (2) being unable to transfer multiple face attributes simultaneously; (3) low quality of generated images, such as low-resolution or artifacts.$$$To address these limitations, we propose a novel model which receives two images of opposite attributes as inputs.$$$Our model can transfer exactly the same type of attributes from one image to another by exchanging certain part of their encodings.$$$All the attributes are encoded in a disentangled manner in the latent space, which enables us to manipulate several attributes simultaneously.$$$Besides, our model learns the residual images so as to facilitate training on higher resolution images.$$$With the help of multi-scale discriminators for adversarial training, it can even generate high-quality images with finer details and less artifacts.$$$We demonstrate the effectiveness of our model on overcoming the above three limitations by comparing with other methods on the CelebA face database.$$$A pytorch implementation is available at https://github.com/Prinsphield/ELEGANT.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS OTHERS
D06334,"While there are many approaches for automatically proving termination of term rewrite systems, up to now there exist only few techniques to disprove their termination automatically.$$$Almost all of these techniques try to find loops, where the existence of a loop implies non-termination of the rewrite system.$$$However, most programming languages use specific evaluation strategies, whereas loop detection techniques usually do not take strategies into account.$$$So even if a rewrite system has a loop, it may still be terminating under certain strategies.$$$Therefore, our goal is to develop decision procedures which can determine whether a given loop is also a loop under the respective evaluation strategy.$$$In earlier work, such procedures were presented for the strategies of innermost, outermost, and context-sensitive evaluation.$$$In the current paper, we build upon this work and develop such decision procedures for important strategies like leftmost-innermost, leftmost-outermost, (max-)parallel-innermost, (max-)parallel-outermost, and forbidden patterns (which generalize innermost, outermost, and context-sensitive strategies).$$$In this way, we obtain the first approach to disprove termination under these strategies automatically.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND RESULTS CONCLUSIONS
D00533,"In research activities regarding Magnetic Resonance Imaging in medicine, simulation tools with a universal approach are rare.$$$Usually, simulators are developed and used which tend to be restricted to a particular, small range of applications.$$$This led to the design and implementation of a new simulator PARSPIN, the subject of this thesis.$$$In medical applications, the Bloch equation is a well-suited mathematical model of the underlying physics with a wide scope.$$$In this thesis, it is shown how analytical solutions of the Bloch equation can be found, which promise substantial execution time advantages over numerical solution methods.$$$From these analytical solutions of the Bloch equation, a new formalism for the description and the analysis of complex imaging experiments is derived, the K-t formalism.$$$It is shown that modern imaging methods can be better explained by the K-t formalism than by observing and analysing the magnetization of each spin of a spin ensemble.$$$Various approaches for a numerical simulation of Magnetic Resonance imaging are discussed.$$$It is shown that a simulation tool based on the K-t formalism promises a substantial gain in execution time.$$$Proper spatial discretization according to the sampling theorem, a topic rarely discussed in literature, is universally derived from the K-t formalism in this thesis.$$$A spin-based simulator is an application with high demands to computing facilities even on modern hardware.$$$In this thesis, two approaches for a parallelized software architecture are designed, analysed and evaluated with regard to a reduction of execution time.$$$A number of possible applications in research and education are demonstrated.$$$For a choice of imaging experiments, results produced both experimentally and by simulation are compared.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS RESULTS RESULTS RESULTS METHODS BACKGROUND METHODS RESULTS RESULTS
D06314,"Multiple-antenna ""based"" transmitter (TX) cooperation has been established as a promising tool towards avoiding, aligning, or shaping the interference resulting from aggressive spectral reuse.$$$The price paid in the form of feedback and exchanging channel state information (CSI) between cooperating devices in most existing methods is often underestimated however.$$$In reality, feedback and information overhead threatens the practicality and scalability of TX cooperation approaches in dense networks.$$$Hereby we addresses a ""Who needs to know what?"" problem, when it comes to CSI at cooperating transmitters.$$$A comprehensive answer to this question remains beyond our reach and the scope of this paper.$$$Nevertheless, recent results in this area suggest that CSI overhead can be contained for even large networks provided the allocation of feedback to TXs is made non-uniform and to properly depend on the network's topology.$$$This paper provides a few hints toward solving the problem.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OTHERS BACKGROUND OBJECTIVES
D06720,"The power network reconfiguration algorithm with an ""R"" modeling approach evaluates its behavior in computing new reconfiguration topologies for the power grid in the context of the Smart Grid.$$$The power distribution network modelling with the R language is used to represent the network and support computation of different algorithm configurations for the evaluation of new reconfiguration topologies.$$$This work presents a reconfiguration solution of distribution networks, with a construction of an algorithm that receiving the network configuration data and the nodal measurements and from these data build a radial network, after this and using a branch exchange algorithm And verifying the best configuration of the network through artificial intelligence, so that there are no unnecessary changes during the operation, and applied an algorithm that analyses the load levels, to suggest changes in the network.",RESULTS RESULTS OBJECTIVES
D02345,"The Robinson-Goforth topology of swaps in adjoining payoffs elegantly arranges 2x2 ordinal games in accordance with important properties including symmetry, number of dominant strategies and Nash Equilibria, and alignment of interests.$$$Adding payoff families based on Nash Equilibria illustrates an additional aspect of this order and aids visualization of the topology.$$$Making ties through half-swaps not only creates simpler games within the topology, but, in reverse, breaking ties shows the evolution of preferences, yielding a natural ordering for the topology of 2x2 games with ties.$$$An ordinal game not only represents an equivalence class of games with real values, but also a discrete equivalent of the normalized version of those games.$$$The topology provides coordinates which could be used to identify related games in a semantic web ontology and facilitate comparative analysis of agent-based simulations and other research in game theory, as well as charting relationships and potential moves between games as a tool for institutional analysis and design.",BACKGROUND RESULTS RESULTS RESULTS OTHERS
D05800,"Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities.$$$A first step towards solving these tasks is the automated discovery of distributed symbol-like representations.$$$In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network.$$$Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities.$$$We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects.$$$We demonstrate that the learned representations are useful for next-step prediction.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS RESULTS
D06715,"A new stereoscopic image quality assessment database rendered using the 2D-image-plus-depth source, called MCL-3D, is described and the performance benchmarking of several known 2D and 3D image quality metrics using the MCL-3D database is presented in this work.$$$Nine image-plus-depth sources are first selected, and a depth image-based rendering (DIBR) technique is used to render stereoscopic image pairs.$$$Distortions applied to either the texture image or the depth image before stereoscopic image rendering include: Gaussian blur, additive white noise, down-sampling blur, JPEG and JPEG-2000 (JP2K) compression and transmission error.$$$Furthermore, the distortion caused by imperfect rendering is also examined.$$$The MCL-3D database contains 693 stereoscopic image pairs, where one third of them are of resolution 1024x728 and two thirds are of resolution 1920x1080.$$$The pair-wise comparison was adopted in the subjective test for user friendliness, and the Mean Opinion Score (MOS) can be computed accordingly.$$$Finally, we evaluate the performance of several 2D and 3D image quality metrics applied to MCL-3D.$$$All texture images, depth images, rendered image pairs in MCL-3D and their MOS values obtained in the subjective test are available to the public (http://mcl.usc.edu/mcl-3d-database/) for future research and development.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS OTHERS
D06368,"Generic word embeddings are trained on large-scale generic corpora; Domain Specific (DS) word embeddings are trained only on data from a domain of interest.$$$This paper proposes a method to combine the breadth of generic embeddings with the specificity of domain specific embeddings.$$$The resulting embeddings, called Domain Adapted (DA) word embeddings, are formed by aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA.$$$Evaluation results on sentiment classification tasks show that the DA embeddings substantially outperform both generic and DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for classification.",BACKGROUND OBJECTIVES METHODS RESULTS
D04556,"We consider the task of learning to estimate human pose in still images.$$$In order to avoid the high cost of full supervision, we propose to use a diverse data set, which consists of two types of annotations: (i) a small number of images are labeled using the expensive ground-truth pose; and (ii) other images are labeled using the inexpensive action label.$$$As action information helps narrow down the pose of a human, we argue that this approach can help reduce the cost of training without significantly affecting the accuracy.$$$To demonstrate this we design a probabilistic framework that employs two distributions: (i) a conditional distribution to model the uncertainty over the human pose given the image and the action; and (ii) a prediction distribution, which provides the pose of an image without using any action information.$$$We jointly estimate the parameters of the two aforementioned distributions by minimizing their dissimilarity coefficient, as measured by a task-specific loss function.$$$During both training and testing, we only require an efficient sampling strategy for both the aforementioned distributions.$$$This allows us to use deep probabilistic networks that are capable of providing accurate pose estimates for previously unseen images.$$$Using the MPII data set, we show that our approach outperforms baseline methods that either do not use the diverse annotations or rely on pointwise estimates of the pose.",OBJECTIVES OBJECTIVES BACKGROUND METHODS METHODS METHODS CONCLUSIONS RESULTS
D04784,"Recently, image-to-image translation has been made much progress owing to the success of conditional Generative Adversarial Networks (cGANs).$$$And some unpaired methods based on cycle consistency loss such as DualGAN, CycleGAN and DiscoGAN are really popular.$$$However, it's still very challenging for translation tasks with the requirement of high-level visual information conversion, such as photo-to-caricature translation that requires satire, exaggeration, lifelikeness and artistry.$$$We present an approach for learning to translate faces in the wild from the source photo domain to the target caricature domain with different styles, which can also be used for other high-level image-to-image translation tasks.$$$In order to capture global structure with local statistics while translation, we design a dual pathway model with one coarse discriminator and one fine discriminator.$$$For generator, we provide one extra perceptual loss in association with adversarial loss and cycle consistency loss to achieve representation learning for two different domains.$$$Also the style can be learned by the auxiliary noise input.$$$Experiments on photo-to-caricature translation of faces in the wild show considerable performance gain of our proposed method over state-of-the-art translation methods as well as its potential real applications.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D01121,"In this paper, we analyze the instability of continuous-time Markov jump linear systems.$$$Although there exist several effective criteria for the stability of Markov jump linear systems, there is a lack of methodologies for verifying their instability.$$$In this paper, we present a novel criterion for the exponential mean instability of Markov jump linear systems.$$$The main tool of our analysis is an auxiliary Markov jump linear system, which results from taking the Kronecker products of the given system matrices and a set of appropriate matrix weights.$$$We furthermore show that the problem of finding matrix weights for tighter instability analysis can be transformed to the spectral optimization of an affine matrix family, which can be efficiently performed by gradient-based non-smooth optimization algorithms.$$$We confirm the effectiveness of the proposed methods by numerical examples.",OBJECTIVES BACKGROUND OBJECTIVES METHODS RESULTS METHODS
D03994,"The availability of large-scale annotated image datasets coupled with recent advances in supervised deep learning methods are enabling the derivation of representative image features that can potentially impact different image analysis problems.$$$However, such supervised approaches are not feasible in the medical domain where it is challenging to obtain a large volume of labelled data due to the complexity of manual annotation and inter- and intra-observer variability in label assignment.$$$Algorithms designed to work on small annotated datasets are useful but have limited applications.$$$In an effort to address the lack of annotated data in the medical image analysis domain, we propose an algorithm for hierarchical unsupervised feature learning.$$$Our algorithm introduces three new contributions: (i) we use kernel learning to identify and represent invariant characteristics across image sub-patches in an unsupervised manner; (ii) we leverage the sparsity inherent to medical image data and propose a new sparse convolutional kernel network (S-CKN) that can be pre-trained in a layer-wise fashion, thereby providing initial discriminative features for medical data; and (iii) we propose a spatial pyramid pooling framework to capture subtle geometric differences in medical image data.$$$Our experiments evaluate our algorithm in two common application areas of medical image retrieval and classification using two public datasets.$$$Our results demonstrate that the medical image feature representations extracted with our algorithm enable a higher accuracy in both application areas compared to features extracted from other conventional unsupervised methods.$$$Furthermore, our approach achieves an accuracy that is competitive with state-of-the-art supervised CNNs.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS RESULTS
D05372,"Recent work has extended the theoretical analysis of boosting algorithms to multiclass problems and to online settings.$$$However, the multiclass extension is in the batch setting and the online extensions only consider binary classification.$$$We fill this gap in the literature by defining, and justifying, a weak learning condition for online multiclass boosting.$$$This condition leads to an optimal boosting algorithm that requires the minimal number of weak learners to achieve a certain accuracy.$$$Additionally, we propose an adaptive algorithm which is near optimal and enjoys an excellent performance on real data due to its adaptive property.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS
D04063,"Many problems in NLP require aggregating information from multiple mentions of the same entity which may be far apart in the text.$$$Existing Recurrent Neural Network (RNN) layers are biased towards short-term dependencies and hence not suited to such tasks.$$$We present a recurrent layer which is instead biased towards coreferent dependencies.$$$The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster.$$$Incorporating this layer into a state-of-the-art reading comprehension model improves performance on three datasets -- Wikihop, LAMBADA and the bAbi AI tasks -- with large gains when training data is scarce.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS
D01069,"Multipath routing is a trivial way to exploit the path diversity to leverage the network throughput.$$$Technologies such as OSPF ECMP use all the available paths in the network to forward traffic, however, we argue that is not necessary to do so to load balance the network.$$$In this paper, we consider multipath routing with only a limited number of end-to-end paths for each source and destination, and found that this can still load balance the traffic.$$$We devised an algorithm to select a few paths for each source-destination pair so that when all traffic are forwarded over these paths, we can achieve a balanced load in the sense that the maximum link utilization is comparable to that of ECMP forwarding.$$$When the constraint of only shortest paths (i.e. equal paths) are relaxed, we can even outperform ECMP in certain cases.$$$As a result, we can use a few end-to-end tunnels between each source and destination nodes to achieve the load balancing of traffic.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D00473,"The finite element method (FEM) has several computational steps to numerically solve a particular problem, to which many efforts have been directed to accelerate the solution stage of the linear system of equations.$$$However, the finite element matrix construction, which is also time-consuming for unstructured meshes, has been less investigated.$$$The generation of the global finite element matrix is performed in two steps, computing the local matrices by numerical integration and assembling them into a global system, which has traditionally been done in serial computing.$$$This work presents a fast technique to construct the global finite element matrix that arises by solving the Poisson's equation in a three-dimensional domain.$$$The proposed methodology consists in computing the numerical integration, due to its intrinsic parallel opportunities, in the graphics processing unit (GPU) and computing the matrix assembly, due to its intrinsic serial operations, in the central processing unit (CPU).$$$In the numerical integration, only the lower triangular part of each local stiffness matrix is computed thanks to its symmetry, which saves GPU memory and computing time.$$$As a result of symmetry, the global sparse matrix also contains non-zero elements only in its lower triangular part, which reduces the assembly operations and memory usage.$$$This methodology allows generating the global sparse matrix from any unstructured finite element mesh size on GPUs with little memory capacity, only limited by the CPU memory.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D05219,"Environment perception is an important task with great practical value and bird view is an essential part for creating panoramas of surrounding environment.$$$Due to the large gap and severe deformation between the frontal view and bird view, generating a bird view image from a single frontal view is challenging.$$$To tackle this problem, we propose the BridgeGAN, i.e., a novel generative model for bird view synthesis.$$$First, an intermediate view, i.e., homography view, is introduced to bridge the large gap.$$$Next, conditioned on the three views (frontal view, homography view and bird view) in our task, a multi-GAN based model is proposed to learn the challenging cross-view translation.$$$Extensive experiments conducted on a synthetic dataset have demonstrated that the images generated by our model are much better than those generated by existing methods, with more consistent global appearance and sharper details.$$$Ablation studies and discussions show its reliability and robustness in some challenging cases.",OBJECTIVES BACKGROUND METHODS METHODS METHODS RESULTS RESULTS
D05949,"A new set of parameters to describe the word frequency behavior of texts is proposed.$$$The analogy between the word frequency distribution and the Bose-distribution is suggested and the notion of ""temperature"" is introduced for this case.$$$The calculations are made for English, Ukrainian, and the Guinean Maninka languages.$$$The correlation between in-deep language structure (the level of analyticity) and the defined parameters is shown to exist.",BACKGROUND/OBJECTIVES METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D03957,"Software defect prediction is an important aspect of preventive maintenance of a software.$$$Many techniques have been employed to improve software quality through defect prediction.$$$This paper introduces an approach of defect prediction through a machine learning algorithm, support vector machines (SVM), by using the code smells as the factor.$$$Smell prediction model based on support vector machines was used to predict defects in the subsequent releases of the eclipse software.$$$The results signify the role of smells in predicting the defects of a software.$$$The results can further be used as a baseline to investigate further the role of smells in predicting defects.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS CONCLUSIONS
D00956,"The great variations of videographic skills, camera designs, compression and processing protocols, and displays lead to an enormous variety of video impairments.$$$Current no-reference (NR) video quality models are unable to handle this diversity of distortions.$$$This is true in part because available video quality assessment databases contain very limited content, fixed resolutions, were captured using a small number of camera devices by a few videographers and have been subjected to a modest number of distortions.$$$As such, these databases fail to adequately represent real world videos, which contain very different kinds of content obtained under highly diverse imaging conditions and are subject to authentic, often commingled distortions that are impossible to simulate.$$$As a result, NR video quality predictors tested on real-world video data often perform poorly.$$$Towards advancing NR video quality prediction, we constructed a large-scale video quality assessment database containing 585 videos of unique content, captured by a large number of users, with wide ranges of levels of complex, authentic distortions.$$$We collected a large number of subjective video quality scores via crowdsourcing.$$$A total of 4776 unique participants took part in the study, yielding more than 205000 opinion scores, resulting in an average of 240 recorded human opinions per video.$$$We demonstrate the value of the new resource, which we call the LIVE Video Quality Challenge Database (LIVE-VQC), by conducting a comparison of leading NR video quality predictors on it.$$$This study is the largest video quality assessment study ever conducted along several key dimensions: number of unique contents, capture devices, distortion types and combinations of distortions, study participants, and recorded subjective scores.$$$The database is available for download on this link: http://live.ece.utexas.edu/research/LIVEVQC/index.html .",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D04459,"Issues regarding explainable AI involve four components: users, laws & regulations, explanations and algorithms.$$$Together these components provide a context in which explanation methods can be evaluated regarding their adequacy.$$$The goal of this chapter is to bridge the gap between expert users and lay users.$$$Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks (DNNs), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified.$$$Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output.$$$However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy.$$$Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased DNNs, as well as the suspicion about unfair outcomes.",CONCLUSIONS OTHERS OBJECTIVES METHODS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D05782,"Convolutional Siamese neural networks have been recently used to track objects using deep features.$$$Siamese architecture can achieve real time speed, however it is still difficult to find a Siamese architecture that maintains the generalization capability, high accuracy and speed while decreasing the number of shared parameters especially when it is very deep.$$$Furthermore, a conventional Siamese architecture usually processes one local neighborhood at a time, which makes the appearance model local and non-robust to appearance changes.$$$To overcome these two problems, this paper proposes DensSiam, a novel convolutional Siamese architecture, which uses the concept of dense layers and connects each dense layer to all layers in a feed-forward fashion with a similarity-learning function.$$$DensSiam also includes a Self-Attention mechanism to force the network to pay more attention to the non-local features during offline training.$$$Extensive experiments are performed on four tracking benchmarks: OTB2013 and OTB2015 for validation set; and VOT2015, VOT2016 and VOT2017 for testing set.$$$The obtained results show that DensSiam achieves superior results on these benchmarks compared to other current state-of-the-art methods.",BACKGROUND BACKGROUND BACKGROUND METHODS METHODS RESULTS CONCLUSIONS
D04875,"Rapid miniaturization and cost reduction of computing, along with the availability of wearable and implantable physiological sensors have led to the growth of human Body Area Network (BAN) formed by a network of such sensors and computing devices.$$$One promising application of such a network is wearable health monitoring where the collected data from the sensors would be transmitted and analyzed to assess the health of a person.$$$Typically, the devices in a BAN are connected through wireless (WBAN), which suffers from energy inefficiency due to the high-energy consumption of wireless transmission.$$$Human Body Communication (HBC) uses the relatively low loss human body as the communication medium to connect these devices, promising order(s) of magnitude better energy-efficiency and built-in security compared to WBAN.$$$In this paper, we demonstrate a health monitoring device and system built using Commercial-Off-The- Shelf (COTS) sensors and components, that can collect data from physiological sensors and transmit it through a) intra-body HBC to another device (hub) worn on the body or b) upload health data through HBC-based human-machine interaction to an HBC capable machine.$$$The system design constraints and signal transfer characteristics for the implemented HBC-based wearable health monitoring system are measured and analyzed, showing reliable connectivity with >8x power savings compared to Bluetooth lowenergy (BTLE).",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D04304,"Scissor lifts, a staple of mechanical design, especially in competitive robotics, are a type of linkage that can be used to raise a load to some height, when acted upon by some force, usually exerted by an actuator.$$$The position of this actuator, however, can affect the mechanical advantage and velocity ratio of the system.$$$Hence, there needs to be a concrete way to analytically compare different actuator positions.$$$However, all current research into the analysis of scissor lifts either focusses only on the screw jack configuration, or derives separate force expressions for different actuator positions.$$$This, once again, leaves the decision between different actuator positions to trial and error, since the expression to test the potency of the position can only be derived once the position is chosen.$$$This paper proposes a derivation for a general force expression, in terms of a few carefully chosen position variables, which can be used to generate the force expression for any actuator position.$$$Hence, this expression illustrates exactly how each of the position variables (called a, b and i in this paper, as defined later) affect the force output, and hence can be used to pick an appropriate actuator position, by choosing values for the position variables that give the desired result.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS/RESULTS RESULTS
D02337,"Time synchronization is important for a variety of applications in wireless sensor networks including scheduling communication resources, coordinating sensor wake/sleep cycles, and aligning signals for distributed transmission/reception.$$$This paper describes a non-hierarchical approach to time synchronization in wireless sensor networks that has low overhead and can be implemented at the physical and/or MAC layers.$$$Unlike most of the prior approaches, the approach described in this paper allows all nodes to use exactly the same distributed algorithm and does not require local averaging of measurements from other nodes.$$$Analytical results show that the non-hierarchical approach can provide monotonic expected convergence of both drifts and offsets under broad conditions on the network topology and local clock update stepsize.$$$Numerical results are also presented verifying the analysis under two particular network topologies.",BACKGROUND METHODS METHODS RESULTS RESULTS
D00369,"Analysis of informative contents and sentiments of social users has been attempted quite intensively in the recent past.$$$Most of the systems are usable only for monolingual data and fails or gives poor results when used on data with code-mixing property.$$$To gather attention and encourage researchers to work on this crisis, we prepared gold standard Bengali-English code-mixed data with language and polarity tag for sentiment analysis purposes.$$$In this paper, we discuss the systems we prepared to collect and filter raw Twitter data.$$$In order to reduce manual work while annotation, hybrid systems combining rule based and supervised models were developed for both language and sentiment tagging.$$$The final corpus was annotated by a group of annotators following a few guidelines.$$$The gold standard corpus thus obtained has impressive inter-annotator agreement obtained in terms of Kappa values.$$$Various metrics like Code-Mixed Index (CMI), Code-Mixed Factor (CF) along with various aspects (language and emotion) also qualitatively polled the code-mixed and sentiment properties of the corpus.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D04910,"Today it is crucial for organizations to pay even greater attention on quality management as the importance of this function in achieving ultimate business objectives is increasingly becoming clearer.$$$Importance of the Quality Management Function in achieving basic need by ensuring compliance with Capability Maturity Model Integrated or International Organization for Standardization is a basic demand from business nowadays.$$$However, Quality Management Function and its processes need to be made much more mature to prevent delivery outages and to achieve business excellence through their review and auditing capability.$$$Many organizations now face challenges in determining the maturity of the Quality Management group along with the service offered by them and the right way to elevate the maturity of the same.$$$The objective of this whitepaper is to propose a new model, the Audit Maturity Model which will provide organizations with a measure of their maturity in quality management in the perspective of auditing, along with recommendations for preventing delivery outage, and identifying risk to achieve business excellence.$$$This will enable organizations to assess Quality Management maturity higher than basic hygiene and will also help them to identify gaps and to take corrective actions for achieving higher maturity levels.$$$Hence the objective is to envisage a new auditing model as a part of organisation quality management function which can be a guide for them to achieve higher level of maturity and ultimately help to achieve delivery and business excellence.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES/METHODS OBJECTIVES/METHODS OBJECTIVES/METHODS
D01806,"We describe the LoopInvGen tool for generating loop invariants that can provably guarantee correctness of a program with respect to a given specification.$$$LoopInvGen is an efficient implementation of the inference technique originally proposed in our earlier work on PIE (https://doi.org/10.1145/2908080.2908099).$$$In contrast to existing techniques, LoopInvGen is not restricted to a fixed set of features -- atomic predicates that are composed together to build complex loop invariants.$$$Instead, we start with no initial features, and use program synthesis techniques to grow the set on demand.$$$This not only enables a less onerous and more expressive approach, but also appears to be significantly faster than the existing tools over the SyGuS-COMP 2017 benchmarks from the INV track.",OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D00352,"Smart city projects address many of the current problems afflicting high populated areas and cities and, as such, are a target for government, institutions and private organizations that plan to explore its foreseen advantages.$$$In technical terms, smart city projects present a complex set of requirements including a large number users with highly different and heterogeneous requirements.$$$In this scenario, this paper proposes and analyses the impact and perspectives on adopting software-defined networking and artificial intelligence as innovative approaches for smart city project development and deployment.$$$Big data is also considered as an inherent element of most smart city project that must be tackled.$$$A framework layered view is proposed with a discussion about software-defined networking and machine learning impacts on innovation followed by a use case that demonstrates the potential benefits of cognitive learning for smart cities.$$$It is argued that the complexity of smart city projects do require new innovative approaches that potentially result in more efficient and intelligent systems.",BACKGROUND BACKGROUND OBJECTIVES OTHERS RESULTS CONCLUSIONS
D04340,"In model-based reinforcement learning, generative and temporal models of environments can be leveraged to boost agent performance, either by tuning the agent's representations during training or via use as part of an explicit planning mechanism.$$$However, their application in practice has been limited to simplistic environments, due to the difficulty of training such models in larger, potentially partially-observed and 3D environments.$$$In this work we introduce a novel action-conditioned generative model of such challenging environments.$$$The model features a non-parametric spatial memory system in which we store learned, disentangled representations of the environment.$$$Low-dimensional spatial updates are computed using a state-space model that makes use of knowledge on the prior dynamics of the moving agent, and high-dimensional visual observations are modelled with a Variational Auto-Encoder.$$$The result is a scalable architecture capable of performing coherent predictions over hundreds of time steps across a range of partially observed 2D and 3D environments.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D02940,"This paper aims at presenting the basic functionality of a radar platform for real-time monitoring of displacement and vibration.$$$The real time capabilities make the radar platform useful when live monitoring of targets is required.$$$The system is based on the RF analog front-end of an USRP, and the range compression (time-domain cross-correlation) is implemented on the FPGA included in the USRP.$$$Further processing is performed on the host computer to plot real time range profiles, displacements, vibration frequencies spectra and spectrograms (waterfall plots) for long term monitoring.$$$The system is currently in experimental form and the present paper aims at proving its functionality.$$$The precision of this system is estimated at 0.6 mm for displacement measurements and 1.8 mm for vibration amplitude measurements.",OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS
D05549,"Avionics is one kind of domain where prevention prevails.$$$Nonetheless fails occur.$$$Sometimes due to pilot misreacting, flooded in information.$$$Sometimes information itself would be better verified than trusted.$$$To avoid some kind of failure, it has been thought to add,in midst of the ARINC664 aircraft data network, a new kind of monitoring.",BACKGROUND OTHERS OTHERS RESULTS/CONCLUSIONS OBJECTIVES/METHODS
D05197,"Multi-view image-based rendering consists in generating a novel view of a scene from a set of source views.$$$In general, this works by first doing a coarse 3D reconstruction of the scene, and then using this reconstruction to establish correspondences between source and target views, followed by blending the warped views to get the final image.$$$Unfortunately, discontinuities in the blending weights, due to scene geometry or camera placement, result in artifacts in the target view.$$$In this paper, we show how to avoid these artifacts by imposing additional constraints on the image gradients of the novel view.$$$We propose a variational framework in which an energy functional is derived and optimized by iteratively solving a linear system.$$$We demonstrate this method on several structured and unstructured multi-view datasets, and show that it numerically outperforms state-of-the-art methods, and eliminates artifacts that result from visibility discontinuities",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D06785,"The Information Flow Framework (IFF) is a descriptive category metatheory currently under development, which is being offered as the structural aspect of the Standard Upper Ontology (SUO).$$$The architecture of the IFF is composed of metalevels, namespaces and meta-ontologies.$$$The main application of the IFF is institutional: the notion of institutions and their morphisms are being axiomatized in the upper metalevels of the IFF, and the lower metalevel of the IFF has axiomatized various institutions in which semantic integration has a natural expression as the colimit of theories.",BACKGROUND/OBJECTIVES METHODS RESULTS/CONCLUSIONS
D04538,"Tremendous technology development in the field of Internet of Things (IoT) has changed the way we work and live.$$$Although the numerous advantages of IoT are enriching our society, it should be reminded that the IoT also consumes energy, embraces toxic pollution and E-waste.$$$These place new stress on the environments and smart world.$$$In order to increase the benefits and reduce the harm of IoT, there is an increasing desire to move toward green IoT.$$$Green IoT is seen as the future of IoT that is environmentally friendly.$$$To achieve that, it is necessary to put a lot of measures to reduce carbon footprint, conserve fewer resources, and promote efficient techniques for energy usage.$$$It is the reason for moving towards green IoT, where the machines, communications, sensors, clouds, and internet are alongside energy efficiency and reducing carbon emission.$$$This paper presents a thorough survey of the current on-going research work and potential technologies of green IoT with an intention to provide some clues for future green IoT research.",BACKGROUND BACKGROUND RESULTS OBJECTIVES CONCLUSIONS OBJECTIVES METHODS CONCLUSIONS
D02694,"Hybrid beamforming (HB) has been widely studied for reducing the number of costly radio frequency (RF) chains in massive multiple-input multiple-output (MIMO) systems.$$$However, previous works on HB are limited to a single user equipment (UE) or a single group of UEs, employing the frequency-flat first-level analog beamforming (AB) that cannot be applied to multiple groups of UEs served in different frequency resources in an orthogonal frequency-division multiplexing (OFDM) system.$$$In this paper, a novel HB algorithm with unified AB based on the spatial covariance matrix (SCM) knowledge of all UEs is proposed for a massive MIMO-OFDM system in order to support multiple groups of UEs.$$$The proposed HB method with a much smaller number of RF chains can achieve more than 95% performance of full digital beamforming.$$$In addition, a novel practical subspace construction (SC) algorithm based on partial channel state information is proposed to estimate the required SCM.$$$The proposed SC method can offer more than 97% performance of the perfect SCM case.$$$With the proposed methods, significant cost and power savings can be achieved without large loss in performance.$$$Furthermore, the proposed methods can be applied to massive MIMO-OFDM systems in both time-division duplex and frequency-division duplex.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS OBJECTIVES/METHODS RESULTS CONCLUSIONS CONCLUSIONS
D04809,"We introduce EigenRec; a versatile and efficient Latent-Factor framework for Top-N Recommendations that includes the well-known PureSVD algorithm as a special case.$$$EigenRec builds a low dimensional model of an inter-item proximity matrix that combines a similarity component, with a scaling operator, designed to control the influence of the prior item popularity on the final model.$$$Seeing PureSVD within our framework provides intuition about its inner workings, exposes its inherent limitations, and also, paves the path towards painlessly improving its recommendation performance.$$$A comprehensive set of experiments on the MovieLens and the Yahoo datasets based on widely applied performance metrics, indicate that EigenRec outperforms several state-of-the-art algorithms, in terms of Standard and Long-Tail recommendation accuracy, exhibiting low susceptibility to sparsity, even in its most extreme manifestations -- the Cold-Start problems.$$$At the same time EigenRec has an attractive computational profile and it can apply readily in large-scale recommendation settings.",OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06487,"Millimeter-wave (mmWave) communications have been considered as a key technology for future 5G wireless networks because of the orders-of-magnitude wider bandwidth than current cellular bands.$$$In this paper, we consider the problem of codebook-based joint analog-digital hybrid precoder and combiner design for spatial multiplexing transmission in a mmWave multiple-input multiple-output (MIMO) system.$$$We propose to jointly select analog precoder and combiner pair for each data stream successively aiming at maximizing the channel gain while suppressing the interference between different data streams.$$$After all analog precoder/combiner pairs have been determined, we can obtain the effective baseband channel.$$$Then, the digital precoder and combiner are computed based on the obtained effective baseband channel to further mitigate the interference and maximize the sum-rate.$$$Simulation results demonstrate that our proposed algorithm exhibits prominent advantages in combating interference between different data streams and offer satisfactory performance improvement compared to the existing codebook-based hybrid beamforming schemes.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D01131,"This article is an empirical contribution to the field of educational technology but also - and above all - a methodological contribution to the analysis of the activities enacted in this field.$$$It takes account of a pilot study conducted within the framework of doctoral research and consisted in describing, analysing and modelling the activity of a trainee teacher in a situation of autonomous use of a video-based digital learning environment (DLE).$$$We were particularly careful to describe the method in great detail.$$$Two types of data were collected and processed within the framework of ""course-of-action"": (i)activity observation data (dynamic screen capture) and (ii) data from resituating interviews supported by digital traces of that activity.$$$The findings (i) validate the method's relevance in relation to the object and issues of the research, (ii)show different levels of organization in the activity deployed in the situation of use, (iii) highlight four registers of concerns orienting use of the DLE.$$$We conclude from a perspective of educational technology, by discussing how, according to certain conditions and different time scales, the findings inform a process of continuous DLE design.",OTHERS OBJECTIVES OBJECTIVES METHODS RESULTS CONCLUSIONS
D00193,"The ultimate goal of this indoor mapping research is to automatically reconstruct a floorplan simply by walking through a house with a smartphone in a pocket.$$$This paper tackles this problem by proposing FloorNet, a novel deep neural architecture.$$$The challenge lies in the processing of RGBD streams spanning a large 3D space.$$$FloorNet effectively processes the data through three neural network branches: 1) PointNet with 3D points, exploiting the 3D information; 2) CNN with a 2D point density image in a top-down view, enhancing the local spatial reasoning; and 3) CNN with RGB images, utilizing the full image information.$$$FloorNet exchanges intermediate features across the branches to exploit the best of all the architectures.$$$We have created a benchmark for floorplan reconstruction by acquiring RGBD video streams for 155 residential houses or apartments with Google Tango phones and annotating complete floorplan information.$$$Our qualitative and quantitative evaluations demonstrate that the fusion of three branches effectively improves the reconstruction quality.$$$We hope that the paper together with the benchmark will be an important step towards solving a challenging vector-graphics reconstruction problem.$$$Code and data are available at https://github.com/art-programmer/FloorNet.",OBJECTIVES METHODS BACKGROUND METHODS METHODS RESULTS RESULTS CONCLUSIONS RESULTS
D05648,"We study a discrete model of repelling particles, and we show using linear programming bounds that many familiar families of error-correcting codes minimize a broad class of potential energies when compared with all other codes of the same size and block length.$$$Examples of these universally optimal codes include Hamming, Golay, and Reed-Solomon codes, among many others, and this helps explain their robustness as the channel model varies.$$$Universal optimality of these codes is equivalent to minimality of their binomial moments, which has been proved in many cases by Ashikhmin and Barg.$$$We highlight connections with mathematical physics and the analogy between these results and previous work by Cohn and Kumar in the continuous setting, and we develop a framework for optimizing the linear programming bounds.$$$Furthermore, we show that if these bounds prove a code is universally optimal, then the code remains universally optimal even if one codeword is removed.",RESULTS CONCLUSIONS BACKGROUND METHODS RESULTS
D02909,"A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance.$$$However, just as we expect humans to be ethical, the same expectation needs to be met by automated systems that increasingly get delegated to act on their behalf.$$$A very important aspect of an ethical behavior is to avoid (intended, perceived, or accidental) bias.$$$Bias occurs when the data distribution is not representative enough of the natural phenomenon one wants to model and reason about.$$$The possibly biased behavior of a service is hard to detect and handle if the AI service is merely being used and not developed from scratch, since the training data set is not available.$$$In this situation, we envisage a 3rd party rating agency that is independent of the API producer or consumer and has its own set of biased and unbiased data, with customizable distributions.$$$We propose a 2-step rating approach that generates bias ratings signifying whether the AI service is unbiased compensating, data-sensitive biased, or biased.$$$The approach also works on composite services.$$$We implement it in the context of text translation and report interesting results.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS OBJECTIVES METHODS CONCLUSIONS RESULTS
D06374,"In most studies for the quantification of the third thermodynamic law, the minimum temperature which can be achieved with a long but finite-time process scales as a negative power of the process duration.$$$In this article, we use our recent complete solution for the optimal control problem of the quantum parametric oscillator to show that the minimum temperature which can be obtained in this system scales exponentially with the available time.$$$The present work is expected to motivate further research in the active quest for absolute zero.",BACKGROUND METHODS/RESULTS/CONCLUSIONS OTHERS
D04355,"In this work, we explore the outage probability (OP) analysis of selective decode and forward (SDF) cooperation protocol employing multiple-input multipleoutput (MIMO) orthogonal space-time block-code (OSTBC) over time varying Rayleigh fading channel conditions with imperfect channel state information (CSI) and mobile nodes.$$$The closed-form expressions of the per-block average OP, probability distribution function (PDF) of sum of independent and identically distributed (i.i.d.)$$$Gamma random variables (RVs), and cumulative distribution function (CDF) are derived and used to investigate the performance of the relaying network.$$$A mathematical framework is developed to derive the optimal source-relay power allocation factors.$$$It is shown that source node mobility affects the per-block average OP performance more significantly than the destination node mobility.$$$Nevertheless, in other node mobility situations, cooperative systems are constrained by an error floor with a higher signal to noise ratio (SNR) regimes.$$$Simulation results show that the equal power allocation is the only possible optimal solution when source to relay link is stronger than the relay to destination link.$$$Also, we allocate almost all the power to the source node when source to relay link is weaker than the relay to destination link.$$$Simulation results also show that OP simulated plots are in close agreement with the OP analytic plots at high SNR regimes.",BACKGROUND OBJECTIVES RESULTS METHODS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D06239,"Symbolic and logic computation systems ranging from computer algebra systems to theorem provers are finding their way into science, technology, mathematics and engineering.$$$But such systems rely on explicitly or implicitly represented mathematical knowledge that needs to be managed to use such systems effectively.$$$While mathematical knowledge management (MKM) ""in the small"" is well-studied, scaling up to large, highly interconnected corpora remains difficult.$$$We hold that in order to realize MKM ""in the large"", we need representation languages and software architectures that are designed systematically with large-scale processing in mind.$$$Therefore, we have designed and implemented the MMT language -- a module system for mathematical theories.$$$MMT is designed as the simplest possible language that combines a module system, a foundationally uncommitted formal semantics, and web-scalable implementations.$$$Due to a careful choice of representational primitives, MMT allows us to integrate existing representation languages for formal mathematical knowledge in a simple, scalable formalism.$$$In particular, MMT abstracts from the underlying mathematical and logical foundations so that it can serve as a standardized representation format for a formal digital library.$$$Moreover, MMT systematically separates logic-dependent and logic-independent concerns so that it can serve as an interface layer between computation systems and MKM systems.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D04715,"A new phenomenon emerging within virtual communities is a blurring between the social and commercial activities and motivations of participants.$$$This paper explores motivations for participating in social commerce at a micro-business level between members of a virtual community of Malay lifestyle bloggers.$$$The selected community was observed in order to understand the community and 21 participants were interviewed.$$$We used laddering techniques to explore community attributes, the perceived consequences, and their links to the values of participants.$$$We found that virtual community relationship was the main influential factor, and virtual community relationship contributed to the sense of social support as well as customers' trust in social commerce.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D04881,"We introduce a universe of regular datatypes with variable binding information, for which we define generic formation and elimination (i.e. induction /recursion) operators.$$$We then define a generic alpha-equivalence relation over the types of the universe based on name-swapping, and derive iteration and induction principles which work modulo alpha-conversion capturing Barendregt's Variable Convention.$$$We instantiate the resulting framework so as to obtain the Lambda Calculus and System F, for which we derive substitution operations and substitution lemmas for alpha-conversion and substitution composition.$$$The whole work is carried out in Constructive Type Theory and machine-checked by the system Agda.",OBJECTIVES RESULTS RESULTS METHODS
D06330,"A simple proof is given for the monotonicity of entropy and Fisher information associated to sums of i.i.d. random variables.$$$The proof relies on a characterization of maximal correlation for partial sums due to Dembo, Kagan and Shepp.",RESULTS/CONCLUSIONS METHODS
D05275,"Developing an appropriate design process for a conceptual model is a stepping stone toward designing car bodies.$$$This paper presents a methodology to design a lightweight and modular space frame chassis for a sedan electric car.$$$The dual phase high strength steel with improved mechanical properties is employed to reduce the weight of the car body.$$$Utilizing the finite element analysis yields two models in order to predict the performance of each component.$$$The first model is a beam structure with a rapid response in structural stiffness simulation.$$$This model is used for performing the static tests including modal frequency, bending stiffens and torsional stiffness evaluation.$$$Whereas the second model, i.e., a shell model, is proposed to illustrate every module's mechanical behavior as well as its crashworthiness efficiency.$$$In order to perform the crashworthiness analysis, the explicit nonlinear dynamic solver provided by ABAQUS, a commercial finite element software, is used.$$$The results of finite element beam and shell models are in line with the concept design specifications.$$$Implementation of this procedure leads to generate a lightweight and modular concept for an electric car.",BACKGROUND METHODS BACKGROUND METHODS RESULTS RESULTS RESULTS METHODS RESULTS CONCLUSIONS
D00842,"Unmanned Aerial Vehicles (UAVs) have been recently considered as means to provide enhanced coverage or relaying services to mobile users (MUs) in wireless systems with limited or no infrastructure.$$$In this paper, a UAV-based mobile cloud computing system is studied in which a moving UAV is endowed with computing capabilities to offer computation offloading opportunities to MUs with limited local processing capabilities.$$$The system aims at minimizing the total mobile energy consumption while satisfying quality of service requirements of the offloaded mobile application.$$$Offloading is enabled by uplink and downlink communications between the mobile devices and the UAV that take place by means of frequency division duplex (FDD) via orthogonal or non-orthogonal multiple access (NOMA) schemes.$$$The problem of jointly optimizing the bit allocation for uplink and downlink communication as well as for computing at the UAV, along with the cloudlet's trajectory under latency and UAV's energy budget constraints is formulated and addressed by leveraging successive convex approximation (SCA) strategies.$$$Numerical results demonstrate the significant energy savings that can be accrued by means of the proposed joint optimization of bit allocation and cloudlet's trajectory as compared to local mobile execution as well as to partial optimization approaches that design only the bit allocation or the cloudlet's trajectory.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D06358,"Millimeter wave (mm-wave) and massive MIMO have been proposed for next generation wireless systems.$$$However, there are many open problems for the implementation of those technologies.$$$In particular, beamforming is necessary in mm-wave systems in order to counter high propagation losses.$$$However, conventional beamsteering is not always appropriate in rich scattering multipath channels with frequency selective fading, such as those found in indoor environments.$$$In this context, time-reversal (TR) is considered a promising beamforming technique for such mm-wave massive MIMO systems.$$$In this paper, we analyze a baseband TR beamforming system for mm-wave multi-user massive MIMO.$$$We verify that, as the number of antennas increases, TR yields good equalization and interference mitigation properties, but inter-user interference (IUI) remains a main impairment.$$$Thus, we propose a novel technique called interference-nulling TR (INTR) to minimize IUI.$$$We evaluate numerically the performance of INTR and compare it with conventional TR and equalized TR beamforming.$$$We use a 60 GHz MIMO channel model with spatial correlation based on the IEEE 802.11ad SISO NLoS model.$$$We demonstrate that INTR outperforms conventional TR with respect to average BER per user and achievable sum rate under diverse conditions, providing both diversity and multiplexing gains simultaneously.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS OBJECTIVES/RESULTS METHODS METHODS CONCLUSIONS
D04386,"Moldable tasks allow schedulers to determine the number of processors assigned to a task, enabling efficient use of large-scale parallel processing systems.$$$A generic assumption is that every task is monotonic, i.e., its workload increases but its execution time decreases as the number of assigned processors increases.$$$In this paper, we study the problem of scheduling moldable tasks on processors.$$$Motivated by many benchmark studies, we introduce a new speedup model: it is linear when the number of assigned processors is small, up to some threshold; then, it possibly declines and even become negative as the number increases.$$$Given any threshold value achievable, we propose a generic approximation algorithm to minimize the makespan, which is simpler and achieves a better performance guarantee than the existing ones under the monotonic assumption.$$$As a by-product, we also propose an approximation algorithm to maximize the sum of values of tasks completed by a deadline; this scheduling objective is considered for moldable tasks for the first time while similar works have been done for other types of parallel tasks.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS RESULTS
D03489,"We study a general class of dynamic games with asymmetric information where agents' beliefs are strategy dependent, i.e. signaling occurs.$$$We show that the notion of sufficient information, introduced in the companion paper team, can be used to effectively compress the agents' information in a mutually consistent manner that is sufficient for decision-making purposes.$$$We present instances of dynamic games with asymmetric information where we can characterize a time-invariant information state for each agent.$$$Based on the notion of sufficient information, we define a class of equilibria for dynamic games called Sufficient Information Based Perfect Bayesian Equilibrium (SIB-PBE).$$$Utilizing the notion of SIB-PBE, we provide a sequential decomposition of dynamic games with asymmetric information over time; this decomposition leads to a dynamic program that determines SIB-PBE of dynamic games.$$$Furthermore, we provide conditions under which we can guarantee the existence of SIB-PBE.",OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS
D05353,"In this study, we combine bibliometric techniques with a machine learning algorithm, the sequential Information Bottleneck, to assess the interdisciplinarity of research produced by the University of Hawaii NASA Astrobiology Institute (UHNAI).$$$In particular, we cluster abstract data to evaluate Thomson Reuters Web of Knowledge subject categories as descriptive labels for astrobiology documents, assess individual researcher interdisciplinarity, and determine where collaboration opportunities might occur.$$$We find that the majority of the UHNAI team is engaged in interdisciplinary research, and suggest that our method could be applied to additional NASA Astrobiology Institute teams in particular, or other interdisciplinary research teams more broadly, to identify and facilitate collaboration opportunities.",BACKGROUND/OBJECTIVES OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D02113,"Different neural networks trained on the same dataset often learn similar input-output mappings with very different weights.$$$Is there some correspondence between these neural network solutions?$$$For linear networks, it has been shown that different instances of the same network architecture encode the same representational similarity matrix, and their neural activity patterns are connected by orthogonal transformations.$$$However, it is unclear if this holds for non-linear networks.$$$Using a shared response model, we show that different neural networks encode the same input examples as different orthogonal transformations of an underlying shared representation.$$$We test this claim using both standard convolutional neural networks and residual networks on CIFAR10 and CIFAR100.",BACKGROUND OBJECTIVES BACKGROUND BACKGROUND METHODS/RESULTS/CONCLUSIONS METHODS/RESULTS
D04258,"Online recommender systems often deal with continuous, potentially fast and unbounded flows of data.$$$Ensemble methods for recommender systems have been used in the past in batch algorithms, however they have never been studied with incremental algorithms that learn from data streams.$$$We evaluate online bagging with an incremental matrix factorization algorithm for top-N recommendation with positive-only -- binary -- ratings.$$$Our results show that online bagging is able to improve accuracy up to 35% over the baseline, with small computational overhead.",BACKGROUND BACKGROUND METHODS RESULTS
D06959,"While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost.$$$This is particularly important for deep learning since these learners need hours (to weeks) to train the model.$$$Such long training time limits the ability of (a)~a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)~other researchers to repeat, improve, or even refute that original work.$$$For example, recently, deep learning was used to find which questions in the Stack Overflow programmer discussion forum can be linked together.$$$That deep learning system took 14 hours to execute.$$$We show here that applying a very simple optimizer called DE to fine tune SVM, it can achieve similar (and sometimes better) results.$$$The DE approach terminated in 10 minutes; i.e.$$$84 times faster hours than deep learning method.$$$We offer these results as a cautionary tale to the software analytics community and suggest that not every new innovation should be applied without critical analysis.$$$If researchers deploy some new and expensive process, that work should be baselined against some simpler and faster alternatives.",BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS/RESULTS METHODS/RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D04446,"Communication systems for multicasting information and energy simultaneously to more than one user are investigated.$$$In the system under study, a transmitter sends the same message and signal to multiple receivers over distinct and independent channels.$$$In this setting, results for compound channels are applied to relate the operational compound capacity to the informational measurements.$$$The fundamental limit under a received energy constraint, called the multicast capacity-energy function, is studied and a single-letter expression is derived.$$$The ideas are illustrated via a numerical example with two receivers.$$$The problem of receiver segmentation, in which the receivers are divided into several groups, is also considered.",OBJECTIVES OBJECTIVES METHODS METHODS/RESULTS RESULTS OBJECTIVES
D00179,"Word reordering is one of the most difficult aspects of statistical machine translation (SMT), and an important factor of its quality and efficiency.$$$Despite the vast amount of research published to date, the interest of the community in this problem has not decreased, and no single method appears to be strongly dominant across language pairs.$$$Instead, the choice of the optimal approach for a new translation task still seems to be mostly driven by empirical trials.$$$To orientate the reader in this vast and complex research area, we present a comprehensive survey of word reordering viewed as a statistical modeling challenge and as a natural language phenomenon.$$$The survey describes in detail how word reordering is modeled within different string-based and tree-based SMT frameworks and as a stand-alone task, including systematic overviews of the literature in advanced reordering modeling.$$$We then question why some approaches are more successful than others in different language pairs.$$$We argue that, besides measuring the amount of reordering, it is important to understand which kinds of reordering occur in a given language pair.$$$To this end, we conduct a qualitative analysis of word reordering phenomena in a diverse sample of language pairs, based on a large collection of linguistic knowledge.$$$Empirical results in the SMT literature are shown to support the hypothesis that a few linguistic facts can be very useful to anticipate the reordering characteristics of a language pair and to select the SMT framework that best suits them.",BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS OBJECTIVES METHODS CONCLUSIONS
D04618,"In the multi-agent systems setting, this paper addresses continuous-time distributed synchronization of columns of rotation matrices.$$$More precisely, k specific columns shall be synchronized and only the corresponding k columns of the relative rotations between the agents are assumed to be available for the control design.$$$When one specific column is considered, the problem is equivalent to synchronization on the (d-1)-dimensional unit sphere and when all the columns are considered, the problem is equivalent to synchronization on SO(d).$$$We design dynamic control laws for these synchronization problems.$$$The control laws are based on the introduction of auxiliary variables in combination with a QR-factorization approach.$$$The benefit of this QR-factorization approach is that we can decouple the dynamics for the k columns from the remaining d-k ones.$$$Under the control scheme, the closed loop system achieves almost global convergence to synchronization for quasi-strong interaction graph topologies.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES METHODS METHODS RESULTS RESULTS
D06198,"Reconstruction of skilled humans sensation and control system often leads to a development of robust control for the robots.$$$We are developing an unscrewing robot for the automated disassembly which requires a comprehensive control system, but unscrewing experiments with robots are often limited to several conditions.$$$On the contrary, humans typically have a broad range of screwing experiences and sensations throughout their lives, and we conducted an experiment to find these haptic patterns.$$$Results show that people apply axial force to the screws to avoid screwdriver slippage (cam-outs), which is one of the key problems during screwing and unscrewing, and this axial force is proportional to the torque which is required for screwing.$$$We have found that type of the screw head influences the amount of axial force applied.$$$Using this knowledge an unscrewing robot for the smart disassembly factory RecyBot is developed, and experiments confirm the optimality of the strategy, used by humans.$$$Finally, a methodology for robust unscrewing algorithm design is presented as a generalization of the findings.$$$It can seriously speed up the development of the screwing and unscrewing robots and tools.",BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D04162,"This paper presents a methodology for temporal logic verification of discrete-time stochastic systems.$$$Our goal is to find a lower bound on the probability that a complex temporal property is satisfied by finite traces of the system.$$$Desired temporal properties of the system are expressed using a fragment of linear temporal logic, called safe LTL over finite traces.$$$We propose to use barrier certificates for computations of such lower bounds, which is computationally much more efficient than the existing discretization-based approaches.$$$The new approach is discretization-free and does not suffer from the curse of dimensionality caused by discretizing state sets.$$$The proposed approach relies on decomposing the negation of the specification into a union of sequential reachabilities and then using barrier certificates to compute upper bounds for these reachability probabilities.$$$We demonstrate the effectiveness of the proposed approach on case studies with linear and polynomial dynamics.",BACKGROUND OBJECTIVES OTHERS METHODS CONCLUSIONS METHODS RESULTS
D05392,"This paper describes how to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving.$$$Based on a parametric body model, we present a robust processing pipeline achieving 3D model fits with 5mm accuracy also for clothed people.$$$Our main contribution is a method to nonrigidly deform the silhouette cones corresponding to the dynamic human silhouettes, resulting in a visual hull in a common reference frame that enables surface reconstruction.$$$This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames.$$$We present evaluation results for a number of test subjects and analyze overall performance.$$$Requiring only a smartphone or webcam, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.",OTHERS CONCLUSIONS METHODS METHODS/CONCLUSIONS CONCLUSIONS CONCLUSIONS
D01074,"Digital image forensics is a young but maturing field, encompassing key areas such as camera identification, detection of forged images, and steganalysis.$$$However, large gaps exist between academic results and applications used by practicing forensic analysts.$$$To move academic discoveries closer to real-world implementations, it is important to use data that represent ""in the wild"" scenarios.$$$For detection of stego images created from steganography apps, images generated from those apps are ideal to use.$$$In this paper, we present our work to perform steg detection on images from mobile apps using two different approaches: ""signature"" detection, and machine learning methods.$$$A principal challenge of the ML task is to create a great many of stego images from different apps with certain embedding rates.$$$One of our main contributions is a procedure for generating a large image database by using Android emulators and reverse engineering techniques.$$$We develop algorithms and tools for signature detection on stego apps, and provide solutions to issues encountered when creating ML classifiers.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND/OBJECTIVES METHODS BACKGROUND METHODS CONCLUSIONS
D04027,"This paper addresses the problem of distributed event localization using noisy range measurements with respect to sensors with known positions.$$$Event localization is fundamental in many wireless sensor network applications such as homeland security, law enforcement, and environmental studies.$$$However, most existing distributed algorithms require the target event to be within the convex hull of the deployed sensors.$$$Based on the alternating direction method of multipliers (ADMM), we propose two scalable distributed algorithms named GS-ADMM and J-ADMM which do not require the target event to be within the convex hull of the deployed sensors.$$$More specifically, the two algorithms can be implemented in a scenario in which the entire sensor network is divided into several clusters with cluster heads collecting measurements within each cluster and exchanging intermediate computation information to achieve localization consistency (consensus) across all clusters.$$$This scenario is important in many applications such as homeland security and law enforcement.$$$Simulation results confirm effectiveness of the proposed algorithms.",OBJECTIVES BACKGROUND BACKGROUND METHODS RESULTS OTHERS CONCLUSIONS
D06743,"We study supervised learning problems using clustering constraints to impose structure on either features or samples, seeking to help both prediction and interpretation.$$$The problem of clustering features arises naturally in text classification for instance, to reduce dimensionality by grouping words together and identify synonyms.$$$The sample clustering problem on the other hand, applies to multiclass problems where we are allowed to make multiple predictions and the performance of the best answer is recorded.$$$We derive a unified optimization formulation highlighting the common structure of these problems and produce algorithms whose core iteration complexity amounts to a k-means clustering step, which can be approximated efficiently.$$$We extend these results to combine sparsity and clustering constraints, and develop a new projection algorithm on the set of clustered sparse vectors.$$$We prove convergence of our algorithms on random instances, based on a union of subspaces interpretation of the clustering structure.$$$Finally, we test the robustness of our methods on artificial data sets as well as real data extracted from movie reviews.",OBJECTIVES BACKGROUND BACKGROUND METHODS RESULTS RESULTS RESULTS
D05738,"A novel tag completion algorithm is proposed in this paper, which is designed with the following features: 1) Low-rank and error s-parsity: the incomplete initial tagging matrix D is decomposed into the complete tagging matrix A and a sparse error matrix E. However, instead of minimizing its nuclear norm, A is further factor-ized into a basis matrix U and a sparse coefficient matrix V, i.e.$$$D=UV+E.$$$This low-rank formulation encapsulating sparse coding enables our algorithm to recover latent structures from noisy initial data and avoid performing too much denoising; 2) Local reconstruction structure consistency: to steer the completion of D, the local linear reconstruction structures in feature space and tag space are obtained and preserved by U and V respectively.$$$Such a scheme could alleviate the negative effect of distances measured by low-level features and incomplete tags.$$$Thus, we can seek a balance between exploiting as much information and not being mislead to suboptimal performance.$$$Experiments conducted on Corel5k dataset and the newly issued Flickr30Concepts dataset demonstrate the effectiveness and efficiency of the proposed method.",OBJECTIVES METHODS METHODS METHODS METHODS CONCLUSIONS
D06521,"Studies estimate that there will be 266,120 new cases of invasive breast cancer and 40,920 breast cancer induced deaths in the year of 2018 alone.$$$Despite the pervasiveness of this affliction, the current process to obtain an accurate breast cancer prognosis is tedious and time consuming, requiring a trained pathologist to manually examine histopathological images in order to identify the features that characterize various cancer severity levels.$$$We propose MITOS-RCNN: a novel region based convolutional neural network (RCNN) geared for small object detection to accurately grade one of the three factors that characterize tumor belligerence described by the Nottingham Grading System: mitotic count.$$$Other computational approaches to mitotic figure counting and detection do not demonstrate ample recall or precision to be clinically viable.$$$Our models outperformed all previous participants in the ICPR 2012 challenge, the AMIDA 2013 challenge and the MITOS-ATYPIA-14 challenge along with recently published works.$$$Our model achieved an F-measure score of 0.955, a 6.11% improvement in accuracy from the most accurate of the previously proposed models.",BACKGROUND BACKGROUND METHODS BACKGROUND RESULTS CONCLUSIONS
D01119,"A low carbon society aims at fighting global warming by stimulating synergic efforts from governments, industry and scientific communities.$$$Decision support systems should be adopted to provide policy makers with possible scenarios, options for prompt countermeasures in case of side effects on environment, economy and society due to low carbon society policies, and also options for information management.$$$A necessary precondition to fulfill this agenda is to face the complexity of this multi-disciplinary domain and to reach a common understanding on it as a formal specification.$$$Ontologies are widely accepted means to share knowledge.$$$Together with semantic rules, they enable advanced semantic services to manage knowledge in a smarter way.$$$Here we address the European Emissions Trading System (EU-ETS) and we present a knowledge base consisting of the EREON ontology and a catalogue of rules.$$$Then we describe two innovative semantic services to manage ETS data and information on ETS scenarios.",BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND/METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D01438,A survey of dictionary models and formats is presented as well as a presentation of corresponding recent standardisation activities.,BACKGROUND
D03118,"Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2).$$$The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data.$$$Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid.$$$Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing.$$$Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).",BACKGROUND/OBJECTIVES/METHODS OBJECTIVES/METHODS/RESULTS OBJECTIVES/METHODS/RESULTS BACKGROUND/OBJECTIVES/METHODS/RESULTS RESULTS
D00946,"This paper presents a tensor alignment (TA) based domain adaptation method for hyperspectral image (HSI) classification.$$$To be specific, HSIs in both domains are first segmented into superpixels and tensors of both domains are constructed to include neighboring samples from single superpixel.$$$Then we consider the subspace invariance between two domains as projection matrices and original tensors are projected as core tensors with lower dimensions into the invariant tensor subspace by applying Tucker decomposition.$$$To preserve geometric information in original tensors, we employ a manifold regularization term for core tensors into the decomposition progress.$$$The projection matrices and core tensors are solved in an alternating optimization manner and the convergence of TA algorithm is analyzed.$$$In addition, a post-processing strategy is defined via pure samples extraction for each superpixel to further improve classification performance.$$$Experimental results on four real HSIs demonstrate that the proposed method can achieve better performance compared with the state-of-the-art subspace learning methods when a limited amount of source labeled samples are available.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D01281,"Enterprise databases usually contain large and complex schemas.$$$Authoring complete schema mapping queries in this case requires deep knowledge about the source and target schemas and is thereby very challenging to programmers.$$$Sample-driven schema mapping allows the user to describe the schema mapping using data records.$$$However, real data records are still harder to specify than other useful insights about the desired schema mapping the user might have.$$$In this project, we develop a schema mapping system, PRISM, that enables multiresolution schema mapping.$$$The end user is not limited to providing high-resolution constraints like exact data records but may also provide constraints of various resolutions, like incomplete data records, value ranges, and data types.$$$This new interaction paradigm gives the user more flexibility in describing the desired schema mapping.$$$This demonstration showcases how to use PRISM for schema mapping in a real database.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS OTHERS
D00418,"We consider the communication scenario where K transmitters are each connected to a common receiver with an orthogonal noiseless link.$$$One of the transmitters has a message for the receiver, who is prohibited from learning anything in the information theoretic sense about which transmitter sends the message (transmitter anonymity is guaranteed).$$$The capacity of anonymous communications is the maximum number of bits of desired information that can be anonymously communicated per bit of total communication.$$$For this anonymous communication problem over a parallel channel with K transmitters and 1 receiver, we show that the capacity is 1/K, i.e., to communicate 1 bit anonymously, each transmitter must send a 1 bit signal.$$$Further, it is required that each transmitter has at least 1 bit correlated randomness (that is independent of the messages) per message bit and the size of correlated randomness at all K transmitters is at least K-1 bits per message bit.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/RESULTS/CONCLUSIONS OBJECTIVES/RESULTS/CONCLUSIONS
D02052,"In programming by example, users ""write"" programs by generating a small number of input-output examples and asking the computer to synthesize consistent programs.$$$We consider a challenging problem in this domain: learning regular expressions (regexes) from positive and negative example strings.$$$This problem is challenging, as (1) user-generated examples may not be informative enough to sufficiently constrain the hypothesis space, and (2) even if user-generated examples are in principle informative, there is still a massive search space to examine.$$$We frame regex induction as the problem of inferring a probabilistic regular grammar and propose an efficient inference approach that uses a novel stochastic process recognition model.$$$This model incrementally ""grows"" a grammar using positive examples as a scaffold.$$$We show that this approach is competitive with human ability to learn regexes from examples.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS
D03758,"Slow adaption processes, like synaptic and intrinsic plasticity, abound in the brain and shape the landscape for the neural dynamics occurring on substantially faster timescales.$$$At any given time the network is characterized by a set of internal parameters, which are adapting continuously, albeit slowly.$$$This set of parameters defines the number and the location of the respective adiabatic attractors.$$$The slow evolution of network parameters hence induces an evolving attractor landscape, a process which we term attractor metadynamics.$$$We study the nature of the metadynamics of the attractor landscape for several continuous-time autonomous model networks.$$$We find both first- and second-order changes in the location of adiabatic attractors and argue that the study of the continuously evolving attractor landscape constitutes a powerful tool for understanding the overall development of the neural dynamics.",BACKGROUND METHODS METHODS RESULTS OBJECTIVES RESULTS
D04573,"This research considers the task of evolving the physical structure of a robot to enhance its performance in various environments, which is a significant problem in the field of Evolutionary Robotics.$$$Inspired by the fields of evolutionary art and sculpture, we evolve only targeted parts of a robot, which simplifies the optimisation problem compared to traditional approaches that must simultaneously evolve both (actuated) body and brain.$$$Exploration fidelity is emphasised in areas of the robot most likely to benefit from shape optimisation, whilst exploiting existing robot structure and control.$$$Our approach uses a Genetic Algorithm to optimise collections of Bezier splines that together define the shape of a legged robot's tibia, and leg performance is evaluated in parallel in a high-fidelity simulator.$$$The leg is represented in the simulator as 3D-printable file, and as such can be readily instantiated in reality.$$$Provisional experiments in three distinct environments show the evolution of environment-specific leg structures that are both high-performing and notably different to those evolved in the other environments.$$$This proof-of-concept represents an important step towards the environment-dependent optimisation of performance-critical components for a range of ubiquitous, standard, and already-capable robots that can carry out a wide variety of tasks.",OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS RESULTS CONCLUSIONS
D03323,"This paper proposes a novel channel estimation method and a cluster-based opportunistic scheduling policy, for a wireless energy transfer (WET) system consisting of multiple low-complex energy receivers (ERs) with limited processing capabilities.$$$Firstly, in the training stage, the energy transmitter (ET) obtains a set of Received Signal Strength Indicator (RSSI) feedback values from all ERs, and these values are used to estimate the channels between the ET and all ERs.$$$Next, based on the channel estimates, the ERs are grouped into clusters, and the cluster that has its members closest to its centroid in phase is selected for dedicated WET.$$$The beamformer that maximizes the minimum harvested energy among all ERs in the selected cluster is found by solving a convex optimization problem.$$$All ERs have the same chance of being selected regardless of their distances from the ET, and hence, this scheduling policy can be considered to be opportunistic as well as fair.$$$It is shown that the proposed method achieves significant performance gains over benchmark schemes.",BACKGROUND METHODS METHODS METHODS RESULTS CONCLUSIONS
D05742,"Facial aging and facial rejuvenation analyze a given face photograph to predict a future look or estimate a past look of the person.$$$To achieve this, it is critical to preserve human identity and the corresponding aging progression and regression with high accuracy.$$$However, existing methods cannot simultaneously handle these two objectives well.$$$We propose a novel generative adversarial network based approach, named the Conditional Multi-Adversarial AutoEncoder with Ordinal Regression (CMAAE-OR).$$$It utilizes an age estimation technique to control the aging accuracy and takes a high-level feature representation to preserve personalized identity.$$$Specifically, the face is first mapped to a latent vector through a convolutional encoder.$$$The latent vector is then projected onto the face manifold conditional on the age through a deconvolutional generator.$$$The latent vector preserves personalized face features and the age controls facial aging and rejuvenation.$$$A discriminator and an ordinal regression are imposed on the encoder and the generator in tandem, making the generated face images to be more photorealistic while simultaneously exhibiting desirable aging effects.$$$Besides, a high-level feature representation is utilized to preserve personalized identity of the generated face.$$$Experiments on two benchmark datasets demonstrate appealing performance of the proposed method over the state-of-the-art.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND METHODS METHODS METHODS METHODS METHODS METHODS METHODS RESULTS
D05208,"We present a survey on maritime object detection and tracking approaches, which are essential for the development of a navigational system for autonomous ships.$$$The electro-optical (EO) sensor considered here is a video camera that operates in the visible or the infrared spectra, which conventionally complement radar and sonar and have demonstrated effectiveness for situational awareness at sea has demonstrated its effectiveness over the last few years.$$$This paper provides a comprehensive overview of various approaches of video processing for object detection and tracking in the maritime environment.$$$We follow an approach-based taxonomy wherein the advantages and limitations of each approach are compared.$$$The object detection system consists of the following modules: horizon detection, static background subtraction and foreground segmentation.$$$Each of these has been studied extensively in maritime situations and has been shown to be challenging due to the presence of background motion especially due to waves and wakes.$$$The main processes involved in object tracking include video frame registration, dynamic background subtraction, and the object tracking algorithm itself.$$$The challenges for robust tracking arise due to camera motion, dynamic background and low contrast of tracked object, possibly due to environmental degradation.$$$The survey also discusses multisensor approaches and commercial maritime systems that use EO sensors.$$$The survey also highlights methods from computer vision research which hold promise to perform well in maritime EO data processing.$$$Performance of several maritime and computer vision techniques is evaluated on newly proposed Singapore Maritime Dataset.",OBJECTIVES BACKGROUND METHODS METHODS METHODS RESULTS RESULTS RESULTS METHODS CONCLUSIONS RESULTS/CONCLUSIONS
D00118,"In this paper we describe the overall idea and results of a recently proposed radio access technique based on filter bank multicarrier (FBMC) communication system using two orthogonal polarizations: dual-polarization FBMC (DP-FBMC).$$$Using this system we can alleviate the intrinsic interference problem in FBMC systems.$$$This enables use of all the multicarrier techniques used in cyclic-prefix orthogonal frequency-division multiplexing (CP-OFDM) systems for channel equalization, multiple-input/multiple-output (MIMO) processing, etc., without using the extra processing required for conventional FBMC.$$$DP-FBMC also provides other interesting advantages over CP-OFDM and FBMC such as more robustness in multipath fading channels, and more robustness to receiver carrier frequency offset (CFO) and timing offset (TO).$$$For DP-FBMC we propose three different structures based on different multiplexing techniques in time, frequency, and polarization.$$$We will show that one of these structures has exactly the same system complexity and equipment as conventional FBMC.$$$In our simulation results DP-FBMC has better bit error ratio (BER) performance in dispersive channels.$$$Based on these results, DP-FBMC has potential as a promising candidate for future wireless communication systems.",BACKGROUND/OBJECTIVES/METHODS RESULTS RESULTS RESULTS OBJECTIVES/METHODS RESULTS RESULTS CONCLUSIONS
D05646,"Time series forecasting is ubiquitous in the modern world.$$$Applications range from health care to astronomy, and include climate modelling, financial trading and monitoring of critical engineering equipment.$$$To offer value over this range of activities, models must not only provide accurate forecasts, but also quantify and adjust their uncertainty over time.$$$In this work, we directly tackle this task with a novel, fully end-to-end deep learning method for time series forecasting.$$$By recasting time series forecasting as an ordinal regression task, we develop a principled methodology to assess long-term predictive uncertainty and describe rich multimodal, non-Gaussian behaviour, which arises regularly in applied settings.$$$Notably, our framework is a wholly general-purpose approach that requires little to no user intervention to be used.$$$We showcase this key feature in a large-scale benchmark test with 45 datasets drawn from both, a wide range of real-world application domains, as well as a comprehensive list of synthetic maps.$$$This wide comparison encompasses state-of-the-art methods in both the Machine Learning and Statistics modelling literature, such as the Gaussian Process.$$$We find that our approach does not only provide excellent predictive forecasts, shadowing true future values, but also allows us to infer valuable information, such as the predictive distribution of the occurrence of critical events of interest, accurately and reliably even over long time horizons.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS METHODS CONCLUSIONS
D06807,"This paper is concerned with how to make efficient use of social information to improve recommendations.$$$Most existing social recommender systems assume people share similar preferences with their social friends.$$$Which, however, may not hold true due to various motivations of making online friends and dynamics of online social networks.$$$Inspired by recent causal process based recommendations that first model user exposures towards items and then use these exposures to guide rating prediction, we utilize social information to capture user exposures rather than user preferences.$$$We assume that people get information of products from their online friends and they do not have to share similar preferences, which is less restrictive and seems closer to reality.$$$Under this new assumption, in this paper, we present a novel recommendation approach (named SERec) to integrate social exposure into collaborative filtering.$$$We propose two methods to implement SERec, namely social regularization and social boosting, each with different ways to construct social exposures.$$$Experiments on four real-world datasets demonstrate that our methods outperform the state-of-the-art methods on top-N recommendations.$$$Further study compares the robustness and scalability of the two proposed methods.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06327,"We address the problem of bootstrapping language acquisition for an artificial system similarly to what is observed in experiments with human infants.$$$Our method works by associating meanings to words in manipulation tasks, as a robot interacts with objects and listens to verbal descriptions of the interactions.$$$The model is based on an affordance network, i.e., a mapping between robot actions, robot perceptions, and the perceived effects of these actions upon objects.$$$We extend the affordance model to incorporate spoken words, which allows us to ground the verbal symbols to the execution of actions and the perception of the environment.$$$The model takes verbal descriptions of a task as the input and uses temporal co-occurrence to create links between speech utterances and the involved objects, actions, and effects.$$$We show that the robot is able form useful word-to-meaning associations, even without considering grammatical structure in the learning process and in the presence of recognition errors.$$$These word-to-meaning associations are embedded in the robot's own understanding of its actions.$$$Thus, they can be directly used to instruct the robot to perform tasks and also allow to incorporate context in the speech recognition task.$$$We believe that the encouraging results with our approach may afford robots with a capacity to acquire language descriptors in their operation's environment as well as to shed some light as to how this challenging process develops with human infants.",OBJECTIVES OBJECTIVES/METHODS METHODS BACKGROUND/METHODS METHODS/RESULTS RESULTS RESULTS RESULTS/CONCLUSIONS CONCLUSIONS
D04352,"One of the defining features of a cryptocurrency is that its ledger, containing all transactions that have ever taken place, is globally visible.$$$As one consequence of this degree of transparency, a long line of recent research has demonstrated that - even in cryptocurrencies that are specifically designed to improve anonymity - it is often possible to track flows of money as it changes hands, and in some cases to de-anonymize users entirely.$$$With the recent proliferation of alternative cryptocurrencies, however, it becomes relevant to ask not only whether or not money can be traced as it moves within the ledger of a single cryptocurrency, but if it can in fact be traced as it moves across ledgers.$$$This is especially pertinent given the rise in popularity of automated trading platforms such as ShapeShift, which make it effortless to carry out such cross-currency trades.$$$In this paper, we use data scraped from ShapeShift over a six-month period and the data from eight different blockchains in order to explore this question.$$$Beyond developing new heuristics and demonstrating the ability to create new types of links across cryptocurrency ledgers, we also identify various patterns of cross-currency trades and of the general usage of these platforms, with the ultimate goal of understanding whether they serve either a criminal or a profit-driven agenda.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS/CONCLUSIONS
D05973,"Navigating safely in urban environments remains a challenging problem for autonomous vehicles.$$$Occlusion and limited sensor range can pose significant challenges to safely navigate among pedestrians and other vehicles in the environment.$$$Enabling vehicles to quantify the risk posed by unseen regions allows them to anticipate future possibilities, resulting in increased safety and ride comfort.$$$This paper proposes an algorithm that takes advantage of the known road layouts to forecast, quantify, and aggregate risk associated with occlusions and limited sensor range.$$$This allows us to make predictions of risk induced by unobserved vehicles even in heavily occluded urban environments.$$$The risk can then be used either by a low-level planning algorithm to generate better trajectories, or by a high-level one to plan a better route.$$$The proposed algorithm is evaluated on intersection layouts from real-world map data with up to five other vehicles in the scene, and verified to reduce collision rates by 4.8x comparing to a baseline method while improving driving comfort.",BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS RESULTS RESULTS/CONCLUSIONS
D03315,"In this paper, we present FPT-algorithms for special cases of the shortest vector problem (SVP) and the integer linear programming problem (ILP), when matrices included to the problems' formulations are near square.$$$The main parameter is the maximal absolute value of rank minors of matrices included to the problem formulation.$$$Additionally, we present FPT-algorithms with respect to the same main parameter for the problems, when the matrices have no singular rank sub-matrices.",OBJECTIVES/RESULTS OBJECTIVES OBJECTIVES/RESULTS
D01687,"With virtual reality, digital painting on 2D canvases is now being extended to 3D spaces.$$$Tilt Brush and Oculus Quill are widely accepted among artists as tools that pave the way to a new form of art - 3D emmersive painting.$$$Current 3D painting systems are only a start, emitting textured triangular geometries.$$$In this paper, we advance this new art of 3D painting to 3D volumetric painting that enables an artist to draw a huge scene with full control of spatial color fields.$$$Inspired by the fact that 2D paintings often use vast space to paint background and small but detailed space for foreground, we claim that supporting a large canvas in varying detail is essential for 3D painting.$$$In order to help artists focus and audiences to navigate the large canvas space, we provide small artist-defined areas, called rooms, that serve as beacons for artist-suggested scales, spaces, locations for intended appreciation view of the painting.$$$Artists and audiences can easily transport themselves between different rooms.$$$Technically, our canvas is represented as an array of deep octrees of depth 24 or higher, built on CPU for volume painting and on GPU for volume rendering using accurate ray casting.$$$In CPU side, we design an efficient iterative algorithm to refine or coarsen octree, as a result of volumetric painting strokes, at highly interactive rates, and update the corresponding GPU textures.$$$Then we use GPU-based ray casting algorithms to render the volumetric painting result.$$$We explore precision issues stemming from ray-casting the octree of high depth, and provide a new analysis and verification.$$$From our experimental results as well as the positive feedback from the participating artists, we strongly believe that our new 3D volume painting system can open up a new possibility for VR-driven digital art medium to professional artists as well as to novice users.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/CONCLUSIONS OTHERS METHODS RESULTS METHODS METHODS METHODS OBJECTIVES RESULTS
D03431,"Recent years have seen a growing interest in understanding deep neural networks from an optimization perspective.$$$It is understood now that converging to low-cost local minima is sufficient for such models to become effective in practice.$$$However, in this work, we propose a new hypothesis based on recent theoretical findings and empirical studies that deep neural network models actually converge to saddle points with high degeneracy.$$$Our findings from this work are new, and can have a significant impact on the development of gradient descent based methods for training deep networks.$$$We validated our hypotheses using an extensive experimental evaluation on standard datasets such as MNIST and CIFAR-10, and also showed that recent efforts that attempt to escape saddles finally converge to saddles with high degeneracy, which we define as `good saddles'.$$$We also verified the famous Wigner's Semicircle Law in our experimental results.",BACKGROUND OBJECTIVES METHODS RESULTS RESULTS RESULTS
D03688,"With an increasing number of web services, providing an end-to-end Quality of Service (QoS) guarantee in responding to user queries is becoming an important concern.$$$Multiple QoS parameters (e.g., response time, latency, throughput, reliability, availability, success rate) are associated with a service, thereby, service composition with a large number of candidate services is a challenging multi-objective optimization problem.$$$In this paper, we study the multi-constrained multi-objective QoS aware web service composition problem and propose three different approaches to solve the same, one optimal, based on Pareto front construction and two other based on heuristically traversing the solution space.$$$We compare the performance of the heuristics against the optimal, and show the effectiveness of our proposals over other classical approaches for the same problem setting, with experiments on WSC-2009 and ICEBE-2005 datasets.",OBJECTIVES OBJECTIVES METHODS RESULTS
D01756,"The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings.$$$Despite the significant increase in adoption of EHR systems, this data remains largely unexplored, but presents a rich data source for knowledge discovery from patient health histories in tasks such as understanding disease correlations and predicting health outcomes.$$$However, the heterogeneity, sparsity, noise, and bias in this data present many complex challenges.$$$This complexity makes it difficult to translate potentially relevant information into machine learning algorithms.$$$In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep representation of longitudinal EHR data which is personalized for each patient.$$$To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods.$$$Patient2Vec produces a vector space with meaningful structure and it achieves an AUC around 0.799 outperforming baseline methods.$$$In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS CONCLUSIONS
D00478,"This is a study of the MOR cryptosystem using the special linear group over finite fields.$$$The automorphism group of the special linear group is analyzed for this purpose.$$$At our current state of knowledge, I show that the MOR cryptosystem has better security than the ElGamal cryptosystem over finite fields.",BACKGROUND METHODS OBJECTIVES/RESULTS
D04033,"Since the advent of deep learning, it has been used to solve various problems using many different architectures.$$$The application of such deep architectures to auditory data is also not uncommon.$$$However, these architectures do not always adequately consider the temporal dependencies in data.$$$We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data.$$$We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS
D00724,"Consider a transmission scheme with a single transmitter and multiple receivers over a faulty broadcast channel.$$$For each receiver, the transmitter has a unique infinite stream of packets, and its goal is to deliver them at the highest throughput possible.$$$While such multiple-unicast models are unsolved in general, several network coding based schemes were suggested.$$$In such schemes, the transmitter can either send an uncoded packet, or a coded packet which is a function of a few packets.$$$The packets sent can be received by the designated receiver (with some probability) or heard and stored by other receivers.$$$Two functional modes are considered; the first presumes that the storage time is unlimited, while in the second it is limited by a given Time to Expire (TTE) parameter.$$$We model the transmission process as an infinite-horizon Markov Decision Process (MDP).$$$Since the large state space renders exact solutions computationally impractical, we introduce policy restricted and induced MDPs with significantly reduced state space, and prove that with proper reward function they have equal optimal value function (hence equal optimal throughput).$$$We then derive a reinforcement learning algorithm, which learns the optimal policy for the induced MDP.$$$This optimal strategy of the induced MDP, once applied to the policy restricted one, significantly improves over uncoded schemes.$$$Next, we enhance the algorithm by means of analysis of the structural properties of the resulting reward functional.$$$We demonstrate that our method scales well in the number of users, and automatically adapts to the packet loss rates, unknown in advance.$$$In addition, the performance is compared to the recent bound by Wang, which assumes much stronger coding (e.g., intra-session and buffering of coded packets), yet is shown to be comparable.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS
D01228,"Community detection is a fundamental problem in network analysis with many methods available to estimate communities.$$$Most of these methods assume that the number of communities is known, which is often not the case in practice.$$$We propose a simple and very fast method for estimating the number of communities based on the spectral properties of certain graph operators, such as the non-backtracking matrix and the Bethe Hessian matrix.$$$We show that the method performs well under several models and a wide range of parameters, and is guaranteed to be consistent under several asymptotic regimes.$$$We compare the new method to several existing methods for estimating the number of communities and show that it is both more accurate and more computationally efficient.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS/CONCLUSIONS
D00509,"This paper presents an iterated local search for the fixed-charge uncapacitated network design problem with user-optimal flow (FCNDP-UOF), which concerns routing multiple commodities from its origin to its destination by signing a network through selecting arcs, with an objective of minimizing the sum of the fixed costs of the selected arcs plus the sum of variable costs associated to the flows on each arc.$$$Besides that, since the FCNDP-UOF is a bi-level problem, each commodity has to be transported through a shortest path, concerning the edges length, in the built network.$$$The proposed algorithm generate a initial solution using a variable fixing heuristic.$$$Then a local branching strategy is applied to improve the quality of the solution.$$$At last, an efficient perturbation strategy is presented to perform cycle-based moves to explore different parts of the solution space.$$$Computational experiments shows that the proposed solution method consistently produces high-quality solutions in reasonable computational times.",BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS
D04815,"We study two-player games played on the infinite graph of sentential forms induced by a context-free grammar (that comes with an ownership partitioning of the non-terminals).$$$The winning condition is inclusion of the derived terminal word in the language of a finite automaton.$$$Our contribution is a new algorithm to decide the winning player and to compute her strategy.$$$It is based on a novel representation of all plays starting in a non-terminal.$$$The representation uses the domain of Boolean formulas over the transition monoid of the target automaton.$$$The elements of the monoid are essentially procedure summaries, and our approach can be seen as the first summary-based algorithm for the synthesis of recursive programs.$$$We show that our algorithm has optimal (doubly exponential) time complexity, that it is compatible with recent antichain optimizations, and that it admits a lazy evaluation strategy.$$$Our preliminary experiments indeed show encouraging results, indicating a speed up of three orders of magnitude over a competitor.",BACKGROUND/OBJECTIVES BACKGROUND RESULTS METHODS METHODS BACKGROUND/METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D04303,"Sites in an infinite d-dimensional lattice, open with probability greater or equal to 1/d, form an infinite open path.",RESULTS/CONCLUSIONS
D01738,"Mobility Management (MM) techniques have conventionally been centralized in nature, wherein a single network entity has been responsible for handling the mobility related tasks of the mobile nodes attached to the network.$$$However, an exponential growth in network traffic and the number of users has ushered in the concept of providing Mobility Management as a Service (MMaaS) to the wireless nodes attached to the 5G networks.$$$Allowing for on-demand mobility management solutions will not only provide the network with the flexibility that it needs to accommodate the many different use cases that are to be served by future networks, but it will also provide the network with the scalability that is needed alongside the flexibility to serve future networks.$$$And hence, in this paper, a detailed study of MMaaS has been provided, highlighting its benefits and challenges for 5G networks.$$$Additionally, the very important property of granularity of service which is deeply intertwined with the scalability and flexibility requirements of the future wireless networks, and a consequence of MMaaS, has also been discussed in detail.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS
D01767,"The Variational Autoencoder (VAE) is a powerful architecture capable of representation learning and generative modeling.$$$When it comes to learning interpretable (disentangled) representations, VAE and its variants show unparalleled performance.$$$However, the reasons for this are unclear, since a very particular alignment of the latent embedding is needed but the design of the VAE does not encourage it in any explicit way.$$$We address this matter and offer the following explanation: the diagonal approximation in the encoder together with the inherent stochasticity force local orthogonality of the decoder.$$$The local behavior of promoting both reconstruction and orthogonality matches closely how the PCA embedding is chosen.$$$Alongside providing an intuitive understanding, we justify the statement with full theoretical analysis as well as with experiments.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS
D04632,"We present a compartmentalized approach to finding the maximum a-posteriori (MAP) estimate of a latent time series that obeys a dynamic stochastic model and is observed through noisy measurements.$$$We specifically consider modern signal processing problems with non-Markov signal dynamics (e.g. group sparsity) and/or non-Gaussian measurement models (e.g. point process observation models used in neuroscience).$$$Through the use of auxiliary variables in the MAP estimation problem, we show that a consensus formulation of the alternating direction method of multipliers (ADMM) enables iteratively computing separate estimates based on the likelihood and prior and subsequently ""averaging"" them in an appropriate sense using a Kalman smoother.$$$As such, this can be applied to a broad class of problem settings and only requires modular adjustments when interchanging various aspects of the statistical model.$$$Under broad log-concavity assumptions, we show that the separate estimation problems are convex optimization problems and that the iterative algorithm converges to the MAP estimate.$$$As such, this framework can capture non-Markov latent time series models and non-Gaussian measurement models.$$$We provide example applications involving (i) group-sparsity priors, within the context of electrophysiologic specrotemporal estimation, and (ii) non-Gaussian measurement models, within the context of dynamic analyses of learning with neural spiking and behavioral observations.",BACKGROUND OBJECTIVES METHODS RESULTS METHODS RESULTS/CONCLUSIONS RESULTS
D00006,"This paper proposes the continuous semantic topic embedding model (CSTEM) which finds latent topic variables in documents using continuous semantic distance function between the topics and the words by means of the variational autoencoder(VAE).$$$The semantic distance could be represented by any symmetric bell-shaped geometric distance function on the Euclidean space, for which the Mahalanobis distance is used in this paper.$$$In order for the semantic distance to perform more properly, we newly introduce an additional model parameter for each word to take out the global factor from this distance indicating how likely it occurs regardless of its topic.$$$It certainly improves the problem that the Gaussian distribution which is used in previous topic model with continuous word embedding could not explain the semantic relation correctly and helps to obtain the higher topic coherence.$$$Through the experiments with the dataset of 20 Newsgroup, NIPS papers and CNN/Dailymail corpus, the performance of the recent state-of-the-art models is accomplished by our model as well as generating topic embedding vectors which makes possible to observe where the topic vectors are embedded with the word vectors in the real Euclidean space and how the topics are related each other semantically.",OBJECTIVES/METHODS METHODS CONCLUSIONS RESULTS RESULTS
D01749,"Gaussian processes (GPs) are versatile tools that have been successfully employed to solve nonlinear estimation problems in machine learning, but that are rarely used in signal processing.$$$In this tutorial, we present GPs for regression as a natural nonlinear extension to optimal Wiener filtering.$$$After establishing their basic formulation, we discuss several important aspects and extensions, including recursive and adaptive algorithms for dealing with non-stationarity, low-complexity solutions, non-Gaussian noise models and classification scenarios.$$$Furthermore, we provide a selection of relevant applications to wireless digital communications.",BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS RESULTS
D03082,"Algorithmic and data bias are gaining attention as a pressing issue in popular press - and rightly so.$$$However, beyond these calls to action, standard processes and tools for practitioners do not readily exist to assess and address unfair algorithmic and data biases.$$$The literature is relatively scattered and the needed interdisciplinary approach means that very different communities are working on the topic.$$$We here provide a number of challenges encountered in assessing and addressing algorithmic and data bias in practice.$$$We describe an early approach that attempts to translate the literature into processes for (production) teams wanting to assess both intended data and algorithm characteristics and unintended, unfair biases.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS
D00852,"We present an effect system for core Eff, a simplified variant of Eff, which is an ML-style programming language with first-class algebraic effects and handlers.$$$We define an expressive effect system and prove safety of operational semantics with respect to it.$$$Then we give a domain-theoretic denotational semantics of core Eff, using Pitts's theory of minimal invariant relations, and prove it adequate.$$$We use this fact to develop tools for finding useful contextual equivalences, including an induction principle.$$$To demonstrate their usefulness, we use these tools to derive the usual equations for mutable state, including a general commutativity law for computations using non-interfering references.$$$We have formalized the effect system, the operational semantics, and the safety theorem in Twelf.",RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS
D03071,"This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input.$$$When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class.$$$It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers.$$$Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine.$$$We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).",OBJECTIVES METHODS CONCLUSIONS BACKGROUND RESULTS
D04030,"This study investigates the mean capacity of multiple-input multiple-output (MIMO) systems for spatially semi-correlated flat fading channels.$$$In reality, the capacity degrades dramatic due to the channel covariance (CC) when correlations exist at the transmitter or receiver or on both sides.$$$Most existing works have so far considered the traditional channel covariance matrices that have not been entirely constructed.$$$Thus, we propose an iterative channel covariance (ICC) matrix using a matrix splitting (MS) technique with a guaranteed zero correlations coefficient in the case of the downlink correlated MIMO channel, to maximize the mean capacity.$$$Our numerical results show that the proposed ICC method achieves the maximum channel gains with high signal-to-noise ratio (SNR) scenarios.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D06351,"We present a package for Mathematica computer algebra system which allows the exploitation of local files as sources of random data.$$$We provide the description of the package and illustrate its usage by showing some examples.$$$We also compare the provided functionality with alternative sources of randomness, namely a built-in pseudo-random generator and the package for accessing hardware true random number generators.",RESULTS METHODS/RESULTS METHODS/RESULTS
D06347,"The problem of error correction for Gallager's low-density parity-check codes is famously equivalent to that of computing marginal Boltzmann probabilities for an Ising-like model with multispin interactions in a non-uniform magnetic field.$$$Since the graph of interactions is locally a tree, the solution is very well approximated by a generalized mean-field (Bethe-Peierls) approximation.$$$Belief propagation (BP) and similar iterative algorithms are an efficient way to perform the calculation, but they sometimes fail to converge, or converge to non-codewords, giving rise to a non-negligible residual error probability (error floor).$$$On the other hand, provably-convergent algorithms are far too complex to be implemented in a real decoder.$$$In this work we consider the application of the probability-damping technique, which can be regarded either as a variant of BP, from which it retains the property of low complexity, or as an approximation of a provably-convergent algorithm, from which it is expected to inherit better convergence properties.$$$We investigate the algorithm behaviour on a real instance of Gallager code, and compare the results with state-of-the-art algorithms.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS/RESULTS/CONCLUSIONS
D03013,"Deep Neural Networks (DNNs) often struggle with one-shot learning where we have only one or a few labeled training examples per category.$$$In this paper, we argue that by using side information, we may compensate the missing information across classes.$$$We introduce two statistical approaches for fusing side information into data representation learning to improve one-shot learning.$$$First, we propose to enforce the statistical dependency between data representations and multiple types of side information.$$$Second, we introduce an attention mechanism to efficiently treat examples belonging to the 'lots-of-examples' classes as quasi-samples (additional training samples) for 'one-example' classes.$$$We empirically show that our learning architecture improves over traditional softmax regression networks as well as state-of-the-art attentional regression networks on one-shot recognition tasks.",BACKGROUND METHODS OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D03967,"Motivated by applications in social network community analysis, we introduce a new clustering paradigm termed motif clustering.$$$Unlike classical clustering, motif clustering aims to minimize the number of clustering errors associated with both edges and certain higher order graph structures (motifs) that represent ""atomic units"" of social organizations.$$$Our contributions are two-fold: We first introduce motif correlation clustering, in which the goal is to agnostically partition the vertices of a weighted complete graph so that certain predetermined ""important"" social subgraphs mostly lie within the same cluster, while ""less relevant"" social subgraphs are allowed to lie across clusters.$$$We then proceed to introduce the notion of motif covers, in which the goal is to cover the vertices of motifs via the smallest number of (near) cliques in the graph.$$$Motif cover algorithms provide a natural solution for overlapping clustering and they also play an important role in latent feature inference of networks.$$$For both motif correlation clustering and its extension introduced via the covering problem, we provide hardness results, algorithmic solutions and community detection results for two well-studied social networks.",OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS
D02372,"In this paper, we propose an interpretable LSTM recurrent neural network, i.e., multi-variable LSTM for time series with exogenous variables.$$$Currently, widely used attention mechanism in recurrent neural networks mostly focuses on the temporal aspect of data and falls short of characterizing variable importance.$$$To this end, our multi-variable LSTM equipped with tensorized hidden states is developed to learn variable specific representations, which give rise to both temporal and variable level attention.$$$Preliminary experiments demonstrate comparable prediction performance of multi-variable LSTM w.r.t. encoder-decoder based baselines.$$$More interestingly, variable importance in real datasets characterized by the variable attention is highly in line with that determined by statistical Granger causality test, which exhibits the prospect of multi-variable LSTM as a simple and uniform end-to-end framework for both forecasting and knowledge discovery.",OBJECTIVES BACKGROUND METHODS RESULTS RESULTS
D03864,"When we search online for content, we are constantly exposed to rankings.$$$For example, web search results are presented as a ranking, and online bookstores often show us lists of best-selling books.$$$While popularity-based ranking algorithms (like Google's PageRank) have been extensively studied in previous works, we still lack a clear understanding of their potential systemic consequences.$$$In this work, we fill this gap by introducing a new model of network growth that allows us to compare the properties of the networks generated under the influence of different ranking algorithms.$$$We show that by correcting for the omnipresent age bias of popularity-based ranking algorithms, the resulting networks exhibit a significantly larger agreement between the nodes' inherent quality and their long-term popularity, and a less concentrated popularity distribution.$$$To further promote popularity diversity, we introduce and validate a perturbation of the original rankings where a small number of randomly-selected nodes are promoted to the top of the ranking.$$$Our findings move the first steps toward a model-based understanding of the long-term impact of popularity-based ranking algorithms, and could be used as an informative tool for the design of improved information filtering tools.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS CONCLUSIONS
D05418,"Neural Machine Translation (NMT) can be improved by including document-level contextual information.$$$For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner.$$$The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden states.$$$Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.",BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D01135,"Processors may find some elementary operations to be faster than the others.$$$Although an operation may be conceptually as simple as some other operation, the processing speeds of the two can vary.$$$A clever programmer will always try to choose the faster instructions for the job.$$$This paper presents an algorithm to display squares of 1st N natural numbers without using multiplication (* operator).$$$Instead, the same work can be done using addition (+ operator).$$$The results can also be used to compute the sum of those squares.$$$If we compare the normal method of computing the squares of 1st N natural numbers with this method, we can conclude that the algorithm discussed in the paper is more optimized in terms of time complexity.",BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D00002,"In this paper, we address the problem of computing optimal paths through three consecutive points for the curvature-constrained forward moving Dubins vehicle.$$$Given initial and final configurations of the Dubins vehicle, and a midpoint with an unconstrained heading, the objective is to compute the midpoint heading that minimizes the total Dubins path length.$$$We provide a novel geometrical analysis of the optimal path, and establish new properties of the optimal Dubins' path through three points.$$$We then show how our method can be used to quickly refine Dubins TSP tours produced using state-of-the-art techniques.$$$We also provide extensive simulation results showing the improvement of the proposed approach in both runtime and solution quality over the conventional method of uniform discretization of the heading at the mid-point, followed by solving the minimum Dubins path for each discrete heading.",OBJECTIVES OTHERS METHODS/RESULTS RESULTS RESULTS
D00308,"Recent years have seen major innovations in developing energy-efficient wireless technologies such as Bluetooth Low Energy (BLE) for Internet of Things (IoT).$$$Despite demonstrating significant benefits in providing low power transmission and massive connectivity, hardly any of these technologies have made it to directly connect to the Internet.$$$Recent advances demonstrate the viability of direct communication among heterogeneous IoT devices with incompatible physical (PHY) layers.$$$These techniques, however, require modifications in transmission power or time, which may affect the media access control (MAC) layer behaviors in legacy networks.$$$In this paper, we argue that the frequency domain can serve as a free side channel with minimal interruptions to legacy networks.$$$To this end, we propose DopplerFi, a communication framework that enables a two-way communication channel between BLE and Wi-Fi by injecting artificial Doppler shifts, which can be decoded by sensing the patterns in the Gaussian frequency shift keying (GFSK) demodulator and Channel State Information (CSI).$$$The artificial Doppler shifts can be compensated by the inherent frequency synchronization module and thus have a negligible impact on legacy communications.$$$Our evaluation using commercial off-the-shelf (COTS) BLE chips and 802.11-compliant testbeds have demonstrated that DopplerFi can achieve throughput up to 6.5~Kbps at the cost of merely less than 0.8% throughput loss.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS
D05843,"Simulation frameworks are important tools for the analysis and design of communication networks and protocols, but they can result extremely costly and/or complex (for the case of very specialized tools), or too naive and lacking proper features and support (for the case of ad-hoc tools).$$$In this paper, we present an analysis of three 5G scenarios using 'simmer', a recent R package for discrete-event simulation that sits between the above two paradigms.$$$As our results show, it provides a simple yet very powerful syntax, supporting the efficient simulation of relatively complex scenarios at a low implementation cost.",BACKGROUND OBJECTIVES RESULTS
D05435,"The immense amount of daily generated and communicated data presents unique challenges in their processing.$$$Clustering, the grouping of data without the presence of ground-truth labels, is an important tool for drawing inferences from data.$$$Subspace clustering (SC) is a relatively recent method that is able to successfully classify nonlinearly separable data in a multitude of settings.$$$In spite of their high clustering accuracy, SC methods incur prohibitively high computational complexity when processing large volumes of high-dimensional data.$$$Inspired by random sketching approaches for dimensionality reduction, the present paper introduces a randomized scheme for SC, termed Sketch-SC, tailored for large volumes of high-dimensional data.$$$Sketch-SC accelerates the computationally heavy parts of state-of-the-art SC approaches by compressing the data matrix across both dimensions using random projections, thus enabling fast and accurate large-scale SC.$$$Performance analysis as well as extensive numerical tests on real data corroborate the potential of Sketch-SC and its competitive performance relative to state-of-the-art scalable SC approaches.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D02590,"Surrogate models are a well established approach to reduce the number of expensive function evaluations in continuous optimization.$$$In the context of genetic programming, surrogate modeling still poses a challenge, due to the complex genotype-phenotype relationships.$$$We investigate how different genotypic and phenotypic distance measures can be used to learn Kriging models as surrogates.$$$We compare the measures and suggest to use their linear combination in a kernel.$$$We test the resulting model in an optimization framework, using symbolic regression problem instances as a benchmark.$$$Our experiments show that the model provides valuable information.$$$Firstly, the model enables an improved optimization performance compared to a model-free algorithm.$$$Furthermore, the model provides information on the contribution of different distance measures.$$$The data indicates that a phenotypic distance measure is important during the early stages of an optimization run when less data is available.$$$In contrast, genotypic measures, such as the tree edit distance, contribute more during the later stages.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06579,"The aim of this article is to present an overview of the existing biomedical data warehouses and to discuss the issues and future trends in this area.$$$We illustrate this topic by presenting the design of an innovative, complex data warehouse for personal, anticipative medicine.",OBJECTIVES METHODS
D03045,"While it is known that shared quantum entanglement can offer improved solutions to a number of purely cooperative tasks for groups of remote agents, controversy remains regarding the legitimacy of quantum games in a competitive setting--in particular, whether they offer any advantage beyond what is achievable using classical resources.$$$We construct a competitive game between four players based on the minority game where the maximal Nash-equilibrium payoff when played with the appropriate quantum resource is greater than that obtainable by classical means, assuming a local hidden variable model.$$$The game is constructed in a manner analogous to a Bell inequality.$$$This result is important in confirming the legitimacy of quantum games.",BACKGROUND RESULTS METHODS CONCLUSIONS
D03946,"In industrial machine learning pipelines, data often arrive in parts.$$$Particularly in the case of deep neural networks, it may be too expensive to train the model from scratch each time, so one would rather use a previously learned model and the new data to improve performance.$$$However, deep neural networks are prone to getting stuck in a suboptimal solution when trained on only new data as compared to the full dataset.$$$Our work focuses on a continuous learning setup where the task is always the same and new parts of data arrive sequentially.$$$We apply a Bayesian approach to update the posterior approximation with each new piece of data and find this method to outperform the traditional approach in our experiments.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS
D02300,"In the present day, AES is one the most widely used and most secure Encryption Systems prevailing.$$$So, naturally lots of research work is going on to mount a significant attack on AES.$$$Many different forms of Linear and differential cryptanalysis have been performed on AES.$$$Of late, an active area of research has been Algebraic Cryptanalysis of AES, where although fast progress is being made, there are still numerous scopes for research and improvement.$$$One of the major reasons behind this being that algebraic cryptanalysis mainly depends on I/O relations of the AES S- Box (a major component of the AES).$$$As, already known, that the key recovery algorithm of AES can be broken down as an MQ problem which is itself considered hard.$$$Solving these equations depends on our ability reduce them into linear forms which are easily solvable under our current computational prowess.$$$The lower the degree of these equations, the easier it is for us to linearlize hence the attack complexity reduces.$$$The aim of this paper is to analyze the various relations involving small number of monomials of the AES S- Box and to answer the question whether it is actually possible to have such monomial equations for the S- Box if we restrict the degree of the monomials.$$$In other words this paper aims to study such equations and see if they can be applicable for AES.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OTHERS BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES
D04338,"Mobile ad-hoc networks (MANETs) are a set of self organized wireless mobile nodes that works without any predefined infrastructure.$$$For routing data in MANETs, the routing protocols relay on mobile wireless nodes.$$$In general, any routing protocol performance suffers i) with resource constraints and ii) due to the mobility of the nodes.$$$Due to existing routing challenges in MANETs clustering based protocols suffers frequently with cluster head failure problem, which degrades the cluster stability.$$$This paper proposes, Enhanced CBRP, a schema to improve the cluster stability and in-turn improves the performance of traditional cluster based routing protocol (CBRP), by electing better cluster head using weighted clustering algorithm and considering some crucial routing challenges.$$$Moreover, proposed protocol suggests a secondary cluster head for each cluster, to increase the stability of the cluster and implicitly the network infrastructure in case of sudden failure of cluster head.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS
D05129,"Visual object tracking is a challenging computer vision task with numerous real-world applications.$$$Here we propose a simple but efficient Spectral Filter Tracking (SFT)method.$$$To characterize rotational and translation invariance of tracking targets, the candidate image region is models as a pixelwise grid graph.$$$Instead of the conventional graph matching, we convert the tracking into a plain least square regression problem to estimate the best center coordinate of the target.$$$But different from the holistic regression of correlation filter based methods, SFT can operate on localized surrounding regions of each pixel (i.e.,vertex) by using spectral graph filters, which thus is more robust to resist local variations and cluttered background.To bypass the eigenvalue decomposition problem of the graph Laplacian matrix L, we parameterize spectral graph filters as the polynomial of L by spectral graph theory, in which L k exactly encodes a k-hop local neighborhood of each vertex.$$$Finally, the filter parameters (i.e., polynomial coefficients) as well as feature projecting functions are jointly integrated into the regression model.",BACKGROUND METHODS METHODS METHODS METHODS METHODS
D04636,"Ecological Momentary Assessment (EMA) data is organized in multiple levels (per-subject, per-day, etc.) and this particular structure should be taken into account in machine learning algorithms used in EMA like decision trees and its variants.$$$We propose a new algorithm called BBT (standing for Bagged Boosted Trees) that is enhanced by a over/under sampling method and can provide better estimates for the conditional class probability function.$$$Experimental results on a real-world dataset show that BBT can benefit EMA data classification and performance.",BACKGROUND OBJECTIVES/METHODS RESULTS
D02450,"This paper proposes a deep cerebellar model articulation controller (DCMAC) for adaptive noise cancellation (ANC).$$$We expand upon the conventional CMAC by stacking sin-gle-layer CMAC models into multiple layers to form a DCMAC model and derive a modified backpropagation training algorithm to learn the DCMAC parameters.$$$Com-pared with conventional CMAC, the DCMAC can characterize nonlinear transformations more effectively because of its deep structure.$$$Experimental results confirm that the pro-posed DCMAC model outperforms the CMAC in terms of residual noise in an ANC task, showing that DCMAC provides enhanced modeling capability based on channel characteristics.",OBJECTIVES METHODS RESULTS RESULTS
D00673,"Recently, researchers started to pay attention to the detection of temporal shifts in the meaning of words.$$$However, most (if not all) of these approaches restricted their efforts to uncovering change over time, thus neglecting other valuable dimensions such as social or political variability.$$$We propose an approach for detecting semantic shifts between different viewpoints--broadly defined as a set of texts that share a specific metadata feature, which can be a time-period, but also a social entity such as a political party.$$$For each viewpoint, we learn a semantic space in which each word is represented as a low dimensional neural embedded vector.$$$The challenge is to compare the meaning of a word in one space to its meaning in another space and measure the size of the semantic shifts.$$$We compare the effectiveness of a measure based on optimal transformations between the two spaces with a measure based on the similarity of the neighbors of the word in the respective spaces.$$$Our experiments demonstrate that the combination of these two performs best.$$$We show that the semantic shifts not only occur over time, but also along different viewpoints in a short period of time.$$$For evaluation, we demonstrate how this approach captures meaningful semantic shifts and can help improve other tasks such as the contrastive viewpoint summarization and ideology detection (measured as classification accuracy) in political texts.$$$We also show that the two laws of semantic change which were empirically shown to hold for temporal shifts also hold for shifts across viewpoints.$$$These laws state that frequent words are less likely to shift meaning while words with many senses are more likely to do so.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS OBJECTIVES METHODS RESULTS RESULTS/CONCLUSIONS RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06039,"We study the underlying structure of data (approximately) generated from a union of independent subspaces.$$$Traditional methods learn only one subspace, failing to discover the multi-subspace structure, while state-of-the-art methods analyze the multi-subspace structure using data themselves as the dictionary, which cannot offer the explicit basis to span each subspace and are sensitive to errors via an indirect representation.$$$Additionally, they also suffer from a high computational complexity, being quadratic or cubic to the sample size.$$$To tackle all these problems, we propose a method, called Matrix Factorization with Column L0-norm constraint (MFC0), that can simultaneously learn the basis for each subspace, generate a direct sparse representation for each data sample, as well as removing errors in the data in an efficient way.$$$Furthermore, we develop a first-order alternating direction algorithm, whose computational complexity is linear to the sample size, to stably and effectively solve the nonconvex objective function and non- smooth l0-norm constraint of MFC0.$$$Experimental results on both synthetic and real-world datasets demonstrate that besides the superiority over traditional and state-of-the-art methods for subspace clustering, data reconstruction, error correction, MFC0 also shows its uniqueness for multi-subspace basis learning and direct sparse representation.",OBJECTIVES BACKGROUND BACKGROUND METHODS METHODS CONCLUSIONS
D05615,"We introduce a high-performance virtual machine (VM) written in a numerically fast language like Fortran or C to evaluate very large expressions.$$$We discuss the general concept of how to perform computations in terms of a VM and present specifically a VM that is able to compute tree-level cross sections for any number of external legs, given the corresponding byte code from the optimal matrix element generator, O'Mega.$$$Furthermore, this approach allows to formulate the parallel computation of a single phase space point in a simple and obvious way.$$$We analyze hereby the scaling behaviour with multiple threads as well as the benefits and drawbacks that are introduced with this method.$$$Our implementation of a VM can run faster than the corresponding native, compiled code for certain processes and compilers, especially for very high multiplicities, and has in general runtimes in the same order of magnitude.$$$By avoiding the tedious compile and link steps, which may fail for source code files of gigabyte sizes, new processes or complex higher order corrections that are currently out of reach could be evaluated with a VM given enough computing power.",OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS
D02804,"Nonnegative Matrix Factorization (NMF) is the problem of approximating a nonnegative matrix with the product of two low-rank nonnegative matrices and has been shown to be particularly useful in many applications, e.g., in text mining, image processing, computational biology, etc.$$$In this paper, we explain how algorithms for NMF can be embedded into the framework of multilevel methods in order to accelerate their convergence.$$$This technique can be applied in situations where data admit a good approximate representation in a lower dimensional space through linear transformations preserving nonnegativity.$$$A simple multilevel strategy is described and is experimentally shown to speed up significantly three popular NMF algorithms (alternating nonnegative least squares, multiplicative updates and hierarchical alternating least squares) on several standard image datasets.",BACKGROUND OBJECTIVES OBJECTIVES/CONCLUSIONS METHODS/RESULTS
D06070,"Detection of transitions between broad phonetic classes in a speech signal is an important problem which has applications such as landmark detection and segmentation.$$$The proposed hierarchical method detects silence to non-silence transitions, high amplitude (mostly sonorants) to low ampli- tude (mostly fricatives/affricates/stop bursts) transitions and vice-versa.$$$A subset of the extremum (minimum or maximum) samples between every pair of successive zero-crossings is selected above a second pass threshold, from each bandpass filtered speech signal frame.$$$Relative to the mid-point (reference) of a frame, locations of the first and the last extrema lie on either side, if the speech signal belongs to a homogeneous segment; else, both these locations lie on the left or the right side of the reference, indicating a transition frame.$$$When tested on the entire TIMIT database, of the transitions detected, 93.6% are within a tolerance of 20 ms from the hand labeled boundaries.$$$Sonorant, unvoiced non-sonorant and silence classes and their respective onsets are detected with an accuracy of about 83.5% for the same tolerance.$$$The results are as good as, and in some respects better than the state-of-the-art methods for similar tasks.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS CONCLUSIONS
D03064,"In this paper, we study the interplay between the epidemic spreading and the diffusion of awareness in multiplex networks.$$$In the model, an infectious disease can spread in one network representing the paths of epidemic spreading (contact network), leading to the diffusion of awareness in the other network (information network), and then the diffusion of awareness will cause individuals to take social distances, which in turn affects the epidemic spreading.$$$As for the diffusion of awareness, we assume that, on the one hand, individuals can be informed by other aware neighbors in information network, on the other hand, the susceptible individuals can be self-awareness induced by the infected neighbors in the contact networks (local information) or mass media (global information).$$$Through Markov chain approach and numerical computations, we find that the density of infected individuals and the epidemic threshold can be affected by the structures of the two networks and the effective transmission rate of the awareness.$$$However, we prove that though the introduction of the self-awareness can lower the density of infection, which cannot increase the epidemic threshold no matter of the local information or global information.$$$Our finding is remarkably different to many previous results--local information based behavioral response can alter the epidemic threshold.",OBJECTIVES OBJECTIVES OBJECTIVES RESULTS RESULTS CONCLUSIONS
D03706,"Electroluminescence (EL) imaging is a useful modality for the inspection of photovoltaic (PV) modules.$$$EL images provide high spatial resolution, which makes it possible to detect even finest defects on the surface of PV modules.$$$However, the analysis of EL images is typically a manual process that is expensive, time-consuming, and requires expert knowledge of many different types of defects.$$$In this work, we investigate two approaches for automatic detection of such defects in a single image of a PV cell.$$$The approaches differ in their hardware requirements, which are dictated by their respective application scenarios.$$$The more hardware-efficient approach is based on hand-crafted features that are classified in a Support Vector Machine (SVM).$$$To obtain a strong performance, we investigate and compare various processing variants.$$$The more hardware-demanding approach uses an end-to-end deep Convolutional Neural Network (CNN) that runs on a Graphics Processing Unit (GPU).$$$Both approaches are trained on 1,968 cells extracted from high resolution EL intensity images of mono- and polycrystalline PV modules.$$$The CNN is more accurate, and reaches an average accuracy of 88.42%.$$$The SVM achieves a slightly lower average accuracy of 82.44%, but can run on arbitrary hardware.$$$Both automated approaches make continuous, highly accurate monitoring of PV cells feasible.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS OBJECTIVES/METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D06240,"Neural Style Transfer based on Convolutional Neural Networks (CNN) aims to synthesize a new image that retains the high-level structure of a content image, rendered in the low-level texture of a style image.$$$This is achieved by constraining the new image to have high-level CNN features similar to the content image, and lower-level CNN features similar to the style image.$$$However in the traditional optimization objective, low-level features of the content image are absent, and the low-level features of the style image dominate the low-level detail structures of the new image.$$$Hence in the synthesized image, many details of the content image are lost, and a lot of inconsistent and unpleasing artifacts appear.$$$As a remedy, we propose to steer image synthesis with a novel loss function: the Laplacian loss.$$$The Laplacian matrix (""Laplacian"" in short), produced by a Laplacian operator, is widely used in computer vision to detect edges and contours.$$$The Laplacian loss measures the difference of the Laplacians, and correspondingly the difference of the detail structures, between the content image and a new image.$$$It is flexible and compatible with the traditional style transfer constraints.$$$By incorporating the Laplacian loss, we obtain a new optimization objective for neural style transfer named Lapstyle.$$$Minimizing this objective will produce a stylized image that better preserves the detail structures of the content image and eliminates the artifacts.$$$Experiments show that Lapstyle produces more appealing stylized images with less artifacts, without compromising their ""stylishness"".",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS
D06224,"Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language.$$$However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective.$$$Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior.$$$We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM.$$$We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans.$$$Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.",BACKGROUND OBJECTIVES METHODS METHODS CONCLUSIONS CONCLUSIONS
D03965,"In this paper, we propose a novel approach (SAPEO) to support the survival selection process in multi-objective evolutionary algorithms with surrogate models - it dynamically chooses individuals to evaluate exactly based on the model uncertainty and the distinctness of the population.$$$We introduce variants that differ in terms of the risk they allow when doing survival selection.$$$Here, the anytime performance of different SAPEO variants is evaluated in conjunction with an SMS-EMOA using the BBOB bi-objective benchmark.$$$We compare the obtained results with the performance of the regular SMS-EMOA, as well as another surrogate-assisted approach.$$$The results open up general questions about the applicability and required conditions for surrogate-assisted multi-objective evolutionary algorithms to be tackled in the future.",OBJECTIVES/METHODS METHODS METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS
D02787,"This paper presents data analysis from a course on Software Engineering in an effort to identify metrics and techniques that would allow instructor to act proactively and identify patterns of low engagement and inefficient peer collaboration.$$$Over the last two terms, 106 students in their second year of studies formed 20 groups and worked collaboratively to develop video games.$$$Throughout the lab, students have to use a variety of tools for managing and developing their projects, such as software version control, static analysis tools, wikis, mailing lists, etc.$$$The students are also supported by weekly meetings with teaching assistants and instructors regarding group progress, code quality, and management issues.$$$Through these meetings and their interactions with the software tools, students leave a detailed trace of data related to their individual engagement and their collaboration behavior in their groups.$$$The paper provides discussion on the different source of data that can be monitored, and present preliminary results on how these data can be used to analyze students' activity.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D00151,"This paper addresses the general problem of blind echo retrieval, i.e., given M sensors measuring in the discrete-time domain M mixtures of K delayed and attenuated copies of an unknown source signal, can the echo locations and weights be recovered?$$$This problem has broad applications in fields such as sonars, seismol-ogy, ultrasounds or room acoustics.$$$It belongs to the broader class of blind channel identification problems, which have been intensively studied in signal processing.$$$Existing methods in the literature proceed in two steps: (i) blind estimation of sparse discrete-time filters and (ii) echo information retrieval by peak-picking on filters.$$$The precision of these methods is fundamentally limited by the rate at which the signals are sampled: estimated echo locations are necessary on-grid, and since true locations never match the sampling grid, the weight estimation precision is impacted.$$$This is the so-called basis-mismatch problem in compressed sensing.$$$We propose a radically different approach to the problem, building on the framework of finite-rate-of-innovation sampling.$$$The approach operates directly in the parameter-space of echo locations and weights, and enables near-exact blind and off-grid echo retrieval from discrete-time measurements.$$$It is shown to outperform conventional methods by several orders of magnitude in precision.",BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS METHODS RESULTS
D03131,"This paper evaluates algorithms for classification and outlier detection accuracies in temporal data.$$$We focus on algorithms that train and classify rapidly and can be used for systems that need to incorporate new data regularly.$$$Hence, we compare the accuracy of six fast algorithms using a range of well-known time-series datasets.$$$The analyses demonstrate that the choice of algorithm is task and data specific but that we can derive heuristics for choosing.$$$Gradient Boosting Machines are generally best for classification but there is no single winner for outlier detection though Gradient Boosting Machines (again) and Random Forest are better.$$$Hence, we recommend running evaluations of a number of algorithms using our heuristics.",BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS RESULTS CONCLUSIONS
D02921,"Current paper reports the advantages of the application of GitHub and LaTeX for the MSc thesis writing.$$$The existing code-based program implemented in GitHub portal provides a great tool for scientists and students for data sharing and notification of the co-workers, tutors and supervisors involved in research about actual updates.$$$It enables to connect collaborators to share around current results, release datasets and updates and more.$$$Using standard command-line interface GitHub allows registered users to push repositories on the website.$$$The availability of both public and private repositories enables to share current data updates with target audience: e.g. unpublished research work only for co-authors or supervisors, or vice versa.$$$Therefore, there is a need in academic centres and universities to strongly popularize and increase the use of GitHub for student works.$$$The case study is given on the graduate study: an MSc work written and maintained using open source GitHub service at the University of Twente, Faculty of Geo-Information Science and Earth Observation (Netherlands).$$$It reports my successful experience of writing MSc thesis based on the effective combination of LaTeX and GitHub.",BACKGROUND/OBJECTIVES BACKGROUND RESULTS METHODS RESULTS/CONCLUSIONS OTHERS BACKGROUND METHODS
D04963,"To analyze the failure risk of asynchronous digital circuits the time-parameter is introduced into the Boolean algebra replacing the arithmetic operations by logical operations.$$$There considered an example of construction of signals passing through the logical elements, using the described below mathematical apparatus.",OBJECTIVES/METHODS RESULTS
D06463,"This paper studies change point detection on networks with community structures.$$$It proposes a framework that can detect both local and global changes in networks efficiently.$$$Importantly, it can clearly distinguish the two types of changes.$$$The framework design is generic and as such several state-of-the-art change point detection algorithms can fit in this design.$$$Experiments on both synthetic and real-world networks show that this framework can accurately detect changes while achieving up to 800X speedup.",OBJECTIVES CONCLUSIONS RESULTS METHODS RESULTS
D00279,"Majority of the face recognition algorithms use query faces captured from uncontrolled, in the wild, environment.$$$Often caused by the cameras limited capabilities, it is common for these captured facial images to be blurred or low resolution.$$$Super resolution algorithms are therefore crucial in improving the resolution of such images especially when the image size is small requiring enlargement.$$$This paper aims to demonstrate the effect of one of the state-of-the-art algorithms in the field of image super resolution.$$$To demonstrate the functionality of the algorithm, various before and after 3D face alignment cases are provided using the images from the Labeled Faces in the Wild (lfw).$$$Resulting images are subject to testing on a closed set face recognition protocol using unsupervised algorithms with high dimension extracted features.$$$The inclusion of super resolution algorithm resulted in significant improved recognition rate over recently reported results obtained from unsupervised algorithms.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D04659,"Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction.$$$We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM.$$$Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa.$$$We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM.$$$Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view.$$$Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.",BACKGROUND/OBJECTIVES METHODS METHODS RESULTS METHODS RESULTS
D04826,"Website can be easily design but to efficient user navigation is not a easy task since user behavior is keep changing and developer view is quite different from what user wants, so to improve navigation one way is reorganization of website structure.$$$For reorganization here proposed strategy is farthest first traversal clustering algorithm perform clustering on two numeric parameters and for finding frequent traversal path of user Apriori algorithm is used.$$$Our aim is to perform reorganization with fewer changes in website structure.",OBJECTIVES OBJECTIVES OBJECTIVES
D02440,"This paper proposes a new objective metric of exceptional motion in VR video contents for VR sickness assessment.$$$In VR environment, VR sickness can be caused by several factors which are mismatched motion, field of view, motion parallax, viewing angle, etc.$$$Similar to motion sickness, VR sickness can induce a lot of physical symptoms such as general discomfort, headache, stomach awareness, nausea, vomiting, fatigue, and disorientation.$$$To address the viewing safety issues in virtual environment, it is of great importance to develop an objective VR sickness assessment method that predicts and analyses the degree of VR sickness induced by the VR content.$$$The proposed method takes into account motion information that is one of the most important factors in determining the overall degree of VR sickness.$$$In this paper, we detect the exceptional motion that is likely to induce VR sickness.$$$Spatio-temporal features of the exceptional motion in the VR video content are encoded using a convolutional autoencoder.$$$For objectively assessing the VR sickness, the level of exceptional motion in VR video content is measured by using the convolutional autoencoder as well.$$$The effectiveness of the proposed method has been successfully evaluated by subjective assessment experiment using simulator sickness questionnaires (SSQ) in VR environment.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D02039,"Evolutionary games on networks traditionally involve the same game at each interaction.$$$Here we depart from this assumption by considering mixed games, where the game played at each interaction is drawn uniformly at random from a set of two different games.$$$While in well-mixed populations the random mixture of the two games is always equivalent to the average single game, in structured populations this is not always the case.$$$We show that the outcome is in fact strongly dependent on the distance of separation of the two games in the parameter space.$$$Effectively, this distance introduces payoff heterogeneity, and the average game is returned only if the heterogeneity is small.$$$For higher levels of heterogeneity the distance to the average game grows, which often involves the promotion of cooperation.$$$The presented results support preceding research that highlights the favorable role of heterogeneity regardless of its origin, and they also emphasize the importance of the population structure in amplifying facilitators of cooperation.",BACKGROUND OBJECTIVES/METHODS BACKGROUND/OBJECTIVES RESULTS/CONCLUSIONS RESULTS RESULTS CONCLUSIONS
D01992,"Representation learning is an essential problem in a wide range of applications and it is important for performing downstream tasks successfully.$$$In this paper, we propose a new model that learns coupled representations of domains, intents, and slots by taking advantage of their hierarchical dependency in a Spoken Language Understanding system.$$$Our proposed model learns the vector representation of intents based on the slots tied to these intents by aggregating the representations of the slots.$$$Similarly, the vector representation of a domain is learned by aggregating the representations of the intents tied to a specific domain.$$$To the best of our knowledge, it is the first approach to jointly learning the representations of domains, intents, and slots using their hierarchical relationships.$$$The experimental results demonstrate the effectiveness of the representations learned by our model, as evidenced by improved performance on the contextual cross-domain reranking task.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D01588,We propose a Markov chain simulation method to generate simple connected random graphs with a specified degree sequence and level of clustering.$$$The networks generated by our algorithm are random in all other respects and can thus serve as generic models for studying the impacts of degree distributions and clustering on dynamical processes as well as null models for detecting other structural properties in empirical networks.,OBJECTIVES RESULTS
D03000,"In this paper, we propose a novel approach for verification of on-line signatures based on user dependent feature selection and symbolic representation.$$$Unlike other signature verification methods, which work with same features for all users, the proposed approach introduces the concept of user dependent features.$$$It exploits the typicality of each and every user to select different features for different users.$$$Initially all possible features are extracted for all users and a method of feature selection is employed for selecting user dependent features.$$$The selected features are clustered using Fuzzy C means algorithm.$$$In order to preserve the intra-class variation within each user, we recommend to represent each cluster in the form of an interval valued symbolic feature vector.$$$A method of signature verification based on the proposed cluster based symbolic representation is also presented.$$$Extensive experimentations are conducted on MCYT-100 User (DB1) and MCYT-330 User (DB2) online signature data sets to demonstrate the effectiveness of the proposed novel approach.",OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS METHODS CONCLUSIONS
D02474,"We present a novel method for high detail-preserving human avatar creation from monocular video.$$$A parameterized body model is refined and optimized to maximally resemble subjects from a video showing them from all sides.$$$Our avatars feature a natural face, hairstyle, clothes with garment wrinkles, and high-resolution texture.$$$Our paper contributes facial landmark and shading-based human body shape refinement, a semantic texture prior, and a novel texture stitching strategy, resulting in the most sophisticated-looking human avatars obtained from a single video to date.$$$Numerous results show the robustness and versatility of our method.$$$A user study illustrates its superiority over the state-of-the-art in terms of identity preservation, level of detail, realism, and overall user preference.",OTHERS OBJECTIVES/METHODS RESULTS CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00135,"Social media for news consumption is becoming increasingly popular due to its easy access, fast dissemination, and low cost.$$$However, social media also enable the wide propagation of ""fake news"", i.e., news with intentionally false information.$$$Fake news on social media poses significant negative societal effects, and also presents unique challenges.$$$To tackle the challenges, many existing works exploit various features, from a network perspective, to detect and mitigate fake news.$$$In essence, news dissemination ecosystem involves three dimensions on social media, i.e., a content dimension, a social dimension, and a temporal dimension.$$$In this chapter, we will review network properties for studying fake news, introduce popular network types and how these networks can be used to detect and mitigation fake news on social media.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS
D02267,"Recent studies show the increasing popularity of distributed cloud applications, which are composed of multiple microservices.$$$Besides their known benefits, microservice architecture also enables to mix and match cloud applications and Network Function Virtualization (NFV) services (service chains), which are composed of Virtual Network Functions (VNFs).$$$Provisioning complex services containing VNFs and microservices in a combined NFV/cloud platform can enhance service quality and optimise cost.$$$Such a platform can be based on the multi-cloud concept.$$$However, current multi-cloud solutions do not support NFV requirements, making them inadequate to support complex services.$$$In this paper, we investigate these challenges and propose a solution for jointly managing and orchestrating microservices and virtual network functions.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES
D00147,"Acoustic event detection for content analysis in most cases relies on lots of labeled data.$$$However, manually annotating data is a time-consuming task, which thus makes few annotated resources available so far.$$$Unlike audio event detection, automatic audio tagging, a multi-label acoustic event classification task, only relies on weakly labeled data.$$$This is highly desirable to some practical applications using audio analysis.$$$In this paper we propose to use a fully deep neural network (DNN) framework to handle the multi-label classification task in a regression way.$$$Considering that only chunk-level rather than frame-level labels are available, the whole or almost whole frames of the chunk were fed into the DNN to perform a multi-label regression for the expected tags.$$$The fully DNN, which is regarded as an encoding function, can well map the audio features sequence to a multi-tag vector.$$$A deep pyramid structure was also designed to extract more robust high-level features related to the target tags.$$$Further improved methods were adopted, such as the Dropout and background noise aware training, to enhance its generalization capability for new audio recordings in mismatched environments.$$$Compared with the conventional Gaussian Mixture Model (GMM) and support vector machine (SVM) methods, the proposed fully DNN-based method could well utilize the long-term temporal information with the whole chunk as the input.$$$The results show that our approach obtained a 15% relative improvement compared with the official GMM-based method of DCASE 2016 challenge.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES METHODS OBJECTIVES/METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D01695,"Long Short-Term Memory networks trained with gradient descent and back-propagation have received great success in various applications.$$$However, point estimation of the weights of the networks is prone to over-fitting problems and lacks important uncertainty information associated with the estimation.$$$However, exact Bayesian neural network methods are intractable and non-applicable for real-world applications.$$$In this study, we propose an approximate estimation of the weights uncertainty using Ensemble Kalman Filter, which is easily scalable to a large number of weights.$$$Furthermore, we optimize the covariance of the noise distribution in the ensemble update step using maximum likelihood estimation.$$$To assess the proposed algorithm, we apply it to outlier detection in five real-world events retrieved from the Twitter platform.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS
D02488,"XML access control policies involving updates may contain security flaws, here called inconsistencies, in which a forbidden operation may be simulated by performing a sequence of allowed operations.$$$This paper investigates the problem of deciding whether a policy is consistent, and if not, how its inconsistencies can be repaired.$$$We consider policies expressed in terms of annotated DTDs defining which operations are allowed or denied for the XML trees that are instances of the DTD.$$$We show that consistency is decidable in PTIME for such policies and that consistent partial policies can be extended to unique ""least-privilege"" consistent total policies.$$$We also consider repair problems based on deleting privileges to restore consistency, show that finding minimal repairs is NP-complete, and give heuristics for finding repairs.",BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D01868,"Among the patch-based image denoising processing methods, smooth ordering of local patches (patch ordering) has been shown to give state-of-art results.$$$For image denoising the patch ordering method forms two large TSPs (Traveling Salesman Problem) comprised of nodes in N-dimensional space.$$$Ten approximate solutions of the two large TSPs are then used in a filtering process to form the reconstructed image.$$$Use of large TSPs makes patch ordering a computationally intensive method.$$$A modified patch ordering method for image denoising is proposed.$$$In the proposed method, several smaller-sized TSPs are formed and the filtering process varied to work with solutions of these smaller TSPs.$$$In terms of PSNR, denoising results of the proposed method differed by 0.032 dB to 0.016 dB on average.$$$In original method, solving TSPs was observed to consume 85% of execution time.$$$In proposed method, the time for solving TSPs can be reduced to half of the time required in original method.$$$The proposed method can denoise images in 40% less time.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS BACKGROUND CONCLUSIONS CONCLUSIONS
D05939,"In this paper, we propose a distributed primal-dual algorithm for computation of a generalized Nash equilibrium (GNE) in noncooperative games over network systems.$$$In the considered game, not only each player's local objective function depends on other players' decisions, but also the feasible decision sets of all the players are coupled together with a globally shared affine inequality constraint.$$$Adopting the variational GNE, that is the solution of a variational inequality, as a refinement of GNE, we introduce a primal-dual algorithm that players can use to seek it in a distributed manner.$$$Each player only needs to know its local objective function, local feasible set, and a local block of the affine constraint.$$$Meanwhile, each player only needs to observe the decisions on which its local objective function explicitly depends through the interference graph and share information related to multipliers with its neighbors through a multiplier graph.$$$Through a primal-dual analysis and an augmentation of variables, we reformulate the problem as finding the zeros of a sum of monotone operators.$$$Our distributed primal-dual algorithm is based on forward-backward operator splitting methods.$$$We prove its convergence to the variational GNE for fixed step-sizes under some mild assumptions.$$$Then a distributed algorithm with inertia is also introduced and analyzed for variational GNE seeking.$$$Finally, numerical simulations for network Cournot competition are given to illustrate the algorithm efficiency and performance.",OBJECTIVES BACKGROUND METHODS BACKGROUND METHODS METHODS METHODS RESULTS RESULTS RESULTS
D01689,"Human and artificial organizations may be described as networks of interacting parts.$$$Those parts exchange data and control information and, as a result of these interactions, organizations produce emergent behaviors and purposes -- traits the characterize ""the whole"" as ""greater than the sum of its parts"".$$$In this chapter it is argued that, rather than a static and immutable property, emergence should be interpreted as the result of dynamic interactions between forces of opposite sign: centripetal (positive) forces strengthening emergence by consolidating the whole and centrifugal (negative) forces that weaken the social persona and as such are detrimental to emergence.$$$The result of this interaction is called in this chapter as ""quality of emergence"".$$$This problem is discussed in the context of a particular class of organizations: conventional hierarchies.$$$We highlight how traditional designs produce behaviors that may severely impact the quality of emergence.$$$Finally we discuss a particular class of organizations that do not suffer from the limitations typical of strict hierarchies and result in greater quality of emergence.$$$In some case, however, these enhancements are counterweighted by a reduced degree of controllability and verifiability.",BACKGROUND BACKGROUND OBJECTIVES CONCLUSIONS METHODS RESULTS RESULTS RESULTS/CONCLUSIONS
D00195,"Business Process Management (BPM) is a central element of today organizations.$$$Despite over the years its main focus has been the support of processes in highly controlled domains, nowadays many domains of interest to the BPM community are characterized by ever-changing requirements, unpredictable environments and increasing amounts of data that influence the execution of process instances.$$$Under such dynamic conditions, BPM systems must increase their level of automation to provide the reactivity and flexibility necessary for process management.$$$On the other hand, the Artificial Intelligence (AI) community has concentrated its efforts on investigating dynamic domains that involve active control of computational entities and physical devices (e.g., robots, software agents, etc.).$$$In this context, Automated Planning, which is one of the oldest areas in AI, is conceived as a model-based approach to synthesize autonomous behaviours in automated way from a model.$$$In this paper, we discuss how automated planning techniques can be leveraged to enable new levels of automation and support for business processing, and we show some concrete examples of their successful application to the different stages of the BPM life cycle.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS/CONCLUSIONS
D03998,"Structured prediction is ubiquitous in applications of machine learning such as knowledge extraction and natural language processing.$$$Structure often can be formulated in terms of logical constraints.$$$We consider the question of how to perform efficient active learning in the presence of logical constraints among variables inferred by different classifiers.$$$We propose several methods and provide theoretical results that demonstrate the inappropriateness of employing uncertainty guided sampling, a commonly used active learning method.$$$Furthermore, experiments on ten different datasets demonstrate that the methods significantly outperform alternatives in practice.$$$The results are of practical significance in situations where labeled data is scarce.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D03076,"Many tasks in music information retrieval, such as recommendation, and playlist generation for online radio, fall naturally into the query-by-example setting, wherein a user queries the system by providing a song, and the system responds with a list of relevant or similar song recommendations.$$$Such applications ultimately depend on the notion of similarity between items to produce high-quality results.$$$Current state-of-the-art systems employ collaborative filter methods to represent musical items, effectively comparing items in terms of their constituent users.$$$While collaborative filter techniques perform well when historical data is available for each item, their reliance on historical data impedes performance on novel or unpopular items.$$$To combat this problem, practitioners rely on content-based similarity, which naturally extends to novel items, but is typically out-performed by collaborative filter methods.$$$In this article, we propose a method for optimizing contentbased similarity by learning from a sample of collaborative filter data.$$$The optimized content-based similarity metric can then be applied to answer queries on novel and unpopular items, while still maintaining high recommendation accuracy.$$$The proposed system yields accurate and efficient representations of audio content, and experimental results show significant improvements in accuracy over competing content-based recommendation techniques.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D03614,"Applications in many domains require processing moving object trajectories.$$$In this work, we focus on a trajectory similarity search that finds all trajectories within a given distance of a query trajectory over a time interval, which we call the distance threshold similarity search.$$$We develop three indexing strategies with spatial, temporal and spatiotemporal selectivity for the GPU that differ significantly from indexes suitable for the CPU, and show the conditions under which each index achieves good performance.$$$Furthermore, we show that the GPU implementations outperform multithreaded CPU implementations in a range of experimental scenarios, making the GPU an attractive technology for processing moving object trajectories.$$$We test our implementations on two synthetic and one real-world dataset of a galaxy merger.",BACKGROUND BACKGROUND/OBJECTIVES METHODS RESULTS RESULTS
D00546,"The original problem of supervised classification considers the task of automatically assigning objects to their respective classes on the basis of numerical measurements derived from these objects.$$$Classifiers are the tools that implement the actual functional mapping from these measurements---also called features or inputs---to the so-called class label---or output.$$$The fields of pattern recognition and machine learning study ways of constructing such classifiers.$$$The main idea behind supervised methods is that of learning from examples: given a number of example input-output relations, to what extent can the general mapping be learned that takes any new and unseen feature vector to its correct class?$$$This chapter provides a basic introduction to the underlying ideas of how to come to a supervised classification problem.$$$In addition, it provides an overview of some specific classification techniques, delves into the issues of object representation and classifier evaluation, and (very) briefly covers some variations on the basic supervised classification task that may also be of interest to the practitioner.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OTHERS OTHERS
D01856,"Graph sampling addresses the problem of selecting a node subset in a graph to collect samples, so that a K-bandlimited signal can be reconstructed in high fidelity.$$$Assuming an independent and identically distributed (i.i.d.) noise model, minimizing the expected mean square error (MMSE) leads to the known A-optimality criterion for graph sampling, which is expensive to compute and difficult to optimize.$$$In this paper, we propose an augmented objective based on Neumann series that well approximates the original criterion and is amenable to greedy optimization.$$$Specifically, we show that a shifted A-optimal criterion can be equivalently written as a function of an ideal low-pass (LP) graph filter, which in turn can be approximated efficiently via fast graph Fourier transform (FGFT).$$$Minimizing the new objective, we select nodes greedily without large matrix inversions using a matrix inverse lemma.$$$Further, for the dynamic network case where node availability varies across time, we propose an extended sampling strategy that replaces offline samples one-by-one in the selected set.$$$For signal reconstruction, we propose an accompanied biased signal recovery strategy that reuses the approximated filter from sampling.$$$Experiments show that our reconstruction is more robust to large noise than the least square (LS) solution, and our sampling strategy far outperforms several existing schemes.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS
D02654,"An important property of programming language semantics is that they should be compositional.$$$However, unstructured low-level code contains goto-like commands making it hard to define a semantics that is compositional.$$$In this paper, we follow the ideas of Saabas and Uustalu to structure low-level code.$$$This gives us the possibility to define a compositional denotational semantics based on least fixed points to allow for the use of inductive verification methods.$$$We capture the semantics of communication using finite traces similar to the denotations of CSP.$$$In addition, we examine properties of this semantics and give an example that demonstrates reasoning about communication and jumps.$$$With this semantics, we lay the foundations for a proof calculus that captures both, the semantics of unstructured low-level code and communication.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS CONCLUSIONS
D05653,"Machine learning algorithms have reached mainstream status and are widely deployed in many applications.$$$The accuracy of such algorithms depends significantly on the size of the underlying training dataset; in reality a small or medium sized organization often does not have the necessary data to train a reasonably accurate model.$$$For such organizations, a realistic solution is to train their machine learning models based on their joint dataset (which is a union of the individual ones).$$$Unfortunately, privacy concerns prevent them from straightforwardly doing so.$$$While a number of privacy-preserving solutions exist for collaborating organizations to securely aggregate the parameters in the process of training the models, we are not aware of any work that provides a rational framework for the participants to precisely balance the privacy loss and accuracy gain in their collaboration.$$$In this paper, by focusing on a two-player setting, we model the collaborative training process as a two-player game where each player aims to achieve higher accuracy while preserving the privacy of its own dataset.$$$We introduce the notion of Price of Privacy, a novel approach for measuring the impact of privacy protection on the accuracy in the proposed framework.$$$Furthermore, we develop a game-theoretical model for different player types, and then either find or prove the existence of a Nash Equilibrium with regard to the strength of privacy protection for each player.$$$Using recommendation systems as our main use case, we demonstrate how two players can make practical use of the proposed theoretical framework, including setting up the parameters and approximating the non-trivial Nash Equilibrium.",BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS METHODS/RESULTS/CONCLUSIONS
D01584,"A method is presented for solving the discrete-time finite-horizon Linear Quadratic Regulator (LQR) problem subject to auxiliary linear equality constraints, such as fixed end-point constraints.$$$The method explicitly determines an affine relationship between the control and state variables, as in standard Riccati recursion, giving rise to feedback control policies that account for constraints.$$$Since the linearly-constrained LQR problem arises commonly in robotic trajectory optimization, having a method that can efficiently compute these solutions is important.$$$We demonstrate some of the useful properties and interpretations of said control policies, and we compare the computation time of our method against existing methods.",OBJECTIVES OBJECTIVES/RESULTS BACKGROUND OBJECTIVES/RESULTS
D02562,"Traditional event detection methods heavily rely on manually engineered rich features.$$$Recent deep learning approaches alleviate this problem by automatic feature engineering.$$$But such efforts, like tradition methods, have so far only focused on single-token event mentions, whereas in practice events can also be a phrase.$$$We instead use forward-backward recurrent neural networks (FBRNNs) to detect events that can be either words or phrases.$$$To the best our knowledge, this is one of the first efforts to handle multi-word events and also the first attempt to use RNNs for event detection.$$$Experimental results demonstrate that FBRNN is competitive with the state-of-the-art methods on the ACE 2005 and the Rich ERE 2015 event detection tasks.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D05550,"During the past years, psychological diseases related to unhealthy work environments, such as burnouts, have drawn more and more public attention.$$$One of the known causes of these affective problems is time pressure.$$$In order to form a theoretical background for time pressure detection in software repositories, this paper combines interdisciplinary knowledge by analyzing 1270 papers found on Scopus database and containing terms related to time pressure.$$$By clustering those papers based on their abstract, we show that time pressure has been widely studied across different fields, but relatively little in software engineering.$$$From a literature review of the most relevant papers, we infer a list of testable hypotheses that we want to verify in future studies in order to assess the impact of time pressures on software developers mental health.",BACKGROUND BACKGROUND METHODS METHODS RESULTS
D01893,"We consider a computational model which is known as set automata.$$$The set automata are one-way finite automata with an additional storage---the set.$$$There are two kinds of set automata---the deterministic and the nondeterministic ones.$$$We denote them as DSA and NSA respectively.$$$The model was introduced by M. Kutrib, A. Malcher, M. Wendlandt in 2014.$$$It was shown that DSA-languages look similar to DCFL due to their closure properties and NSA-languages look similar to CFL due to their undecidability properties.$$$In this paper we show that this similarity is natural: we prove that languages recognizable by NSA form a rational cone, so as CFL.$$$The main topic of this paper is computational complexity: we prove that   - languages recognizable by DSA belong to P and there are P-complete languages among them;   - languages recognizable by NSA are in NP and there are NP-complete languages among them;   - the word membership problem is P-complete for DSA without epsilon-loops and PSPACE-complete for general DSA;   - the emptiness problem is in PSPACE for NSA and, moreover, it is PSPACE-complete for DSA.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND RESULTS RESULTS/CONCLUSIONS
D02432,"One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samples and detecting various levels of mode collapse.$$$In this work, we construct a novel measure of performance of a GAN by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation.$$$Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data.$$$We test the obtained metric on various real-life models and datasets and demonstrate that our method provides new insights into properties of GANs.",BACKGROUND/OBJECTIVES METHODS/RESULTS RESULTS METHODS/RESULTS
D01792,"Bedside caregivers assess infants' pain at constant intervals by observing specific behavioral and physiological signs of pain.$$$This standard has two main limitations.$$$The first limitation is the intermittent assessment of pain, which might lead to missing pain when the infants are left unattended.$$$Second, it is inconsistent since it depends on the observer's subjective judgment and differs between observers.$$$The intermittent and inconsistent assessment can induce poor treatment and, therefore, cause serious long-term consequences.$$$To mitigate these limitations, the current standard can be augmented by an automated system that monitors infants continuously and provides quantitative and consistent assessment of pain.$$$Several automated methods have been introduced to assess infants' pain automatically based on analysis of behavioral or physiological pain indicators.$$$This paper comprehensively reviews the automated approaches (i.e., approaches to feature extraction) for analyzing infants' pain and the current efforts in automatic pain recognition.$$$In addition, it reviews the databases available to the research community and discusses the current limitations of the automated pain assessment.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS
D02781,"Recently the second two authors characterized quasiperiodic Sturmian words, proving that a Sturmian word is non-quasiperiodic if and only if it is an infinite Lyndon word.$$$Here we extend this study to episturmian words (a natural generalization of Sturmian words) by describing all the quasiperiods of an episturmian word, which yields a characterization of quasiperiodic episturmian words in terms of their ""directive words"".$$$Even further, we establish a complete characterization of all episturmian words that are Lyndon words.$$$Our main results show that, unlike the Sturmian case, there is a much wider class of episturmian words that are non-quasiperiodic, besides those that are infinite Lyndon words.$$$Our key tools are morphisms and directive words, in particular ""normalized"" directive words, which we introduced in an earlier paper.$$$Also of importance is the use of ""return words"" to characterize quasiperiodic episturmian words, since such a method could be useful in other contexts.",BACKGROUND RESULTS RESULTS CONCLUSIONS METHODS METHODS
D01251,"Information spreading has been studied for decades, but its underlying mechanism is still under debate, especially for those ones spreading extremely fast through Internet.$$$By focusing on the information spreading data of six typical events on Sina Weibo, we surprisingly find that the spreading of modern information shows some new features, i.e. either extremely fast or slow, depending on the individual events.$$$To understand its mechanism, we present a Susceptible-Accepted-Recovered (SAR) model with both information sensitivity and social reinforcement.$$$Numerical simulations show that the model can reproduce the main spreading patterns of the six typical events.$$$By this model we further reveal that the spreading can be speeded up by increasing either the strength of information sensitivity or social reinforcement.$$$Depending on the transmission probability and information sensitivity, the final accepted size can change from continuous to discontinuous transition when the strength of the social reinforcement is large.$$$Moreover, an edge-based compartmental theory is presented to explain the numerical results.$$$These findings may be of significance on the control of information spreading in modern society.",BACKGROUND/OBJECTIVES RESULTS METHODS RESULTS RESULTS RESULTS METHODS CONCLUSIONS
D02720,"The paper discusses various applications of permutation group theory in the synthesis of reversible logic circuits consisting of Toffoli gates with negative control lines.$$$An asymptotically optimal synthesis algorithm for circuits consisting of gates from the NCT library is described.$$$An algorithm for gate complexity reduction, based on equivalent replacements of gates compositions, is introduced.$$$A new approach for combining a group-theory-based synthesis algorithm with a Reed-Muller-spectra-based synthesis algorithm is described.$$$Experimental results are presented to show that the proposed synthesis techniques allow a reduction in input lines count, gate complexity or quantum cost of reversible circuits for various benchmark functions.",BACKGROUND RESULTS RESULTS RESULTS RESULTS
D01501,"The controversy about the cause(s) of abnormal death of bee colonies in France is investigated through an extensive analysis of the french speaking press.$$$A statistical analysis of textual data is first performed on the lexicon used by journalists to describe the facts and to present associated informations during the period 1998-2010.$$$Three states are identified to explain the phenomenon.$$$The first state asserts a unique cause, the second one focuses on multifactor causes and the third one states the absence of current proof.$$$Assigning each article to one of the three states, we are able to follow the associated opinion dynamics among the journalists over 13 years.$$$Then, we apply the Galam sequential probabilistic model of opinion dynamic to those data.$$$Assuming journalists are either open mind or inflexible about their respective opinions, the results are reproduced precisely provided we account for a series of annual changes in the proportions of respective inflexibles.$$$The results shed a new counter intuitive light on the various pressure supposed to apply on the journalists by either chemical industries or beekeepers and experts or politicians.$$$The obtained dynamics of respective inflexibles shows the possible effect of lobbying, the inertia of the debate and the net advantage gained by the first whistleblowers.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D05979,"We consider the general problem of matching a subspace to a signal in R^N that has been observed indirectly (compressed) through a random projection.$$$We are interested in the case where the collection of K-dimensional subspaces is continuously parameterized, i.e. naturally indexed by an interval from the real line, or more generally a region of R^D.$$$Our main results show that if the dimension of the random projection is on the order of K times a geometrical constant that describes the complexity of the collection, then the match obtained from the compressed observation is nearly as good as one obtained from a full observation of the signal.$$$We give multiple concrete examples of collections of subspaces for which this geometrical constant can be estimated, and discuss the relevance of the results to the general problems of template matching and source localization.",BACKGROUND/OBJECTIVES OBJECTIVES METHODS/RESULTS CONCLUSIONS
D01709,"Bond prices are a reflection of extremely complex market interactions and policies, making prediction of future prices difficult.$$$This task becomes even more challenging due to the dearth of relevant information, and accuracy is not the only consideration--in trading situations, time is of the essence.$$$Thus, machine learning in the context of bond price predictions should be both fast and accurate.$$$In this course project, we use a dataset describing the previous 10 trades of a large number of bonds among other relevant descriptive metrics to predict future bond prices.$$$Each of 762,678 bonds in the dataset is described by a total of 61 attributes, including a ground truth trade price.$$$We evaluate the performance of various supervised learning algorithms for regression followed by ensemble methods, with feature and model selection considerations being treated in detail.$$$We further evaluate all methods on both accuracy and speed.$$$Finally, we propose a novel hybrid time-series aided machine learning method that could be applied to such datasets in future work.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND METHODS METHODS RESULTS
D06938,"Real-world multi-agent planning problems cannot be solved using decision-theoretic planning methods due to the exponential complexity.$$$We approximate firefighting in rescue simulation as a spatially distributed task and model with multi-agent Markov decision process.$$$We use recent approximation methods for spatial task problems to reduce the model complexity.$$$Our approximations are single-agent, static task, shortest path pruning, dynamic planning horizon, and task clustering.$$$We create scenarios from RoboCup Rescue Simulation maps and evaluate our methods on these graph worlds.$$$The results show that our approach is faster and better than comparable methods and has negligible performance loss compared to the optimal policy.$$$We also show that our method has a similar performance as DCOP methods on example RCRS scenarios.",BACKGROUND METHODS METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS
D02732,"With the development of robotics, there are growing needs for real time motion planning.$$$However, due to obstacles in the environment, the planning problem is highly non-convex, which makes it difficult to achieve real time computation using existing non-convex optimization algorithms.$$$This paper introduces the convex feasible set algorithm (CFS) which is a fast algorithm for non-convex optimization problems that have convex costs and non-convex constraints.$$$The idea is to find a convex feasible set for the original problem and iteratively solve a sequence of subproblems using the convex constraints.$$$The feasibility and the convergence of the proposed algorithm are proved in the paper.$$$The application of this method on motion planning for mobile robots is discussed.$$$The simulations demonstrate the effectiveness of the proposed algorithm.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS RESULTS
D04939,"This paper discusses two existing approaches to the correlation analysis between automatic evaluation metrics and human scores in the area of natural language generation.$$$Our experiments show that depending on the usage of a system- or sentence-level correlation analysis, correlation results between automatic scores and human judgments are inconsistent.",BACKGROUND RESULTS
D06933,"The process of designing neural architectures requires expert knowledge and extensive trial and error.$$$While automated architecture search may simplify these requirements, the recurrent neural network (RNN) architectures generated by existing methods are limited in both flexibility and components.$$$We propose a domain-specific language (DSL) for use in automated architecture search which can produce novel RNNs of arbitrary depth and width.$$$The DSL is flexible enough to define standard architectures such as the Gated Recurrent Unit and Long Short Term Memory and allows the introduction of non-standard RNN components such as trigonometric curves and layer normalization.$$$Using two different candidate generation techniques, random search with a ranking function and reinforcement learning, we explore the novel architectures produced by the RNN DSL for language modeling and machine translation domains.$$$The resulting architectures do not follow human intuition yet perform well on their targeted tasks, suggesting the space of usable RNN architectures is far larger than previously assumed.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D04669,"Modeling should play a central role in K-12 STEM education, where it could make classes much more engaging.$$$A model underlies every scientific theory, and models are central to all the STEM disciplines (Science, Technology, Engineering, Math).$$$This paper describes executable concept modeling of STEM concepts using immutable objects and pure functions in Python.$$$I present examples in math, physics, chemistry, and engineering, built using a proof-of-concept tool called PySTEMM .$$$The approach applies to all STEM areas and supports learning with pictures, narrative, animation, and graph plots.$$$Models can extend each other, simplifying getting started.$$$The functional-programming style reduces incidental complexity and code debugging.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS BACKGROUND METHODS
D03421,"In a world of pervasive cameras, public spaces are often captured from multiple perspectives by cameras of different types, both fixed and mobile.$$$An important problem is to organize these heterogeneous collections of videos by finding connections between them, such as identifying correspondences between the people appearing in the videos and the people holding or wearing the cameras.$$$In this paper, we wish to solve two specific problems: (1) given two or more synchronized third-person videos of a scene, produce a pixel-level segmentation of each visible person and identify corresponding people across different views (i.e., determine who in camera A corresponds with whom in camera B), and (2) given one or more synchronized third-person videos as well as a first-person video taken by a mobile or wearable camera, segment and identify the camera wearer in the third-person videos.$$$Unlike previous work which requires ground truth bounding boxes to estimate the correspondences, we perform person segmentation and identification jointly.$$$We find that solving these two problems simultaneously is mutually beneficial, because better fine-grained segmentation allows us to better perform matching across views, and information from multiple views helps us perform more accurate segmentation.$$$We evaluate our approach on two challenging datasets of interacting people captured from multiple wearable cameras, and show that our proposed method performs significantly better than the state-of-the-art on both person segmentation and identification.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D00971,"A recent independent study resulted in a ranking system which ranked Astronomy and Computing (ASCOM) much higher than most of the older journals highlighting the niche prominence of the particular journal.$$$We investigate the remarkable ascendancy in reputation of ASCOM by proposing a novel differential equation based modeling.$$$The Modeling is a consequence of knowledge discovery from big data-centric methods, namely L1-SVD.$$$The inadequacy of the ranking method in explaining the reason behind the growth in reputation of ASCOM is reasonable to understand given that the study was post-facto.$$$Thus, we propose a growth model by accounting for the behavior of parameters that contribute to the growth of a field.$$$It is worthwhile to spend some time in analysing the cause and control variables behind rapid rise in reputation of a journal in a niche area.$$$We intent to probe and bring out parameters responsible for its growing influence.$$$Delay differential equations are used to model the change of influence on a journal's status by exploiting the effects of historical data.",BACKGROUND METHODS METHODS BACKGROUND METHODS OBJECTIVES OBJECTIVES METHODS
D04880,"In this paper, a new offline actor-critic learning algorithm is introduced: Sampled Policy Gradient (SPG).$$$SPG samples in the action space to calculate an approximated policy gradient by using the critic to evaluate the samples.$$$This sampling allows SPG to search the action-Q-value space more globally than deterministic policy gradient (DPG), enabling it to theoretically avoid more local optima.$$$SPG is compared to Q-learning and the actor-critic algorithms CACLA and DPG in a pellet collection task and a self play environment in the game Agar.io.$$$The online game Agar.io has become massively popular on the internet due to intuitive game design and the ability to instantly compete against players around the world.$$$From the point of view of artificial intelligence this game is also very intriguing: The game has a continuous input and action space and allows to have diverse agents with complex strategies compete against each other.$$$The experimental results show that Q-Learning and CACLA outperform a pre-programmed greedy bot in the pellet collection task, but all algorithms fail to outperform this bot in a fighting scenario.$$$The SPG algorithm is analyzed to have great extendability through offline exploration and it matches DPG in performance even in its basic form without extensive sampling.",BACKGROUND/OBJECTIVES METHODS METHODS METHODS BACKGROUND BACKGROUND RESULTS RESULTS
D00063,"As deep neural networks (DNNs) have been integrated into critical systems, several methods to attack these systems have been developed.$$$These adversarial attacks make imperceptible modifications to an image that fool DNN classifiers.$$$We present an adaptive JPEG encoder which defends against many of these attacks.$$$Experimentally, we show that our method produces images with high visual quality while greatly reducing the potency of state-of-the-art attacks.$$$Our algorithm requires only a modest increase in encoding time, produces a compressed image which can be decompressed by an off-the-shelf JPEG decoder, and classified by an unmodified classifier",BACKGROUND BACKGROUND OBJECTIVES RESULTS METHODS
D02264,"Cooperative ITS is enabling vehicles to communicate with the infrastructure to provide improvements in traffic control.$$$A promising approach consists in anticipating the road profile and the upcoming dynamic events like traffic lights.$$$This topic has been addressed in the French public project Co-Drive through functions developed by Valeo named Green Light Optimal Speed Advisor (GLOSA).$$$The system advises the optimal speed to pass the next traffic light without stopping.$$$This paper presents results of its performance in different scenarios through simulations and real driving measurements.$$$A scaling is done in an urban area, with different penetration rates in vehicle and infrastructure equipment for vehicular communication.$$$Our simulation results indicate that GLOSA can reduce CO2 emissions, waiting time and travel time, both in experimental conditions and in real traffic conditions.",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS RESULTS CONCLUSIONS
D05081,"In this paper, we introduce a novel task for machine learning in healthcare, namely personalized modeling of the female hormonal cycle.$$$The motivation for this work is to model the hormonal cycle and predict its phases in time, both for healthy individuals and for those with disorders of the reproductive system.$$$Because there are individual differences in the menstrual cycle, we are particularly interested in personalized models that can account for individual idiosyncracies, towards identifying phenotypes of menstrual cycles.$$$As a first step, we consider the hormonal cycle as a set of observations through time.$$$We use a previously validated mechanistic model to generate realistic hormonal patterns, and experiment with Gaussian process regression to estimate their values over time.$$$Specifically, we are interested in the feasibility of predicting menstrual cycle phases under varying learning conditions: number of cycles used for training, hormonal measurement noise and sampling rates, and informed vs. agnostic sampling of hormonal measurements.$$$Our results indicate that Gaussian processes can help model the female menstrual cycle.$$$We discuss the implications of our experiments in the context of modeling the female menstrual cycle.",RESULTS OBJECTIVES BACKGROUND BACKGROUND METHODS OBJECTIVES RESULTS CONCLUSIONS
D06704,"Convolutional neural networks (CNNs) are widely used in many image recognition tasks due to their extraordinary performance.$$$However, training a good CNN model can still be a challenging task.$$$In a training process, a CNN model typically learns a large number of parameters over time, which usually results in different performance.$$$Often, it is difficult to explore the relationships between the learned parameters and the model performance due to a large number of parameters and different random initializations.$$$In this paper, we present a visual analytics approach to compare two different snapshots of a trained CNN model taken after different numbers of epochs, so as to provide some insight into the design or the training of a better CNN model.$$$Our system compares snapshots by exploring the differences in operation parameters and the corresponding blob data at different levels.$$$A case study has been conducted to demonstrate the effectiveness of our system.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D01007,"We study a class of evolutionary game dynamics defined by balancing a gain determined by the game's payoffs against a cost of motion that captures the difficulty with which the population moves between states.$$$Costs of motion are represented by a Riemannian metric, i.e., a state-dependent inner product on the set of population states.$$$The replicator dynamics and the (Euclidean) projection dynamics are the archetypal examples of the class we study.$$$Like these representative dynamics, all Riemannian game dynamics satisfy certain basic desiderata, including positive correlation and global convergence in potential games.$$$Moreover, when the underlying Riemannian metric satisfies a Hessian integrability condition, the resulting dynamics preserve many further properties of the replicator and projection dynamics.$$$We examine the close connections between Hessian game dynamics and reinforcement learning in normal form games, extending and elucidating a well-known link between the replicator dynamics and exponential reinforcement learning.",OBJECTIVES METHODS BACKGROUND RESULTS RESULTS RESULTS/CONCLUSIONS
D04630,"In this paper we present a working model of an automatic pill reminder and dispenser setup that can alleviate irregularities in taking prescribed dosage of medicines at the right time dictated by the medical practitioner and switch from approaches predominantly dependent on human memory to automation with negligible supervision, thus relieving persons from error-prone tasks of giving wrong medicine at the wrong time in the wrong amount.",METHODS
D03422,"This is the preprint version of our paper on 2015 9th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth2015).$$$An assistive training tool software for rehabilitation of dysphonic patients is evaluated according to the practical clinical feedback from the treatments.$$$One stroke sufferer and one parkinson sufferer have provided earnest suggestions for the improvement of our tool software.$$$The assistive tool employs a serious game as the attractive logic part, and running on the tablet with normal microphone as input device.$$$Seven pitch estimation algorithms have been evaluated and compared with selected patients voice database.$$$A series of benchmarks have been generated during the evaluation process for technology selection.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D03919,"This paper presents a new major release of the program FIESTA (Feynman Integral Evaluation by a Sector decomposiTion Approach).$$$The new release is mainly aimed at optimal performance at large scales when one is increasing the number of sampling points in order to reduce the uncertainty estimates.$$$The release now supports graphical processor units (GPU) for the numerical integration, methods to optimize cluster-usage, as well as other speed, memory, and stability improvements.",CONCLUSIONS OBJECTIVES METHODS
D01791,"In this lecture note, we describe high dynamic range (HDR) imaging systems; such systems are able to represent luminances of much larger brightness and, typically, also a larger range of colors than conventional standard dynamic range (SDR) imaging systems.$$$The larger luminance range greatly improve the overall quality of visual content, making it appears much more realistic and appealing to observers.$$$HDR is one of the key technologies of the future imaging pipeline, which will change the way the digital visual content is represented and manipulated today.",BACKGROUND/OBJECTIVES OTHERS OTHERS
D06472,"The understanding of cascading failures in complex systems has been hindered by the lack of realistic large-scale modeling and analysis that can account for variable system conditions.$$$Here, using the North American power grid, we identify, quantify, and analyze the set of network components that are vulnerable to cascading failures under any out of multiple conditions.$$$We show that the vulnerable set consists of a small but topologically central portion of the network and that large cascades are disproportionately more likely to be triggered by initial failures close to this set.$$$These results elucidate aspects of the origins and causes of cascading failures relevant for grid design and operation, and demonstrate vulnerability analysis methods that are applicable to a wider class of cascade-prone networks.",BACKGROUND/OBJECTIVES METHODS/RESULTS RESULTS CONCLUSIONS
D03535,"Visualisation of data is critical to understanding astronomical phenomena.$$$Today, many instruments produce datasets that are too big to be downloaded to a local computer, yet many of the visualisation tools used by astronomers are deployed only on desktop computers.$$$Cloud computing is increasingly used to provide a computation and simulation platform in astronomy, but it also offers great potential as a visualisation platform.$$$Virtual hosted desktops, with graphics processing unit (GPU) acceleration, allow interactive, graphics-intensive desktop applications to operate co-located with astronomy datasets stored in remote data centres.$$$By combining benchmarking and user experience testing, with a cohort of 20 astronomers, we investigate the viability of replacing physical desktop computers with virtual hosted desktops.$$$In our work, we compare two Apple MacBook computers (one old and one new, representing hardware and opposite ends of the useful lifetime) with two virtual hosted desktops: one commercial (Amazon Web Services) and one in a private research cloud (the Australian Nectar Research Cloud).$$$For two-dimensional image-based tasks and graphics-intensive three-dimensional operations -- typical of astronomy visualisation workflows -- we found that benchmarks do not necessarily provide the best indication of performance.$$$When compared to typical laptop computers, virtual hosted desktops can provide a better user experience, even with lower performing graphics cards.$$$We also found that virtual hosted desktops are equally simple to use, provide greater flexibility in choice of configuration, and may actually be a more cost-effective option for typical usage profiles.",OBJECTIVES/RESULTS/CONCLUSIONS BACKGROUND BACKGROUND METHODS/RESULTS/CONCLUSIONS OBJECTIVES/METHODS BACKGROUND/METHODS METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06601,"Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting to learn not only a classifier/regressor but also the best kernel for the training task, usually from a combination of existing kernel functions.$$$Most MKL methods seek the combined kernel that performs best over every training example, sacrificing performance in some areas to seek a global optimum.$$$Localized kernel learning (LKL) overcomes this limitation by allowing the training algorithm to match a component kernel to the examples that can exploit it best.$$$Several approaches to the localized kernel learning problem have been explored in the last several years.$$$We unify many of these approaches under one simple system and design a new algorithm with improved performance.$$$We also develop enhanced versions of existing algorithms, with an eye on scalability and performance.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND RESULTS RESULTS
D02573,"Autonomous vehicles (AVs) will revolutionarize ground transport and take a substantial role in the future transportation system.$$$Most AVs are likely to be electric vehicles (EVs) and they can participate in the vehicle-to-grid (V2G) system to support various V2G services.$$$Although it is generally infeasible for EVs to dictate their routes, we can design AV travel plans to fulfill certain system-wide objectives.$$$In this paper, we focus on the AVs looking for parking and study how they can be led to appropriate parking facilities to support V2G services.$$$We formulate the Coordinated Parking Problem (CPP), which can be solved by a standard integer linear program solver but requires long computational time.$$$To make it more practical, we develop a distributed algorithm to address CPP based on dual decomposition.$$$We carry out a series of simulations to evaluate the proposed solution methods.$$$Our results show that the distributed algorithm can produce nearly optimal solutions with substantially less computational time.$$$A coarser time scale can improve computational time but degrade the solution quality resulting in possible infeasible solution.$$$Even with communication loss, the distributed algorithm can still perform well and converge with only little degradation in speed.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS RESULTS
D05156,"A recent theoretical analysis shows the equivalence between non-negative matrix factorization (NMF) and spectral clustering based approach to subspace clustering.$$$As NMF and many of its variants are essentially linear, we introduce a nonlinear NMF with explicit orthogonality and derive general kernel-based orthogonal multiplicative update rules to solve the subspace clustering problem.$$$In nonlinear orthogonal NMF framework, we propose two subspace clustering algorithms, named kernel-based non-negative subspace clustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral normalized cut and ratio cut clustering.$$$We further extend the nonlinear orthogonal NMF framework and introduce a graph regularization to obtain a factorization that respects a local geometric structure of the data after the nonlinear mapping.$$$The proposed NMF-based approach to subspace clustering takes into account the nonlinear nature of the manifold, as well as its intrinsic local geometry, which considerably improves the clustering performance when compared to the several recently proposed state-of-the-art methods.",BACKGROUND OBJECTIVES/RESULTS OBJECTIVES/METHODS/RESULTS OBJECTIVES/METHODS/RESULTS CONCLUSIONS
D02596,"Network visualization allows a quick glance at how nodes (or actors) are connected by edges (or ties).$$$A conventional network diagram of ""contact tree"" maps out a root and branches that represent the structure of nodes and edges, often without further specifying leaves or fruits that would have grown from small branches.$$$By furnishing such a network structure with leaves and fruits, we reveal details about ""contacts"" in our ContactTrees that underline ties and relationships.$$$Our elegant design employs a bottom-up approach that resembles a recent attempt to understand subjective well-being by means of a series of emotions.$$$Such a bottom-up approach to social-network studies decomposes each tie into a series of interactions or contacts, which help deepen our understanding of the complexity embedded in a network structure.$$$Unlike previous network visualizations, ContactTrees can highlight how relationships form and change based upon interactions among actors, and how relationships and networks vary by contact attributes.$$$Based on a botanical tree metaphor, the design is easy to construct and the resulting tree-like visualization can display many properties at both tie and contact levels, a key ingredient missing from conventional techniques of network visualization.$$$We first demonstrate ContactTrees using a dataset consisting of three waves of 3-month contact diaries over the 2004-2012 period, then compare ContactTrees with alternative tools and discuss how this tool can be applied to other types of datasets.",BACKGROUND BACKGROUND METHODS METHODS METHODS OBJECTIVES OBJECTIVES RESULTS
D00939,"How to tell if a review is real or fake?$$$What does the underworld of fraudulent reviewing look like?$$$Detecting suspicious reviews has become a major issue for many online services.$$$We propose the use of a clique-finding approach to discover well-organized suspicious reviewers.$$$From a Yelp dataset with over one million reviews, we construct multiple Reviewer Similarity graphs to link users that have unusually similar behavior: two reviewers are connected in the graph if they have reviewed the same set of venues within a few days.$$$From these graphs, our algorithms extracted many large cliques and quasi-cliques, the largest one containing a striking 11 users who coordinated their review activities in identical ways.$$$Among the detected cliques, a large portion contain Yelp Scouts who are paid by Yelp to review venues in new areas.$$$Our work sheds light on their little-known operation.",BACKGROUND/OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS RESULTS RESULTS/CONCLUSIONS CONCLUSIONS
D05870,"In this paper we present a new efficient algorithm for factoring the RSA and the Rabin moduli in the particular case when the difference between their two prime factors is bounded.$$$As an extension, we also give some theoretical results on factoring integers.",OBJECTIVES/METHODS METHODS
D01122,"The economic model of the Internet of Things (IoT) consists of end users, advertisers and three different kinds of providers--IoT service provider (IoTSP), Wireless service provider (WSP) and cloud service provider (CSP).$$$We investigate three different kinds of interactions among the providers.$$$First, we consider that the IoTSP prices a bundled service to the end-users, and the WSP and CSP pay the IoTSP (push model).$$$Next, we consider the model where the end-users independently pay the each provider (pull model).$$$Finally, we consider a hybrid model of the above two where the IoTSP and WSP quote their prices to the end-users, but the CSP quotes its price to the IoTSP.$$$We characterize and quantify the impact of the advertisement revenue on the equilibrium pricing strategy and payoff of providers, and corresponding demands of end users in each of the above interaction models.$$$Our analysis reveals that the demand of end-users, and the payoffs of the providers are non decreasing functions of the advertisement revenue.$$$For sufficiently high advertisement revenue, the IoTSP will offer its service free of cost in each interaction model.$$$However, the payoffs of the providers, and the demand of end-users vary across different interaction models.$$$Our analysis shows that the demand of end-users, and the payoff of the WSP are the highest in the pull (push, resp.) model in the low (high, resp.) advertisement revenue regime.$$$The payoff of the IoTSP is always higher in the pull model irrespective of the advertisement revenue.$$$The payoff of the CSP is the highest in the hybrid model in the low advertisement revenue regime.$$$However, in the high advertisement revenue regime the payoff of the CSP in the hybrid model or in the push model can be higher depending on the equilibrium chosen in the push model.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D03932,"In this paper, we present a new approach of distributed clustering for spatial datasets, based on an innovative and efficient aggregation technique.$$$This distributed approach consists of two phases: 1) local clustering phase, where each node performs a clustering on its local data, 2) aggregation phase, where the local clusters are aggregated to produce global clusters.$$$This approach is characterised by the fact that the local clusters are represented in a simple and efficient way.$$$And The aggregation phase is designed in such a way that the final clusters are compact and accurate while the overall process is efficient in both response time and memory allocation.$$$We evaluated the approach with different datasets and compared it to well-known clustering techniques.$$$The experimental results show that our approach is very promising and outperforms all those algorithms",OBJECTIVES METHODS METHODS METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D01837,"Autonomous planetary vehicles, also known as rovers, are small autonomous vehicles equipped with a variety of sensors used to perform exploration and experiments on a planet's surface.$$$Rovers work in a partially unknown environment, with narrow energy/time/movement constraints and, typically, small computational resources that limit the complexity of on-line planning and scheduling, thus they represent a great challenge in the field of autonomous vehicles.$$$Indeed, formal models for such vehicles usually involve hybrid systems with nonlinear dynamics, which are difficult to handle by most of the current planning algorithms and tools.$$$Therefore, when offline planning of the vehicle activities is required, for example for rovers that operate without a continuous Earth supervision, such planning is often performed on simplified models that are not completely realistic.$$$In this paper we show how the UPMurphi model checking based planning tool can be used to generate resource-optimal plans to control the engine of an autonomous planetary vehicle, working directly on its hybrid model and taking into account several safety constraints, thus achieving very accurate results.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS/CONCLUSIONS
D06017,"Journal of the History of Biology provides a fifty-year long record for examining the evolution of the history of biology as a scholarly discipline.$$$In this paper, we present a new dataset and preliminary quantitative analysis of the thematic content of JHB from the perspectives of geography, organisms, and thematic fields.$$$The geographic diversity of authors whose work appears in JHB has increased steadily since 1968, but the geographic coverage of the content of JHB articles remains strongly lopsided toward the United States, United Kingdom, and western Europe and has diversified much less dramatically over time.$$$The taxonomic diversity of organisms discussed in JHB increased steadily between 1968 and the late 1990s but declined in later years, mirroring broader patterns of diversification previously reported in the biomedical research literature.$$$Finally, we used a combination of topic modeling and nonlinear dimensionality reduction techniques to develop a model of multi-article fields within JHB.$$$We found evidence for directional changes in the representation of fields on multiple scales.$$$The diversity of JHB with regard to the representation of thematic fields has increased overall, with most of that diversification occurring in recent years.$$$Drawing on the dataset generated in the course of this analysis, as well as web services in the emerging digital history and philosophy of science ecosystem, we have developed an interactive web platform for exploring the content of JHB, and we provide a brief overview of the platform in this article.$$$As a whole, the data and analyses presented here provide a starting-place for further critical reflection on the evolution of the history of biology over the past half-century.",BACKGROUND OBJECTIVES RESULTS RESULTS METHODS RESULTS RESULTS OTHERS OTHERS
D03260,"We give an algorithm to compute the periods of smooth projective hypersurfaces of any dimension.$$$This is an improvement over existing algorithms which could only compute the periods of plane curves.$$$Our algorithm reduces the evaluation of period integrals to an initial value problem for ordinary differential equations of Picard-Fuchs type.$$$In this way, the periods can be computed to extreme-precision in order to study their arithmetic properties.$$$The initial conditions are obtained by an exact determination of the cohomology pairing on Fermat hypersurfaces with respect to a natural basis.",RESULTS BACKGROUND METHODS CONCLUSIONS METHODS
D02180,"Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train.$$$However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation.$$$This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption.$$$In this paper, we present Group Normalization (GN) as a simple alternative to BN.$$$GN divides the channels into groups and computes within each group the mean and variance for normalization.$$$GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes.$$$On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants.$$$Moreover, GN can be naturally transferred from pre-training to fine-tuning.$$$GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks.$$$GN can be easily implemented by a few lines of code in modern libraries.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS RESULTS RESULTS
D00552,"Myerson derived a simple and elegant solution to the single-parameter revenue-maximization problem in his seminal work on optimal auction design assuming the usual model of quasi-linear utilities.$$$In this paper, we consider a slight generalization of this usual model---from linear to convex ""perceived"" payments.$$$This more general problem does not appear to admit a solution as simple and elegant as Myerson's.$$$While some of Myerson's results extend to our setting, like his payment formula (suitably adjusted), others do not.$$$For example, we observe that the solutions to the Bayesian and the robust (i.e., non-Bayesian) optimal auction design problems in the convex perceived payment setting do not coincide like they do in the case of linear payments.$$$We therefore study the two problems in turn.$$$We derive an upper and a heuristic lower bound on expected revenue in our setting.$$$These bounds are easily computed pointwise, and yield monotonic allocation rules, so can be supported by Myerson payments (suitably adjusted).$$$In this way, our bounds yield heuristics that approximate the optimal robust auction, assuming convex perceived payments.$$$We close with experiments, the final set of which massages the output of one of the closed-form heuristics for the robust problem into an extremely fast, near-optimal heuristic solution to the Bayesian optimal auction design problem.",BACKGROUND OBJECTIVES OBJECTIVES RESULTS RESULTS METHODS RESULTS RESULTS CONCLUSIONS RESULTS
D00892,"We present an iterative overlap estimation technique to augment existing point cloud registration algorithms that can achieve high performance in difficult real-world situations where large pose displacement and non-overlapping geometry would otherwise cause traditional methods to fail.$$$Our approach estimates overlapping regions through an iterative Expectation Maximization procedure that encodes the sensor field-of-view into the registration process.$$$The proposed technique, Expected Overlap Estimation (EOE), is derived from the observation that differences in field-of-view violate the iid assumption implicitly held by all maximum likelihood based registration techniques.$$$We demonstrate how our approach can augment many popular registration methods with minimal computational overhead.$$$Through experimentation on both synthetic and real-world datasets, we find that adding an explicit overlap estimation step can aid robust outlier handling and increase the accuracy of both ICP-based and GMM-based registration methods, especially in large unstructured domains and where the amount of overlap between point clouds is very small.",CONCLUSIONS METHODS OBJECTIVES RESULTS RESULTS
D06618,"Replacing a portion of current light duty vehicles (LDV) with plug-in hybrid electric vehicles (PHEVs) offers the possibility to reduce the dependence on petroleum fuels together with environmental and economic benefits.$$$The charging activity of PHEVs will certainly introduce new load to the power grid.$$$In the framework of the development of a smarter grid, the primary focus of the present study is to propose a model for the electrical daily demand in presence of PHEVs charging.$$$Expected PHEV demand is modeled by the PHEV charging time and the starting time of charge according to real world data.$$$A normal distribution for starting time of charge is assumed.$$$Several distributions for charging time are considered: uniform distribution, Gaussian with positive support, Rician distribution and a non-uniform distribution coming from driving patterns in real-world data.$$$We generate daily demand profiles by using real-world residential profiles throughout 2014 in the presence of different expected PHEV demand models.$$$Support vector machines (SVMs), a set of supervised machine learning models, are employed in order to find the best model to fit the data.$$$SVMs with radial basis function (RBF) and polynomial kernels were tested.$$$Model performances are evaluated by means of mean squared error (MSE) and mean absolute percentage error (MAPE).$$$Best results are obtained with RBF kernel: maximum (worst) values for MSE and MAPE were about 2.89 10-8 and 0.023, respectively.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS METHODS RESULTS
D04982,"Image quality assessment (IQA) is traditionally classified into full-reference (FR) IQA and no-reference (NR) IQA according to whether the original image is required.$$$Although NR-IQA is widely used in practical applications, room for improvement still remains because of the lack of the reference image.$$$Inspired by the fact that in many applications, such as parameter selection, a series of distorted images are available, the authors propose a novel comparison-based image quality assessment (C-IQA) method.$$$The new comparison-based framework parallels FR-IQA by requiring two input images, and resembles NR-IQA by not using the original image.$$$As a result, the new comparison-based approach has more application scenarios than FR-IQA does, and takes greater advantage of the accessible information than the traditional single-input NR-IQA does.$$$Further, C-IQA is compared with other state-of-the-art NR-IQA methods on two widely used IQA databases.$$$Experimental results show that C-IQA outperforms the other NR-IQA methods for parameter selection, and the parameter trimming framework combined with C-IQA saves the computation of iterative image reconstruction up to 80%.",BACKGROUND BACKGROUND OBJECTIVES METHODS CONCLUSIONS RESULTS RESULTS
D04529,"This study concerned the active use of Wikipedia as a teaching tool in the classroom in higher education, trying to identify different usage profiles and their characterization.$$$A questionnaire survey was administrated to all full-time and part-time teachers at the Universitat Oberta de Catalunya and the Universitat Pompeu Fabra, both in Barcelona, Spain.$$$The questionnaire was designed using the Technology Acceptance Model as a reference, including items about teachers web 2.0 profile, Wikipedia usage, expertise, perceived usefulness, easiness of use, visibility and quality, as well as Wikipedia status among colleagues and incentives to use it more actively.$$$Clustering and statistical analysis were carried out using the k-medoids algorithm and differences between clusters were assessed by means of contingency tables and generalized linear models (logit).$$$The respondents were classified in four clusters, from less to more likely to adopt and use Wikipedia in the classroom, namely averse (25.4%), reluctant (17.9%), open (29.5%) and proactive (27.2%).$$$Proactive faculty are mostly men teaching part-time in STEM fields, mainly engineering, while averse faculty are mostly women teaching full-time in non-STEM fields.$$$Nevertheless, questionnaire items related to visibility, quality, image, usefulness and expertise determine the main differences between clusters, rather than age, gender or domain.$$$Clusters involving a positive view of Wikipedia and at least some frequency of use clearly outnumber those with a strictly negative stance.$$$This goes against the common view that faculty members are mostly sceptical about Wikipedia.$$$Environmental factors such as academic culture and colleagues opinion are more important than faculty personal characteristics, especially with respect to what they think about Wikipedia quality.",OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D02331,"In order to achieve high efficiency of classification in intrusion detection, a compressed model is proposed in this paper which combines horizontal compression with vertical compression.$$$OneR is utilized as horizontal com-pression for attribute reduction, and affinity propagation is employed as vertical compression to select small representative exemplars from large training data.$$$As to be able to computationally compress the larger volume of training data with scalability, MapReduce based parallelization approach is then implemented and evaluated for each step of the model compression process abovementioned, on which common but efficient classification methods can be directly used.$$$Experimental application study on two publicly available datasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the classification using the compressed model proposed can effectively speed up the detection procedure at up to 184 times, most importantly at the cost of a minimal accuracy difference with less than 1% on average.",BACKGROUND/OBJECTIVES/METHODS METHODS OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D03285,"Face anti-spoofing is the crucial step to prevent face recognition systems from a security breach.$$$Previous deep learning approaches formulate face anti-spoofing as a binary classification problem.$$$Many of them struggle to grasp adequate spoofing cues and generalize poorly.$$$In this paper, we argue the importance of auxiliary supervision to guide the learning toward discriminative and generalizable cues.$$$A CNN-RNN model is learned to estimate the face depth with pixel-wise supervision, and to estimate rPPG signals with sequence-wise supervision.$$$Then we fuse the estimated depth and rPPG to distinguish live vs. spoof faces.$$$In addition, we introduce a new face anti-spoofing database that covers a large range of illumination, subject, and pose variations.$$$Experimental results show that our model achieves the state-of-the-art performance on both intra-database and cross-database testing.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D02194,"One of the key research interests in the area of Constraint Satisfaction Problem (CSP) is to identify tractable classes of constraints and develop efficient solutions for them.$$$In this paper, we introduce generalized staircase (GS) constraints which is an important generalization of one such tractable class found in the literature, namely, staircase constraints.$$$GS constraints are of two kinds, down staircase (DS) and up staircase (US).$$$We first examine several properties of GS constraints, and then show that arc consistency is sufficient to determine a solution to a CSP over DS constraints.$$$Further, we propose an optimal O(cd) time and space algorithm to compute arc consistency for GS constraints where c is the number of constraints and d is the size of the largest domain.$$$Next, observing that arc consistency is not necessary for solving a DSCSP, we propose a more efficient algorithm for solving it.$$$With regard to US constraints, arc consistency is not known to be sufficient to determine a solution, and therefore, methods such as path consistency or variable elimination are required.$$$Since arc consistency acts as a subroutine for these existing methods, replacing it by our optimal O(cd) arc consistency algorithm produces a more efficient method for solving a USCSP.",BACKGROUND BACKGROUND/METHODS BACKGROUND OBJECTIVES/METHODS METHODS METHODS/RESULTS BACKGROUND METHODS/RESULTS
D01642,"Deep learning stands at the forefront in many computer vision tasks.$$$However, deep neural networks are usually data-hungry and require a huge amount of well-annotated training samples.$$$Collecting sufficient annotated data is very expensive in many applications, especially for pixel-level prediction tasks such as semantic segmentation.$$$To solve this fundamental issue, we consider a new challenging vision task, Internetly supervised semantic segmentation, which only uses Internet data with noisy image-level supervision of corresponding query keywords for segmentation model training.$$$We address this task by proposing the following solution.$$$A class-specific attention model unifying multiscale forward and backward convolutional features is proposed to provide initial segmentation ""ground truth"".$$$The model trained with such noisy annotations is then improved by an online fine-tuning procedure.$$$It achieves state-of-the-art performance under the weakly-supervised setting on PASCAL VOC2012 dataset.$$$The proposed framework also paves a new way towards learning from the Internet without human interaction and could serve as a strong baseline therein.$$$Code and data will be released upon the paper acceptance.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS OTHERS
D06908,"Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution.$$$There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control.$$$In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement.$$$We aim to separate the internal representation into three parts.$$$The shared part contains information for both domains.$$$The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain.$$$We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component.$$$Our model offers multiple advantages.$$$We can output diverse samples covering multiple modes of the distributions of both domains, perform domain-specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images.$$$We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS RESULTS RESULTS RESULTS
D01309,"The sharing of network traces is an important prerequisite for the development and evaluation of efficient anomaly detection mechanisms.$$$Unfortunately, privacy concerns and data protection laws prevent network operators from sharing these data.$$$Anonymization is a promising solution in this context; however, it is unclear if the sanitization of data preserves the traffic characteristics or introduces artifacts that may falsify traffic analysis results.$$$In this paper, we examine the utility of anonymized flow traces for anomaly detection.$$$We quantitatively evaluate the impact of IP address anonymization, namely variations of permutation and truncation, on the detectability of large-scale anomalies.$$$Specifically, we analyze three weeks of un-sampled and non-anonymized network traces from a medium-sized backbone network.$$$We find that all anonymization techniques, except prefix-preserving permutation, degrade the utility of data for anomaly detection.$$$We show that the degree of degradation depends to a large extent on the nature and mix of anomalies present in a trace.$$$Moreover, we present a case study that illustrates how traffic characteristics of individual hosts are distorted by anonymization.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS
D03933,"General treebank analyses are graph structured, but parsers are typically restricted to tree structures for efficiency and modeling reasons.$$$We propose a new representation and algorithm for a class of graph structures that is flexible enough to cover almost all treebank structures, while still admitting efficient learning and inference.$$$In particular, we consider directed, acyclic, one-endpoint-crossing graph structures, which cover most long-distance dislocation, shared argumentation, and similar tree-violating linguistic phenomena.$$$We describe how to convert phrase structure parses, including traces, to our new representation in a reversible manner.$$$Our dynamic program uniquely decomposes structures, is sound and complete, and covers 97.3% of the Penn English Treebank.$$$We also implement a proof-of-concept parser that recovers a range of null elements and trace types.",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES RESULTS RESULTS
D06549,"We present a novel unsupervised approach for multilingual sentiment analysis driven by compositional syntax-based rules.$$$On the one hand, we exploit some of the main advantages of unsupervised algorithms: (1) the interpretability of their output, in contrast with most supervised models, which behave as a black box and (2) their robustness across different corpora and domains.$$$On the other hand, by introducing the concept of compositional operations and exploiting syntactic information in the form of universal dependencies, we tackle one of their main drawbacks: their rigidity on data that are structured differently depending on the language concerned.$$$Experiments show an improvement both over existing unsupervised methods, and over state-of-the-art supervised models when evaluating outside their corpus of origin.$$$Experiments also show how the same compositional operations can be shared across languages.$$$The system is available at http://www.grupolys.org/software/UUUSA/",OBJECTIVES BACKGROUND METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS OTHERS
D04217,"Defects4J is a large, peer-reviewed, structured dataset of real-world Java bugs.$$$Each bug in Defects4J comes with a test suite and at least one failing test case that triggers the bug.$$$In this paper, we report on an experiment to explore the effectiveness of automatic test-suite based repair on Defects4J.$$$The result of our experiment shows that the considered state-of-the-art repair methods can generate patches for 47 out of 224 bugs.$$$However, those patches are only test-suite adequate, which means that they pass the test suite and may potentially be incorrect beyond the test-suite satisfaction correctness criterion.$$$We have manually analyzed 84 different patches to assess their real correctness.$$$In total, 9 real Java bugs can be correctly repaired with test-suite based repair.$$$This analysis shows that test-suite based repair suffers from under-specified bugs, for which trivial or incorrect patches still pass the test suite.$$$With respect to practical applicability, it takes on average 14.8 minutes to find a patch.$$$The experiment was done on a scientific grid, totaling 17.6 days of computation time.$$$All the repair systems and experimental results are publicly available on Github in order to facilitate future research on automatic repair.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS METHODS RESULTS CONCLUSIONS OTHERS OTHERS OTHERS
D01112,"Given the great interest in creating keyframe summaries from video, it is surprising how little has been done to formalise their evaluation and comparison.$$$User studies are often carried out to demonstrate that a proposed method generates a more appealing summary than one or two rival methods.$$$But larger comparison studies cannot feasibly use such user surveys.$$$Here we propose a discrimination capacity measure as a formal way to quantify the improvement over the uniform baseline, assuming that one or more ground truth summaries are available.$$$Using the VSUMM video collection, we examine 10 video feature types, including CNN and SURF, and 6 methods for matching frames from two summaries.$$$Our results indicate that a simple frame representation through hue histograms suffices for the purposes of comparing keyframe summaries.$$$We subsequently propose a formal protocol for comparing summaries when ground truth is available.",BACKGROUND BACKGROUND BACKGROUND METHODS METHODS RESULTS/CONCLUSIONS METHODS
D05314,"We give a new algorithm to construct optimal alphabetic ternary trees, where every internal node has at most three children.$$$This algorithm generalizes the classic Hu-Tucker algorithm, though the overall computational complexity has yet to be determined.",OBJECTIVES BACKGROUND
D03274,"Automatic speech recognition can potentially benefit from the lip motion patterns, complementing acoustic speech to improve the overall recognition performance, particularly in noise.$$$In this paper we propose an audio-visual fusion strategy that goes beyond simple feature concatenation and learns to automatically align the two modalities, leading to enhanced representations which increase the recognition accuracy in both clean and noisy conditions.$$$We test our strategy on the TCD-TIMIT and LRS2 datasets, designed for large vocabulary continuous speech recognition, applying three types of noise at different power ratios.$$$We also exploit state of the art Sequence-to-Sequence architectures, showing that our method can be easily integrated.$$$Results show relative improvements from 7% up to 30% on TCD-TIMIT over the acoustic modality alone, depending on the acoustic noise level.$$$We anticipate that the fusion strategy can easily generalise to many other multimodal tasks which involve correlated modalities.$$$Code available online on GitHub: https://github.com/georgesterpu/Sigmedia-AVSR",BACKGROUND OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS OTHERS
D02511,"Automatically determining the optimal size of a neural network for a given task without prior information currently requires an expensive global search and training many networks from scratch.$$$In this paper, we address the problem of automatically finding a good network size during a single training cycle.$$$We introduce *nonparametric neural networks*, a non-probabilistic framework for conducting optimization over all possible network sizes and prove its soundness when network growth is limited via an L_p penalty.$$$We train networks under this framework by continuously adding new units while eliminating redundant units via an L_2 penalty.$$$We employ a novel optimization algorithm, which we term *adaptive radial-angular gradient descent* or *AdaRad*, and obtain promising results.",BACKGROUND OBJECTIVES METHODS METHODS METHODS
D01841,"The numerical size of academic publications that are being published in recent years had grown rapidly.$$$Accessing and searching massive academic publications that are distributed over several locations need large amount of computing resources to increase the system performance.$$$Therefore, many grid-based search techniques were proposed to provide flexible methods for searching the distributed extensive data.$$$This paper proposes search technique that is capable of searching the extensive publications by utilizing grid computing technology.$$$The search technique is implemented as interconnected grid services to offer a mechanism to access different data locations.$$$The experimental result shows that the grid-based search technique has enhanced the performance of the search.",BACKGROUND BACKGROUND OTHERS OBJECTIVES METHODS RESULTS
D04034,"This paper explores the problem of page migration in ring networks.$$$A ring network is a connected graph, in which each node is connected with exactly two other nodes.$$$In this problem, one of the nodes in a given network holds a page of size D. This node is called the server and the page is a non-duplicable data in the network.$$$Requests are issued by nodes to access the page one after another.$$$Every time a new request is issued, the server must serve the request and may migrate to another node before the next request arrives.$$$A service costs the distance between the server and the requesting node, and the migration costs the distance of the migration multiplied by D. The problem is to minimize the total costs of services and migrations.$$$We study this problem in uniform model, for which the page has a unit size, i.e.$$$D=1.$$$A 3.326-competitive algorithm improving the current best upper bound is designed.$$$We show that this ratio is tight for our algorithm.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS CONCLUSIONS
D01082,"This paper presents a new connection between the generalized Marcum-Q function and the confluent hypergeometric function of two variables, phi3.$$$This result is then applied to the closed-form characterization of the bivariate Nakagami-m distribution and of the distribution of the minimum eigenvalue of correlated non-central Wishart matrices, both important in communication theory.$$$New expressions for the corresponding cumulative distributions are obtained and a number of communication-theoretic problems involving them are pointed out.",RESULTS RESULTS RESULTS
D00929,"We use the method of Maximum (relative) Entropy to process information in the form of observed data and moment constraints.$$$The generic ""canonical"" form of the posterior distribution for the problem of simultaneous updating with data and moments is obtained.$$$We discuss the general problem of non-commuting constraints, when they should be processed sequentially and when simultaneously.$$$As an illustration, the multinomial example of die tosses is solved in detail for two superficially similar but actually very different problems.",OBJECTIVES CONCLUSIONS OBJECTIVES RESULTS
D06499,"The Moral Foundations Dictionary (MFD) is a useful tool for applying the conceptual framework developed in Moral Foundations Theory and quantifying the moral meanings implicated in the linguistic information people convey.$$$However, the applicability of the MFD is limited because it is available only in English.$$$Translated versions of the MFD are therefore needed to study morality across various cultures, including non-Western cultures.$$$The contribution of this paper is two-fold.$$$We developed the first Japanese version of the MFD (referred to as the J-MFD) by introducing a semi-automated method---this serves as a reference when translating the MFD into other languages.$$$We next tested the validity of the J-MFD by analyzing open-ended written texts about the situations that Japanese participants thought followed and violated the five moral foundations.$$$We found that the J-MFD correctly categorized the Japanese participants' descriptions into the corresponding moral foundations, and that the Moral Foundations Questionnaire (MFQ) scores were correlated with the frequency of situations, of total words, and of J-MFD words in the participants' descriptions for the Harm and Fairness foundations.$$$The J-MFD can be used to study morality unique to the Japanese and cultural differences in moral behavior.",BACKGROUND BACKGROUND OBJECTIVES OTHERS METHODS METHODS RESULTS CONCLUSIONS
D05261,"Over the past decade, the study of extrasolar planets has evolved rapidly from plain detection and identification to comprehensive categorization and characterization of exoplanet systems and their atmospheres.$$$Atmospheric retrieval, the inverse modeling technique used to determine an exoplanetary atmosphere's temperature structure and composition from an observed spectrum, is both time-consuming and compute-intensive, requiring complex algorithms that compare thousands to millions of atmospheric models to the observational data to find the most probable values and associated uncertainties for each model parameter.$$$For rocky, terrestrial planets, the retrieved atmospheric composition can give insight into the surface fluxes of gaseous species necessary to maintain the stability of that atmosphere, which may in turn provide insight into the geological and/or biological processes active on the planet.$$$These atmospheres contain many molecules, some of them biosignatures, spectral fingerprints indicative of biological activity, which will become observable with the next generation of telescopes.$$$Runtimes of traditional retrieval models scale with the number of model parameters, so as more molecular species are considered, runtimes can become prohibitively long.$$$Recent advances in machine learning (ML) and computer vision offer new ways to reduce the time to perform a retrieval by orders of magnitude, given a sufficient data set to train with.$$$Here we present an ML-based retrieval framework called Intelligent exoplaNet Atmospheric RetrievAl (INARA) that consists of a Bayesian deep learning model for retrieval and a data set of 3,000,000 synthetic rocky exoplanetary spectra generated using the NASA Planetary Spectrum Generator.$$$Our work represents the first ML retrieval model for rocky, terrestrial exoplanets and the first synthetic data set of terrestrial spectra generated at this scale.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS/RESULTS CONCLUSIONS
D05644,"Scientific Computing typically requires large computational needs which have been addressed with High Performance Distributed Computing.$$$It is essential to efficiently deploy a number of complex scientific applications, which have different characteristics, and so require distinct computational resources too.$$$However, in many research laboratories, this high performance architecture is not dedicated.$$$So, the architecture must be shared to execute a set of scientific applications, with so many different execution times and relative importance to research.$$$Also, the high performance architectures have different characteristics and costs.$$$When a new infrastructure has to be acquired to meet the needs of this scenario, the decision-making is hard and complex.$$$In this work, we present a Gain Function as a model of an utility function, with which it is possible a decision-making with confidence.$$$With the function is possible to evaluate the best architectural option taking into account aspects of applications and architectures, including the executions time, cost of architecture, the relative importance of each application and also the relative importance of performance and cost on the final evaluation.$$$This paper presents the Gain Function, examples, and a real case showing their applicabilities.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS
D01099,"We initiate the study of quantum races, games where two or more quantum computers compete to solve a computational problem.$$$While the problem of dueling algorithms has been studied for classical deterministic algorithms, the quantum case presents additional sources of uncertainty for the players.$$$The foremost among these is that players do not know if they have solved the problem until they measure their quantum state.$$$This question of `when to measure?' presents a very interesting strategic problem.$$$We develop a game-theoretic model of a multiplayer quantum race, and find an approximate Nash equilibrium where all players play the same strategy.$$$In the two-party case, we further show that this strategy is nearly optimal in terms of payoff among all symmetric Nash equilibria.$$$A key role in our analysis of quantum races is played by a more tractable version of the game where there is no payout on a tie; for such races we completely characterize the Nash equilibria in the two-party case.$$$One application of our results is to the stability of the Bitcoin protocol when mining is done by quantum computers.$$$Bitcoin mining is a race to solve a computational search problem, with the winner gaining the right to create a new block.$$$Our results inform the strategies that eventual quantum miners should use, and also indicate that the collision probability---the probability that two miners find a new block at the same time---would not be too high in the case of quantum miners.$$$Such collisions are undesirable as they lead to forking of the Bitcoin blockchain.",OTHERS OTHERS OTHERS OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS BACKGROUND CONCLUSIONS OTHERS
D02912,"Understanding the loss surface of neural networks is essential for the design of models with predictable performance and their success in applications.$$$Experimental results suggest that sufficiently deep and wide neural networks are not negatively impacted by suboptimal local minima.$$$Despite recent progress, the reason for this outcome is not fully understood.$$$Could deep networks have very few, if at all, suboptimal local optima? or could all of them be equally good?$$$We provide a construction to show that suboptimal local minima (i.e. non-global ones), even though degenerate, exist for fully connected neural networks with sigmoid activation functions.$$$The local minima obtained by our proposed construction belong to a connected set of local solutions that can be escaped from via a non-increasing path on the loss curve.$$$For extremely wide neural networks with two hidden layers, we prove that every suboptimal local minimum belongs to such a connected set.$$$This provides a partial explanation for the successful application of deep neural networks.$$$In addition, we also characterize under what conditions the same construction leads to saddle points instead of local minima for deep neural networks.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS RESULTS RESULTS CONCLUSIONS RESULTS
D06988,"The effects of molecularly targeted drug perturbations on cellular activities and fates are difficult to predict using intuition alone because of the complex behaviors of cellular regulatory networks.$$$An approach to overcoming this problem is to develop mathematical models for predicting drug effects.$$$Such an approach beckons for co-development of computational methods for extracting insights useful for guiding therapy selection and optimizing drug scheduling.$$$Here, we present and evaluate a generalizable strategy for identifying drug dosing schedules that minimize the amount of drug needed to achieve sustained suppression or elevation of an important cellular activity/process, the recycling of cytoplasmic contents through (macro)autophagy.$$$Therapeutic targeting of autophagy is currently being evaluated in diverse clinical trials but without the benefit of a control engineering perspective.$$$Using a nonlinear ordinary differential equation (ODE) model that accounts for activating and inhibiting influences among protein and lipid kinases that regulate autophagy (MTORC1, ULK1, AMPK and VPS34) and methods guaranteed to find locally optimal control strategies, we find optimal drug dosing schedules (open-loop controllers) for each of six classes of drugs and drug pairs.$$$Our approach is generalizable to designing monotherapy and multi therapy drug schedules that affect different cell signaling networks of interest.",BACKGROUND BACKGROUND/RESULTS/CONCLUSIONS BACKGROUND/OBJECTIVES/METHODS/RESULTS/CONCLUSIONS BACKGROUND/OBJECTIVES/METHODS/RESULTS/CONCLUSIONS BACKGROUND METHODS/RESULTS METHODS/CONCLUSIONS
D02964,"A subspace projection to improve channel estimation in massive multi-antenna systems is proposed and analyzed.$$$Together with power-controlled hand-off, it can mitigate the pilot contamination problem without the need for coordination among cells.$$$The proposed method is blind in the sense that it does not require pilot data to find the appropriate subspace.$$$It is based on the theory of large random matrices that predicts that the eigenvalue spectra of large sample covariance matrices can asymptotically decompose into disjoint bulks as the matrix size grows large.$$$Random matrix and free probability theory are utilized to predict under which system parameters such a bulk decomposition takes place.$$$Simulation results are provided to confirm that the proposed method outperforms conventional linear channel estimation if bulk separation occurs.",OBJECTIVES RESULTS METHODS METHODS METHODS RESULTS
D04101,"This paper proposes an evolutionary Particle Filter with a memory guided proposal step size update and an improved, fully-connected Quantum-behaved Particle Swarm Optimization (QPSO) resampling scheme for visual tracking applications.$$$The proposal update step uses importance weights proportional to velocities encountered in recent memory to limit the swarm movement within probable regions of interest.$$$The QPSO resampling scheme uses a fitness weighted mean best update to bias the swarm towards the fittest section of particles while also employing a simulated annealing operator to avoid subpar fine tune during latter course of iterations.$$$By moving particles closer to high likelihood landscapes of the posterior distribution using such constructs, the sample impoverishment problem that plagues the Particle Filter is mitigated to a great extent.$$$Experimental results using benchmark sequences imply that the proposed method outperforms competitive candidate trackers such as the Particle Filter and the traditional Particle Swarm Optimization based Particle Filter on a suite of tracker performance indices.",BACKGROUND/OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D06684,"Microbiology research has access to a very large amount of public information on the habitats of microorganisms.$$$Many areas of microbiology research uses this information, primarily in biodiversity studies.$$$However the habitat information is expressed in unstructured natural language form, which hinders its exploitation at large-scale.$$$It is very common for similar habitats to be described by different terms, which makes them hard to compare automatically, e.g. intestine and gut.$$$The use of a common reference to standardize these habitat descriptions as claimed by (Ivana et al., 2010) is a necessity.$$$We propose the ontology called OntoBiotope that we have been developing since 2010.$$$The OntoBiotope ontology is in a formal machine-readable representation that enables indexing of information as well as conceptualization and reasoning.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D02197,"A team of robots sharing a common goal can benefit from coordination of the activities of team members, helping the team to reach the goal more reliably or quickly.$$$We address the problem of coordinating the actions of a team of robots with periodic communication capability executing an information gathering task.$$$We cast the problem as a multi-agent optimal decision-making problem with an information theoretic objective function.$$$We show that appropriate techniques for solving decentralized partially observable Markov decision processes (Dec-POMDPs) are applicable in such information gathering problems.$$$We quantify the usefulness of coordinated information gathering through simulation studies, and demonstrate the feasibility of the method in a real-world target tracking domain.",BACKGROUND BACKGROUND/OBJECTIVES METHODS RESULTS/CONCLUSIONS RESULTS
D00856,"Contemporary electricity distribution systems are being challenged by the variability of renewable energy sources.$$$Slow response times and long energy management periods cannot efficiently integrate intermittent renewable generation and demand.$$$Yet stochasticity can be judiciously coupled with system flexibilities to enhance grid operation efficiency.$$$Voltage magnitudes for instance can transiently exceed regulation limits, while smart inverters can be overloaded over short time intervals.$$$To implement such a mode of operation, an ergodic energy management framework is developed here.$$$Considering a distribution grid with distributed energy sources and a feed-in tariff program, active power curtailment and reactive power compensation are formulated as a stochastic optimization problem.$$$Tighter operational constraints are enforced in an average sense, while looser margins are enforced to be satisfied at all times.$$$Stochastic dual subgradient solvers are developed based on exact and approximate grid models of varying complexity.$$$Numerical tests on a real-world 56-bus distribution grid and the IEEE 123-bus test feeder relying on both grid models corroborate the advantages of the novel schemes over their deterministic alternatives.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS METHODS METHODS RESULTS/CONCLUSIONS
D01043,"This paper addresses the problem of localizing an unknown number of targets, all having the same radar signature, by a distributed MIMO radar consisting of single antenna transmitters and receivers that cannot determine directions of departure and arrival.$$$Furthermore, we consider the presence of multipath propagation, and the possible (correlated) blocking of the direct paths (going from the transmitter and reflecting off a target to the receiver).$$$In its most general form, this problem can be cast as a Bayesian estimation problem where every multipath component is accounted for.$$$However, when the environment map is unknown, this problem is ill-posed and hence, a tractable approximation is derived where only direct paths are accounted for.$$$In particular, we take into account the correlated blocking by scatterers in the environment which appears as a prior term in the Bayesian estimation framework.$$$A sub-optimal polynomial-time algorithm to solve the Bayesian multi-target localization problem with correlated blocking is proposed and its performance is evaluated using simulations.$$$We found that when correlated blocking is severe, assuming the blocking events to be independent and having constant probability (as was done in previous papers) resulted in poor detection performance, with false alarms more likely to occur than detections.",BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D02614,"Quantum information brings together theories of physics and computer science.$$$This synthesis challenges the basic intuitions of both fields.$$$In this thesis, we show that adopting a unified and general language for process theories advances foundations and practical applications of quantum information.$$$Our first set of results analyze quantum algorithms with a process theoretic structure.$$$We contribute new constructions of the Fourier transform and Pontryagin duality in dagger symmetric monoidal categories.$$$We then use this setting to study generalized unitary oracles and give a new quantum blackbox algorithm for the identification of group homomorphisms, solving the GROUPHOMID problem.$$$In the remaining section, we construct a novel model of quantum blackbox algorithms in non-deterministic classical computation.$$$Our second set of results concerns quantum foundations.$$$We complete work begun by Coecke et al., definitively connecting the Mermin non-locality of a process theory with a simple algebraic condition on that theory's phase groups.$$$This result allows us to offer new experimental tests for Mermin non-locality and new protocols for quantum secret sharing.$$$In our final chapter, we exploit the shared process theoretic structure of quantum information and distributional compositional linguistics.$$$We propose a quantum algorithm adapted from Weibe et al. to classify sentences by meaning.$$$The clarity of the process theoretic setting allows us to recover a speedup that is lost in the naive application of the algorithm.$$$The main mathematical tools used in this thesis are group theory (esp.$$$Fourier theory on finite groups), monoidal category theory, and categorical algebra.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS BACKGROUND BACKGROUND
D00068,"Distribution regression has recently attracted much interest as a generic solution to the problem of supervised learning where labels are available at the group level, rather than at the individual level.$$$Current approaches, however, do not propagate the uncertainty in observations due to sampling variability in the groups.$$$This effectively assumes that small and large groups are estimated equally well, and should have equal weight in the final regression.$$$We account for this uncertainty with a Bayesian distribution regression formalism, improving the robustness and performance of the model when group sizes vary.$$$We frame our models in a neural network style, allowing for simple MAP inference using backpropagation to learn the parameters, as well as MCMC-based inference which can fully propagate uncertainty.$$$We demonstrate our approach on illustrative toy datasets, as well as on a challenging problem of predicting age from images.",BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS RESULTS
D04718,"State-of-the-art methods of people counting in crowded scenes rely on deep networks to estimate people density in the image plane.$$$Perspective distortion effects are handled implicitly by either learning scale-invariant features or estimating density in patches of different sizes, neither of which accounts for the fact that scale changes must be consistent over the whole scene.$$$In this paper, we show that feeding an explicit model of the scale changes to the network considerably increases performance.$$$An added benefit is that it lets us reason in terms of number of people per square meter on the ground, allowing us to enforce physically-inspired temporal consistency constraints that do not have to be learned.$$$This yields an algorithm that outperforms state-of-the-art methods on crowded scenes, especially when perspective effects are strong.",RESULTS METHODS METHODS OBJECTIVES RESULTS
D02105,"This paper introduces a new computing model based on the cooperation among Turing machines called orchestrated machines.$$$Like universal Turing machines, orchestrated machines are also designed to simulate Turing machines but they can also modify the original operation of the included Turing machines to create a new layer of some kind of collective behavior.$$$Using this new model we can define some interested notions related to cooperation ability of Turing machines such as the intelligence quotient or the emotional intelligence quotient for Turing machines.",OBJECTIVES BACKGROUND OBJECTIVES
D02930,"This paper evaluates eight parallel graph processing systems: Hadoop, HaLoop, Vertica, Giraph, GraphLab (PowerGraph), Blogel, Flink Gelly, and GraphX (SPARK) over four very large datasets (Twitter, World Road Network, UK 200705, and ClueWeb) using four workloads (PageRank, WCC, SSSP and K-hop).$$$The main objective is to perform an independent scale-out study by experimentally analyzing the performance, usability, and scalability (using up to 128 machines) of these systems.$$$In addition to performance results, we discuss our experiences in using these systems and suggest some system tuning heuristics that lead to better performance.",OBJECTIVES OBJECTIVES/METHODS METHODS
D06631,"One of the most attractive features of untyped languages is the flexibility in term creation and manipulation.$$$However, with such power comes the responsibility of ensuring the correctness of these operations.$$$A solution is adding run-time checks to the program via assertions, but this can introduce overheads that are in many cases impractical.$$$While static analysis can greatly reduce such overheads, the gains depend strongly on the quality of the information inferred.$$$Reusable libraries, i.e., library modules that are pre-compiled independently of the client, pose special challenges in this context.$$$We propose a technique which takes advantage of module systems which can hide a selected set of functor symbols to significantly enrich the shape information that can be inferred for reusable libraries, as well as an improved run-time checking approach that leverages the proposed mechanisms to achieve large reductions in overhead, closer to those of static languages, even in the reusable-library context.$$$While the approach is general and system-independent, we present it for concreteness in the context of the Ciao assertion language and combined static/dynamic checking framework.$$$Our method maintains the full expressiveness of the assertion language in this context.$$$In contrast to other approaches it does not introduce the need to switch the language to a (static) type system, which is known to change the semantics in languages like Prolog.$$$We also study the approach experimentally and evaluate the overhead reduction achieved in the run-time checks.",BACKGROUND OTHERS BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS METHODS
D04118,"Interest has been revived in the creation of a ""bill of rights"" for Internet users.$$$This paper analyzes users' rights into ten broad principles, as a basis for assessing what users regard as important and for comparing different multi-issue Internet policy proposals.$$$Stability of the principles is demonstrated in an experimental survey, which also shows that freedoms of users to participate in the design and coding of platforms appear to be viewed as inessential relative to other rights.$$$An analysis of users' rights frameworks that have emerged over the past twenty years similarly shows that such proposals tend to leave out freedoms related to software platforms, as opposed to user data or public networks.$$$Evaluating policy frameworks in a comparative analysis based on prior principles may help people to see what is missing and what is important as the future of the Internet continues to be debated.",BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS RESULTS CONCLUSIONS
D04516,"An interesting problem in synchronization is the study of coupled oscillators, wherein oscillators with different natural frequencies synchronize to a common frequency and equilibrium phase difference.$$$In this paper, we investigate the stability and convergence in a network of coupled oscillators described by the Kuramoto model.$$$We consider networks with finite number of oscillators, arbitrary interconnection topology, non-uniform coupling gains and non-identical natural frequencies.$$$We show that such a network synchronizes provided the underlying graph is connected and certain conditions on the coupling gains are satisfied.$$$In the analysis, we consider as states the phase and angular frequency differences between the oscillators, and the resulting dynamics possesses a continuum of equilibria.$$$The synchronization problem involves establishing the Lyapunov stability of the fixed points and showing convergence of trajectories to these points.$$$The synchronization result is established in the framework of semistability theory.",BACKGROUND OBJECTIVES METHODS RESULTS METHODS METHODS METHODS
D01205,"The problem of unicity and reidentifiability of records in large-scale databases has been studied in different contexts and approaches, with focus on preserving privacy or matching records from different data sources.$$$With an increasing number of service providers nowadays routinely collecting location traces of their users on unprecedented scales, there is a pronounced interest in the possibility of matching records and datasets based on spatial trajectories.$$$Extending previous work on reidentifiability of spatial data and trajectory matching, we present the first large-scale analysis of user matchability in real mobility datasets on realistic scales, i.e. among two datasets that consist of several million people's mobility traces, coming from a mobile network operator and transportation smart card usage.$$$We extract the relevant statistical properties which influence the matching process and analyze their impact on the matchability of users.$$$We show that for individuals with typical activity in the transportation system (those making 3-4 trips per day on average), a matching algorithm based on the co-occurrence of their activities is expected to achieve a 16.8% success only after a one-week long observation of their mobility traces, and over 55% after four weeks.$$$We show that the main determinant of matchability is the expected number of co-occurring records in the two datasets.$$$Finally, we discuss different scenarios in terms of data collection frequency and give estimates of matchability over time.$$$We show that with higher frequency data collection becoming more common, we can expect much higher success rates in even shorter intervals.",BACKGROUND BACKGROUND OBJECTIVES/RESULTS METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D05722,"Programming is a valuable skill in the labor market, making the underrepresentation of women in computing an increasingly important issue.$$$Online question and answer platforms serve a dual purpose in this field: they form a body of knowledge useful as a reference and learning tool, and they provide opportunities for individuals to demonstrate credible, verifiable expertise.$$$Issues, such as male-oriented site design or overrepresentation of men among the site's elite may therefore compound the issue of women's underrepresentation in IT.$$$In this paper we audit the differences in behavior and outcomes between men and women on Stack Overflow, the most popular of these Q&A sites.$$$We observe significant differences in how men and women participate in the platform and how successful they are.$$$For example, the average woman has roughly half of the reputation points, the primary measure of success on the site, of the average man.$$$Using an Oaxaca-Blinder decomposition, an econometric technique commonly applied to analyze differences in wages between groups, we find that most of the gap in success between men and women can be explained by differences in their activity on the site and differences in how these activities are rewarded.$$$Specifically, 1) men give more answers than women and 2) are rewarded more for their answers on average, even when controlling for possible confounders such as tenure or buy-in to the site.$$$Women ask more questions and gain more reward per question.$$$We conclude with a hypothetical redesign of the site's scoring system based on these behavioral differences, cutting the reputation gap in half.",BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES OBJECTIVES/METHODS RESULTS RESULTS METHODS/RESULTS RESULTS RESULTS METHODS/RESULTS/CONCLUSIONS
D05621,"The discrimination and simplicity of features are very important for effective and efficient pedestrian detection.$$$However, most state-of-the-art methods are unable to achieve good tradeoff between accuracy and efficiency.$$$Inspired by some simple inherent attributes of pedestrians (i.e., appearance constancy and shape symmetry), we propose two new types of non-neighboring features (NNF): side-inner difference features (SIDF) and symmetrical similarity features (SSF).$$$SIDF can characterize the difference between the background and pedestrian and the difference between the pedestrian contour and its inner part.$$$SSF can capture the symmetrical similarity of pedestrian shape.$$$However, it's difficult for neighboring features to have such above characterization abilities.$$$Finally, we propose to combine both non-neighboring and neighboring features for pedestrian detection.$$$It's found that non-neighboring features can further decrease the average miss rate by 4.44%.$$$Experimental results on INRIA and Caltech pedestrian datasets demonstrate the effectiveness and efficiency of the proposed method.$$$Compared to the state-of-the-art methods without using CNN, our method achieves the best detection performance on Caltech, outperforming the second best method (i.e., Checkboards) by 1.63%.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D04241,"This paper proposes a novel framework to regularize the highly ill-posed and non-linear Fourier ptychography problem using generative models.$$$We demonstrate experimentally that our proposed algorithm, Deep Ptych, outperforms the existing Fourier ptychography techniques, in terms of quality of reconstruction and robustness against noise, using far fewer samples.$$$We further modify the proposed approach to allow the generative model to explore solutions outside the range, leading to improved performance.",METHODS CONCLUSIONS RESULTS
D05959,"Learning sparse linear models with two-way interactions is desirable in many application domains such as genomics.$$$l1-regularised linear models are popular to estimate sparse models, yet standard implementations fail to address specifically the quadratic explosion of candidate two-way interactions in high dimensions, and typically do not scale to genetic data with hundreds of thousands of features.$$$Here we present WHInter, a working set algorithm to solve large l1-regularised problems with two-way interactions for binary design matrices.$$$The novelty of WHInter stems from a new bound to efficiently identify working sets while avoiding to scan all features, and on fast computations inspired from solutions to the maximum inner product search problem.$$$We apply WHInter to simulated and real genetic data and show that it is more scalable and two orders of magnitude faster than the state of the art.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS
D03695,"We consider the problem of finding the isolated common roots of a set of polynomial functions defining a zero-dimensional ideal I in a ring R of polynomials over C. Normal form algorithms provide an algebraic approach to solve this problem.$$$The framework presented in Telen et al.$$$(2018) uses truncated normal forms (TNFs) to compute the algebra structure of R/I and the solutions of I.$$$This framework allows for the use of much more general bases than the standard monomials for R/I.$$$This is exploited in this paper to introduce the use of two special (nonmonomial) types of basis functions with nice properties.$$$This allows, for instance, to adapt the basis functions to the expected location of the roots of I.$$$We also propose algorithms for efficient computation of TNFs and a generalization of the construction of TNFs in the case of non-generic zero-dimensional systems.$$$The potential of the TNF method and usefulness of the new results are exposed by many experiments.",OBJECTIVES BACKGROUND METHODS RESULTS RESULTS RESULTS RESULTS RESULTS
D05335,"Energy price forecasting is a relevant yet hard task in the field of multi-step time series forecasting.$$$In this paper we compare a well-known and established method, ARMA with exogenous variables with a relatively new technique Gradient Boosting Regression.$$$The method was tested on data from Global Energy Forecasting Competition 2014 with a year long rolling window forecast.$$$The results from the experiment reveal that a multi-model approach is significantly better performing in terms of error metrics.$$$Gradient Boosting can deal with seasonality and auto-correlation out-of-the box and achieve lower rate of normalized mean absolute error on real-world data.",BACKGROUND OBJECTIVES/METHODS METHODS RESULTS CONCLUSIONS
D01903,"Sentence embedding is an important research topic in natural language processing.$$$It is essential to generate a good embedding vector that fully reflects the semantic meaning of a sentence in order to achieve an enhanced performance for various natural language processing tasks, such as machine translation and document classification.$$$Thus far, various sentence embedding models have been proposed, and their feasibility has been demonstrated through good performances on tasks following embedding, such as sentiment analysis and sentence classification.$$$However, because the performances of sentence classification and sentiment analysis can be enhanced by using a simple sentence representation method, it is not sufficient to claim that these models fully reflect the meanings of sentences based on good performances for such tasks.$$$In this paper, inspired by human language recognition, we propose the following concept of semantic coherence, which should be satisfied for a good sentence embedding method: similar sentences should be located close to each other in the embedding space.$$$Then, we propose the Paraphrase-Thought (P-thought) model to pursue semantic coherence as much as possible.$$$Experimental results on two paraphrase identification datasets (MS COCO and STS benchmark) show that the P-thought models outperform the benchmarked sentence embedding methods.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D02739,"In mobile crowdsensing, finding the best match between tasks and users is crucial to ensure both the quality and effectiveness of a crowdsensing system.$$$Existing works usually assume a centralized task assignment by the crowdsensing platform, without addressing the need of fine-grained personalized task matching.$$$In this paper, we argue that it is essential to match tasks to users based on a careful characterization of both the users' preference and reliability.$$$To that end, we propose a personalized task recommender system for mobile crowdsensing, which recommends tasks to users based on a recommendation score that jointly takes each user's preference and reliability into consideration.$$$We first present a hybrid preference metric to characterize users' preference by exploiting their implicit feedback.$$$Then, to profile users' reliability levels, we formalize the problem as a semi-supervised learning model, and propose an efficient block coordinate descent algorithm to solve the problem.$$$For some tasks that lack users' historical information, we further propose a matrix factorization method to infer the users' reliability levels on those tasks.$$$We conduct extensive experiments to evaluate the performance of our system, and the evaluation results demonstrate that our system can achieve superior performance to the benchmarks in both user profiling and personalized task recommendation.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS
D04083,"It is not known whether Thompson's group F is automatic.$$$With the recent extensions of the notion of an automatic group to graph automatic by Kharlampovich, Khoussainov and Miasnikov and then to C-graph automatic by the authors, a compelling question is whether F is graph automatic or C-graph automatic for an appropriate language class C. The extended definitions allow the use of a symbol alphabet for the normal form language, replacing the dependence on generating set.$$$In this paper we construct a 1-counter graph automatic structure for F based on the standard infinite normal form for group elements.",BACKGROUND OBJECTIVES RESULTS
D03650,"This paper deals with the issue of the perceptual quality evaluation of user-generated videos shared online, which is an important step toward designing video-sharing services that maximize users' satisfaction in terms of quality.$$$We first analyze viewers' quality perception patterns by applying graph analysis techniques to subjective rating data.$$$We then examine the performance of existing state-of-the-art objective metrics for the quality estimation of user-generated videos.$$$In addition, we investigate the feasibility of metadata accompanied with videos in online video-sharing services for quality estimation.$$$Finally, various issues in the quality assessment of online user-generated videos are discussed, including difficulties and opportunities.",BACKGROUND/OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D06360,"Moments capture a huge part of our lives.$$$Accurate recognition of these moments is challenging due to the diverse and complex interpretation of the moments.$$$Action recognition refers to the act of classifying the desired action/activity present in a given video.$$$In this work, we perform experiments on Moments in Time dataset to recognize accurately activities occurring in 3 second clips.$$$We use state of the art techniques for visual, auditory and spatio temporal localization and develop method to accurately classify the activity in the Moments in Time dataset.$$$Our novel approach of using Visual Based Textual features and fusion techniques performs well providing an overall 89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the Baseline TRN model.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES METHODS RESULTS
D00380,"Scheduling surgeries is a challenging task due to the fundamental uncertainty of the clinical environment, as well as the risks and costs associated with under- and over-booking.$$$We investigate neural regression algorithms to estimate the parameters of surgery case durations, focusing on the issue of heteroscedasticity.$$$We seek to simultaneously estimate the duration of each surgery, as well as a surgery-specific notion of our uncertainty about its duration.$$$Estimating this uncertainty can lead to more nuanced and effective scheduling strategies, as we are able to schedule surgeries more efficiently while allowing an informed and case-specific margin of error.$$$Using surgery records %from the UC San Diego Health System, from a large United States health system we demonstrate potential improvements on the order of 20% (in terms of minutes overbooked) compared to current scheduling techniques.$$$Moreover, we demonstrate that surgery durations are indeed heteroscedastic.$$$We show that models that estimate case-specific uncertainty better fit the data (log likelihood).$$$Additionally, we show that the heteroscedastic predictions can more optimally trade off between over and under-booking minutes, especially when idle minutes and scheduling collisions confer disparate costs.",BACKGROUND OBJECTIVES OBJECTIVES/METHODS OBJECTIVES/RESULTS METHODS/RESULTS RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06545,"Machine translation (MT) was developed as one of the hottest research topics in the natural language processing (NLP) literature.$$$One important issue in MT is that how to evaluate the MT system reasonably and tell us whether the translation system makes an improvement or not.$$$The traditional manual judgment methods are expensive, time-consuming, unrepeatable, and sometimes with low agreement.$$$On the other hand, the popular automatic MT evaluation methods have some weaknesses.$$$Firstly, they tend to perform well on the language pairs with English as the target language, but weak when English is used as source.$$$Secondly, some methods rely on many additional linguistic features to achieve good performance, which makes the metric unable to replicate and apply to other language pairs easily.$$$Thirdly, some popular metrics utilize incomprehensive factors, which result in low performance on some practical tasks.$$$In this thesis, to address the existing problems, we design novel MT evaluation methods and investigate their performances on different languages.$$$Firstly, we design augmented factors to yield highly accurate evaluation.Secondly, we design a tunable evaluation model where weighting of factors can be optimised according to the characteristics of languages.$$$Thirdly, in the enhanced version of our methods, we design concise linguistic feature using POS to show that our methods can yield even higher performance when using some external linguistic resources.$$$Finally, we introduce the practical performance of our metrics in the ACL-WMT workshop shared tasks, which show that the proposed methods are robust across different languages.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D04363,"Video description is the automatic generation of natural language sentences that describe the contents of a given video.$$$It is useful for helping the visually impaired, video subtitling and robotics.$$$The past few years have seen a surge of research in this area due to the unprecedented success of deep learning in computer vision and natural language processing.$$$Numerous methods, datasets and evaluation metrics have been proposed in the literature, calling the need for a comprehensive survey to focus research efforts in this flourishing new direction.$$$This paper fills the gap by surveying the state of the art approaches with a focus on deep learning models; comparing benchmark datasets in terms of their domain, number of classes, and repository size; and identifying the pros and cons of various evaluation metrics like SPICE, CIDEr, ROUGE, BLEU, METEOR, and WMD.$$$Classical approaches combined subject, object and verb detection with template based language models to generate sentences.$$$However, the release of large datasets revealed that these methods can not cope with the diversity in open domain videos.$$$Classical approaches were followed by a very short era of statistical methods which were soon replaced with deep learning, the current state of the art in video description.$$$Our survey shows that despite the fast-paced developments, video description research is still in its infancy due to the following reasons.$$$Firstly, existing datasets neither contain adequate visual diversity nor complexity of linguistic structures.$$$Secondly, current evaluation metrics fall short of measuring the agreement between machine generated descriptions with that of humans.$$$From an algorithmic point of view, diagnosis of new models is challenging because it is difficult to ascertain the contributions of the visual features and the adopted language model to the final description.$$$We conclude...",OTHERS BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D01665,"Varying weather conditions, including rainfall and snowfall, are generally regarded as a challenge for computer vision algorithms.$$$One proposed solution to the challenges induced by rain and snowfall is to artificially remove the rain from images or video using rain removal algorithms.$$$It is the promise of these algorithms that the rain-removed image frames will improve the performance of subsequent segmentation and tracking algorithms.$$$However, rain removal algorithms are typically evaluated on their ability to remove synthetic rain on a small subset of images.$$$Currently, their behavior is unknown on real-world videos when integrated with a typical computer vision pipeline.$$$In this paper, we review the existing rain removal algorithms and propose a new dataset that consists of 22 traffic surveillance sequences under a broad variety of weather conditions that all include either rain or snowfall.$$$We propose a new evaluation protocol that evaluates the rain removal algorithms on their ability to improve the performance of subsequent segmentation, instance segmentation, and feature tracking algorithms under rain and snow.$$$If successful, the de-rained frames of a rain removal algorithm should improve segmentation performance and increase the number of accurately tracked features.$$$The results show that a recent single-frame-based rain removal algorithm increases the segmentation performance by 19.7% on our proposed dataset, but it eventually decreases the feature tracking performance and showed mixed results with recent instance segmentation methods.$$$However, the best video-based rain removal algorithm improves the feature tracking accuracy by 7.72%.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS OBJECTIVES RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05525,"Data mining techniques have been widely used to mine knowledgeable information from medical data bases.$$$In data mining classification is a supervised learning that can be used to design models describing important data classes, where class attribute is involved in the construction of the classifier.$$$Nearest neighbor (KNN) is very simple, most popular, highly efficient and effective algorithm for pattern recognition.KNN is a straight forward classifier, where samples are classified based on the class of their nearest neighbor.$$$Medical data bases are high volume in nature.$$$If the data set contains redundant and irrelevant attributes, classification may produce less accurate result.$$$Heart disease is the leading cause of death in INDIA.$$$In Andhra Pradesh heart disease was the leading cause of mortality accounting for 32%of all deaths, a rate as high as Canada (35%) and USA.Hence there is a need to define a decision support system that helps clinicians decide to take precautionary steps.$$$In this paper we propose a new algorithm which combines KNN with genetic algorithm for effective classification.$$$Genetic algorithms perform global search in complex large and multimodal landscapes and provide optimal solution.$$$Experimental results shows that our algorithm enhance the accuracy in diagnosis of heart disease.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D03268,"We perform a statistical analysis of scientific-publication data with a goal to provide quantitative analysis of scientific process.$$$Such an investigation belongs to the newly established field of scientometrics: a branch of the general science of science that covers all quantitative methods to analyze science and research process.$$$As a case study we consider download and citation statistics of the journal `Europhysics Letters' (EPL), as Europe's flagship letters journal of broad interest to the physics community.$$$While citations are usually considered as an indicator of academic impact, downloads reflect rather the level of attractiveness or popularity of a publication.$$$We discuss peculiarities of both processes and correlations between them.",METHODS BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES
D05559,This is the preprint version of our paper on 2015 International Conference on Virtual Rehabilitation (ICVR2015).$$$The purpose of this work is designing and implementing a rehabilitation software for dysphonic patients.$$$Constant training is a key factor for this type of therapy.$$$The patient can play the game as well as conduct the voice training simultaneously guided by therapists at clinic or exercise independently at home.$$$The voice information can be recorded and extracted for evaluating the long-time rehabilitation progress.,BACKGROUND OBJECTIVES BACKGROUND METHODS RESULTS
D03349,"A geometrical pattern is a set of points with all pairwise distances (or, more generally, relative distances) specified.$$$Finding matches to such patterns has applications to spatial data in seismic, astronomical, and transportation contexts.$$$For example, a particularly interesting geometric pattern in astronomy is the Einstein cross, which is an astronomical phenomenon in which a single quasar is observed as four distinct sky objects (due to gravitational lensing) when captured by earth telescopes.$$$Finding such crosses, as well as other geometric patterns, is a challenging problem as the potential number of sets of elements that compose shapes is exponentially large in the size of the dataset and the pattern.$$$In this paper, we denote geometric patterns as constellation queries and propose algorithms to find them in large data applications.$$$Our methods combine quadtrees, matrix multiplication, and unindexed join processing to discover sets of points that match a geometric pattern within some additive factor on the pairwise distances.$$$Our distributed experiments show that the choice of composition algorithm (matrix multiplication or nested loops) depends on the freedom introduced in the query geometry through the distance additive factor.$$$Three clearly identified blocks of threshold values guide the choice of the best composition algorithm.$$$Finally, solving the problem for relative distances requires a novel continuous-to-discrete transformation.$$$To the best of our knowledge this paper is the first to investigate constellation queries at scale.",OTHERS BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS METHODS OTHERS
D06789,"This paper provides guidance to an analyst who wants to extract insight from a spreadsheet model.$$$It discusses the terminology of spreadsheet analytics, how to prepare a spreadsheet model for analysis, and a hierarchy of analytical techniques.$$$These techniques include sensitivity analysis, tornado charts,and backsolving (or goal-seeking).$$$This paper presents native-Excel approaches for automating these techniques, and discusses add-ins that are even more efficient.$$$Spreadsheet optimization and spreadsheet Monte Carlo simulation are briefly discussed.$$$The paper concludes by calling for empirical research, and describing desired features spreadsheet sensitivity analysis and spreadsheet optimization add-ins.",OBJECTIVES RESULTS RESULTS RESULTS RESULTS RESULTS
D00521,"Optical flow, semantic segmentation, and surface normals represent different information modalities, yet together they bring better cues for scene understanding problems.$$$In this paper, we study the influence between the three modalities: how one impacts on the others and their efficiency in combination.$$$We employ a modular approach using a convolutional refinement network which is trained supervised but isolated from RGB images to enforce joint modality features.$$$To assist the training process, we create a large-scale synthetic outdoor dataset that supports dense annotation of semantic segmentation, optical flow, and surface normals.$$$The experimental results show positive influence among the three modalities, especially for objects' boundaries, region consistency, and scene structures.",BACKGROUND OBJECTIVES METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D01885,"We describe a novel approach to interpret a polar code as a low-density parity-check (LDPC)-like code with an underlying sparse decoding graph.$$$This sparse graph is based on the encoding factor graph of polar codes and is suitable for conventional belief propagation (BP) decoding.$$$We discuss several pruning techniques based on the check node decoder (CND) and variable node decoder (VND) update equations, significantly reducing the size (i.e., decoding complexity) of the parity-check matrix.$$$As a result, iterative polar decoding can then be conducted on a sparse graph, akin to the traditional well-established LDPC decoding, e.g., using a fully parallel sum-product algorithm (SPA).$$$This facilitates the systematic analysis and design of polar codes using the well-established tools known from analyzing LDPC codes.$$$We show that the proposed iterative polar decoder has a negligible performance loss for short-to-intermediate codelengths compared to Arikan's original BP decoder.$$$Finally, the proposed decoder is shown to benefit from both reduced complexity and reduced memory requirements and, thus, is more suitable for hardware implementations.",OBJECTIVES RESULTS METHODS RESULTS RESULTS RESULTS RESULTS
D04510,"Autonomous vehicles (AVs) require accurate metric and topological location estimates for safe, effective navigation and decision-making.$$$Although many high-definition (HD) roadmaps exist, they are not always accurate since public roads are dynamic, shaped unpredictably by both human activity and nature.$$$Thus, AVs must be able to handle situations in which the topology specified by the map does not agree with reality.$$$We present the Variable Structure Multiple Hidden Markov Model (VSM-HMM) as a framework for localizing in the presence of topological uncertainty, and demonstrate its effectiveness on an AV where lane membership is modeled as a topological localization process.$$$VSM-HMMs use a dynamic set of HMMs to simultaneously reason about location within a set of most likely current topologies and therefore may also be applied to topological structure estimation as well as AV lane estimation.$$$In addition, we present an extension to the Earth Mover's Distance which allows uncertainty to be taken into account when computing the distance between belief distributions on simplices of arbitrary relative sizes.",BACKGROUND BACKGROUND OBJECTIVES RESULTS METHODS RESULTS
D01694,"Despite the significant progress that has been made on estimating optical flow recently, most estimation methods, including classical and deep learning approaches, still have difficulty with multi-scale estimation, real-time computation, and/or occlusion reasoning.$$$In this paper, we introduce dilated convolution and occlusion reasoning into unsupervised optical flow estimation to address these issues.$$$The dilated convolution allows our network to avoid upsampling via deconvolution and the resulting gridding artifacts.$$$Dilated convolution also results in a smaller memory footprint which speeds up interference.$$$The occlusion reasoning prevents our network from learning incorrect deformations due to occluded image regions during training.$$$Our proposed method outperforms state-of-the-art unsupervised approaches on the KITTI benchmark.$$$We also demonstrate its generalization capability by applying it to action recognition in video.",BACKGROUND METHODS RESULTS RESULTS RESULTS RESULTS OBJECTIVES
D01666,"Coherent network error correction is the error-control problem in network coding with the knowledge of the network codes at the source and sink nodes.$$$With respect to a given set of local encoding kernels defining a linear network code, we obtain refined versions of the Hamming bound, the Singleton bound and the Gilbert-Varshamov bound for coherent network error correction.$$$Similar to its classical counterpart, this refined Singleton bound is tight for linear network codes.$$$The tightness of this refined bound is shown by two construction algorithms of linear network codes achieving this bound.$$$These two algorithms illustrate different design methods: one makes use of existing network coding algorithms for error-free transmission and the other makes use of classical error-correcting codes.$$$The implication of the tightness of the refined Singleton bound is that the sink nodes with higher maximum flow values can have higher error correction capabilities.",BACKGROUND RESULTS RESULTS METHODS OTHERS CONCLUSIONS
D00001,"Rapid popularity of Internet of Things (IoT) and cloud computing permits neuroscientists to collect multilevel and multichannel brain data to better understand brain functions, diagnose diseases, and devise treatments.$$$To ensure secure and reliable data communication between end-to-end (E2E) devices supported by current IoT and cloud infrastructure, trust management is needed at the IoT and user ends.$$$This paper introduces a Neuro-Fuzzy based Brain-inspired trust management model (TMM) to secure IoT devices and relay nodes, and to ensure data reliability.$$$The proposed TMM utilizes node behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference System and weighted-additive methods respectively to assess the nodes trustworthiness.$$$In contrast to the existing fuzzy based TMMs, the NS2 simulation results confirm the robustness and accuracy of the proposed TMM in identifying malicious nodes in the communication network.$$$With the growing usage of cloud based IoT frameworks in Neuroscience research, integrating the proposed TMM into the existing infrastructure will assure secure and reliable data communication among the E2E devices.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D03423,"This paper explores the problem of breast tissue classification of microscopy images.$$$Based on the predominant cancer type the goal is to classify images into four categories of normal, benign, in situ carcinoma, and invasive carcinoma.$$$Given a suitable training dataset, we utilize deep learning techniques to address the classification problem.$$$Due to the large size of each image in the training dataset, we propose a patch-based technique which consists of two consecutive convolutional neural networks.$$$The first ""patch-wise"" network acts as an auto-encoder that extracts the most salient features of image patches while the second ""image-wise"" network performs classification of the whole image.$$$The first network is pre-trained and aimed at extracting local information while the second network obtains global information of an input image.$$$We trained the networks using the ICIAR 2018 grand challenge on BreAst Cancer Histology (BACH) dataset.$$$The proposed method yields 95 % accuracy on the validation set compared to previously reported 77 % accuracy rates in the literature.$$$Our code is publicly available at https://github.com/ImagingLab/ICIAR2018",OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS OTHERS
D02415,"Interpretability and small labelled datasets are key issues in the practical application of deep learning, particularly in areas such as medicine.$$$In this paper, we present a semi-supervised technique that addresses both these issues by leveraging large unlabelled datasets to encode and decode images into a dense latent representation.$$$Using chest radiography as an example, we apply this encoder to other labelled datasets and apply simple models to the latent vectors to learn algorithms to identify heart failure.$$$For each prediction, we generate visual rationales by optimizing a latent representation to minimize the prediction of disease while constrained by a similarity measure in image space.$$$Decoding the resultant latent representation produces an image without apparent disease.$$$The difference between the original decoding and the altered image forms an interpretable visual rationale for the algorithm's prediction on that image.$$$We also apply our method to the MNIST dataset and compare the generated rationales to other techniques described in the literature.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D01198,"The project presented in this article aims to formalize criteria and procedures in order to extract semantic information from parsed dictionary glosses.$$$The actual purpose of the project is the generation of a semantic network (nearly an ontology) issued from a monolingual Italian dictionary, through unsupervised procedures.$$$Since the project involves rule-based Parsing, Semantic Tagging and Word Sense Disambiguation techniques, its outcomes may find an interest also beyond this immediate intent.$$$The cooperation of both syntactic and semantic features in meaning construction are investigated, and procedures which allows a translation of syntactic dependencies in semantic relations are discussed.$$$The procedures that rise from this project can be applied also to other text types than dictionary glosses, as they convert the output of a parsing process into a semantic representation.$$$In addition some mechanism are sketched that may lead to a kind of procedural semantics, through which multiple paraphrases of an given expression can be generated.$$$Which means that these techniques may find an application also in 'query expansion' strategies, interesting Information Retrieval, Search Engines and Question Answering Systems.",OBJECTIVES OBJECTIVES METHODS/CONCLUSIONS METHODS OTHERS METHODS OTHERS
D04535,"We present a structural clustering algorithm for large-scale datasets of small labeled graphs, utilizing a frequent subgraph sampling strategy.$$$A set of representatives provides an intuitive description of each cluster, supports the clustering process, and helps to interpret the clustering results.$$$The projection-based nature of the clustering approach allows us to bypass dimensionality and feature extraction problems that arise in the context of graph datasets reduced to pairwise distances or feature vectors.$$$While achieving high quality and (human) interpretable clusterings, the runtime of the algorithm only grows linearly with the number of graphs.$$$Furthermore, the approach is easy to parallelize and therefore suitable for very large datasets.$$$Our extensive experimental evaluation on synthetic and real world datasets demonstrates the superiority of our approach over existing structural and subspace clustering algorithms, both, from a runtime and quality point of view.",BACKGROUND/OBJECTIVES METHODS BACKGROUND/METHODS RESULTS METHODS RESULTS/CONCLUSIONS
D00869,"Computationally synthesized blood vessels can be used for training and evaluation of medical image analysis applications.$$$We propose a deep generative model to synthesize blood vessel geometries, with an application to coronary arteries in cardiac CT angiography (CCTA).$$$In the proposed method, a Wasserstein generative adversarial network (GAN) consisting of a generator and a discriminator network is trained.$$$While the generator tries to synthesize realistic blood vessel geometries, the discriminator tries to distinguish synthesized geometries from those of real blood vessels.$$$Both real and synthesized blood vessel geometries are parametrized as 1D signals based on the central vessel axis.$$$The generator can optionally be provided with an attribute vector to synthesize vessels with particular characteristics.$$$The GAN was optimized using a reference database with parametrizations of 4,412 real coronary artery geometries extracted from CCTA scans.$$$After training, plausible coronary artery geometries could be synthesized based on random vectors sampled from a latent space.$$$A qualitative analysis showed strong similarities between real and synthesized coronary arteries.$$$A detailed analysis of the latent space showed that the diversity present in coronary artery anatomy was accurately captured by the generator.$$$Results show that Wasserstein generative adversarial networks can be used to synthesize blood vessel geometries.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D06203,"Deep Neural Networks (DNNs) are becoming an important tool in modern computing applications.$$$Accelerating their training is a major challenge and techniques range from distributed algorithms to low-level circuit design.$$$In this survey, we describe the problem from a theoretical perspective, followed by approaches for its parallelization.$$$We present trends in DNN architectures and the resulting implications on parallelization strategies.$$$We then review and model the different types of concurrency in DNNs: from the single operator, through parallelism in network inference and training, to distributed deep learning.$$$We discuss asynchronous stochastic optimization, distributed system architectures, communication schemes, and neural architecture search.$$$Based on those approaches, we extrapolate potential directions for parallelism in deep learning.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS CONCLUSIONS
D02515,"In this work, we study the effects of finite buffers on the throughput and delay of line networks with erasure links.$$$We identify the calculation of performance parameters such as throughput and delay to be equivalent to determining the stationary distribution of an irreducible Markov chain.$$$We note that the number of states in the Markov chain grows exponentially in the size of the buffers with the exponent scaling linearly with the number of hops in a line network.$$$We then propose a simplified iterative scheme to approximately identify the steady-state distribution of the chain by decoupling the chain to smaller chains.$$$The approximate solution is then used to understand the effect of buffer sizes on throughput and distribution of packet delay.$$$Further, we classify nodes based on congestion that yields an intelligent scheme for memory allocation using the proposed framework.$$$Finally, by simulations we confirm that our framework yields an accurate prediction of the variation of the throughput and delay distribution.",OBJECTIVES METHODS BACKGROUND METHODS RESULTS CONCLUSIONS RESULTS
D06681,"Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports.$$$To mine these data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline.$$$However, considering a large number of ambiguous words in one language or technical domain, we may encounter limiting constraints for proper deployment of existing WSD models.$$$This paper attempts to address the problem of one-classifier-per-one-word WSD algorithms by proposing a single Bidirectional Long Short-Term Memory (BLSTM) network which by considering senses and context sequences works on all ambiguous words collectively.$$$Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms.$$$We also discuss how applying additional modifications alleviates the model fault and the need for more training data.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS METHODS/RESULTS
D01761,"Many real-world systems are profitably described as complex networks that grow over time.$$$Preferential attachment and node fitness are two simple growth mechanisms that not only explain certain structural properties commonly observed in real-world systems, but are also tied to a number of applications in modeling and inference.$$$While there are statistical packages for estimating various parametric forms of the preferential attachment function, there is no such package implementing non-parametric estimation procedures.$$$The non-parametric approach to the estimation of the preferential attachment function allows for comparatively finer-grained investigations of the `rich-get-richer' phenomenon that could lead to novel insights in the search to explain certain nonstandard structural properties observed in real-world networks.$$$This paper introduces the R package PAFit, which implements non-parametric procedures for estimating the preferential attachment function and node fitnesses in a growing network, as well as a number of functions for generating complex networks from these two mechanisms.$$$The main computational part of the package is implemented in C++ with OpenMP to ensure scalability to large-scale networks.$$$We first introduce the main functionalities of PAFit through simulated examples, and then use the package to analyze a collaboration network between scientists in the field of complex networks.$$$The results indicate the joint presence of `rich-get-richer' and `fit-get-richer' phenomena in the collaboration network.$$$The estimated attachment function is observed to be near-linear, which we interpret as meaning that the chance an author gets a new collaborator is proportional to their current number of collaborators.$$$Furthermore, the estimated author fitnesses reveal a host of familiar faces from the complex networks community among the field's topmost fittest network scientists.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS
D00581,"The adaptability of the convolutional neural network (CNN) technique for aerodynamic meta-modeling tasks is probed in this work.$$$The primary objective is to develop suitable CNN architecture for variable flow conditions and object geometry, in addition to identifying a sufficient data preparation process.$$$Multiple CNN structures were trained to learn the lift coefficients of the airfoils with a variety of shapes in multiple flow Mach numbers, Reynolds numbers, and diverse angles of attack.$$$This is conducted to illustrate the concept of the technique.$$$A multi-layered perceptron (MLP) is also used for the training sets.$$$The MLP results are compared with that of the CNN results.$$$The newly proposed meta-modeling concept has been found to be comparable with the MLP in learning capability; and more importantly, our CNN model exhibits a competitive prediction accuracy with minimal constraints in a geometric representation.",OBJECTIVES OBJECTIVES METHODS OBJECTIVES METHODS RESULTS RESULTS/CONCLUSIONS
D06048,"In this paper, the multiple-input multiple-output (MIMO) transmit beampattern matching problem is considered.$$$The problem is formulated to approximate a desired transmit beampattern (i.e., an energy distribution in space and frequency) and to minimize the cross-correlation of signals reflected back to the array by considering different practical waveform constraints at the same time.$$$Due to the nonconvexity of the objective function and the waveform constraints, the optimization problem is highly nonconvex.$$$An efficient one-step method is proposed to solve this problem based on the majorization-minimization (MM) method.$$$The performance of the proposed algorithms compared to the state-of-art algorithms is shown through numerical simulations.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS RESULTS/CONCLUSIONS
D02332,"In this paper we present reclaimID: An architecture that allows users to reclaim their digital identities by securely sharing identity attributes without the need for a centralised service provider.$$$We propose a design where user attributes are stored in and shared over a name system under user-owned namespaces.$$$Attributes are encrypted using attribute-based encryption (ABE), allowing the user to selectively authorize and revoke access of requesting parties to subsets of his attributes.$$$We present an implementation based on the decentralised GNU Name System (GNS) in combination with ciphertext-policy ABE using type-1 pairings.$$$To show the practicality of our implementation, we carried out experimental evaluations of selected implementation aspects including attribute resolution performance.$$$Finally, we show that our design can be used as a standard OpenID Connect Identity Provider allowing our implementation to be integrated into standard-compliant services.",OBJECTIVES/RESULTS METHODS METHODS RESULTS METHODS/CONCLUSIONS METHODS/CONCLUSIONS
D03202,"Information quality in social media is an increasingly important issue, but web-scale data hinders experts' ability to assess and correct much of the inaccurate content, or `fake news,' present in these platforms.$$$This paper develops a method for automating fake news detection on Twitter by learning to predict accuracy assessments in two credibility-focused Twitter datasets: CREDBANK, a crowdsourced dataset of accuracy assessments for events in Twitter, and PHEME, a dataset of potential rumors in Twitter and journalistic assessments of their accuracies.$$$We apply this method to Twitter content sourced from BuzzFeed's fake news dataset and show models trained against crowdsourced workers outperform models based on journalists' assessment and models trained on a pooled dataset of both crowdsourced workers and journalists.$$$All three datasets, aligned into a uniform format, are also publicly available.$$$A feature analysis then identifies features that are most predictive for crowdsourced and journalistic accuracy assessments, results of which are consistent with prior work.$$$We close with a discussion contrasting accuracy and credibility and why models of non-experts outperform models of journalists for fake news detection in Twitter.",BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS OTHERS RESULTS CONCLUSIONS
D06353,"A critical task of a radar receiver is data association, which assigns radar target detections to target filter tracks.$$$Motivated by its importance, this paper introduces the problem of jointly designing multiple-input multiple-output (MIMO) radar transmit beam patterns and the corresponding data association schemes.$$$We show that the coupling of the beamforming and the association subproblems can be conveniently parameterized by what we term an ambiguity graph, which prescribes if two targets are to be disambiguated by the beamforming design or by the data association scheme.$$$The choice of ambiguity graph determines which of the two subproblems is more difficult and therefore allows to trade performance of one versus the other, resulting in a detection-association trade-off.$$$This paper shows how to design both the beam pattern and the association scheme for a given ambiguity graph.$$$It then discusses how to choose an ambiguity graph achieving close to the optimal detection-association trade-off.",BACKGROUND OBJECTIVES METHODS METHODS METHODS/RESULTS METHODS/RESULTS
D06791,"Recent developments in quaternion-valued widely linear processing have established that the exploitation of complete second-order statistics requires consideration of both the standard covariance and the three complementary covariance matrices.$$$Although such matrices have a tremendous amount of structure and their decomposition is a powerful tool in a variety of applications, the non-commutative nature of the quaternion product has been prohibitive to the development of quaternion uncorrelating transforms.$$$To this end, we introduce novel techniques for a simultaneous decomposition of the covariance and complementary covariance matrices in the quaternion domain, whereby the quaternion version of the Takagi factorisation is explored to diagonalise symmetric quaternion-valued matrices.$$$This gives new insights into the quaternion uncorrelating transform (QUT) and forms a basis for the proposed quaternion approximate uncorrelating transform (QAUT) which simultaneously diagonalises all four covariance matrices associated with improper quaternion signals.$$$The effectiveness of the proposed uncorrelating transforms is validated by simulations on both synthetic and real-world quaternion-valued signals.",BACKGROUND BACKGROUND/OBJECTIVES METHODS METHODS RESULTS
D06953,"Standardized corpora of undeciphered scripts, a necessary starting point for computational epigraphy, requires laborious human effort for their preparation from raw archaeological records.$$$Automating this process through machine learning algorithms can be of significant aid to epigraphical research.$$$Here, we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered Indus script, as found in archaeological artifacts, and returns as output a string of graphemes, suitable for inclusion in a standard corpus.$$$The image is first decomposed into regions using Selective Search and these regions are classified as containing textual and/or graphical information using a convolutional neural network.$$$Regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information.$$$The remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes.$$$This set is finally passed to a second convolutional neural network to classify the graphemes, based on a standard corpus.$$$The classifier can identify the presence or absence of the most frequent Indus grapheme, the ""jar"" sign, with an accuracy of 92%.$$$Our results demonstrate the great potential of deep learning approaches in computational epigraphy and, more generally, in the digital humanities.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D05602,"There is little doubt in scientific circles that--counting from the origin of life towards today--evolution has led to an increase in the amount of information stored within the genomes of the biosphere.$$$This trend of increasing information on average likely holds for every successful line of descent, but it is not clear whether this increase is due to a general law, or whether it is a secondary effect linked to an overall increase in fitness.$$$Here, we use ""digital life"" evolution experiments to study whether information is under selection if treated as an organismal trait, using the Price equation.$$$By measuring both sides of the equation individually in an adapting population, the strength of selection on a trait appears as a ""gap"" between the two terms of the right-hand-side of the Price equation.$$$We find that information is strongly selected (as it encodes all fitness-producing traits) by comparing the strength of selection on information to a weakly selected trait (sequence length), as well as to a neutral marker.$$$We observe that while strength of selection on arbitrary traits can vary during an experiment (including reversing sign), information is a selectable trait that must increase in a fixed environment.",BACKGROUND BACKGROUND/OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS CONCLUSIONS
D05601,"For many biological image segmentation tasks, including topological knowledge, such as the nesting of classes, can greatly improve results.$$$However, most `out-of-the-box' CNN models are still blind to such prior information.$$$In this paper, we propose a novel approach to encode this information, through a multi-level activation layer and three compatible losses.$$$We benchmark all of them on nuclei segmentation in bright-field microscopy cell images from the 2018 Data Science Bowl challenge, offering an exemplary segmentation task with cells and nested subcellular structures.$$$Our scheme greatly speeds up learning, and outperforms standard multi-class classification with soft-max activation and a previously proposed method stemming from it, improving the Dice score significantly (p-values<0.007).$$$Our approach is conceptually simple, easy to implement and can be integrated in any CNN architecture.$$$It can be generalized to a higher number of classes, with or without further relations of containment.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D03811,"Learning with recurrent neural networks (RNNs) on long sequences is a notoriously difficult task.$$$There are three major challenges: 1) complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization.$$$In this paper, we introduce a simple yet effective RNN connection structure, the DilatedRNN, which simultaneously tackles all of these challenges.$$$The proposed architecture is characterized by multi-resolution dilated recurrent skip connections and can be combined flexibly with diverse RNN cells.$$$Moreover, the DilatedRNN reduces the number of parameters needed and enhances training efficiency significantly, while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies.$$$To provide a theory-based quantification of the architecture's advantages, we introduce a memory capacity measure, the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures.$$$We rigorously prove the advantages of the DilatedRNN over other recurrent neural architectures.$$$The code for our method is publicly available at https://github.com/code-terminator/DilatedRNN",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS OTHERS
D01245,"Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performance in many tasks such as image classification and language understanding.$$$However, most existing works only optimize for model accuracy and largely ignore other important factors imposed by the underlying hardware and devices, such as latency and energy, when making inference.$$$In this paper, we first introduce the problem of NAS and provide a survey on recent works.$$$Then we deep dive into two recent advancements on extending NAS into multiple-objective frameworks: MONAS and DPP-Net.$$$Both MONAS and DPP-Net are capable of optimizing accuracy and other objectives imposed by devices, searching for neural architectures that can be best deployed on a wide spectrum of devices: from embedded systems and mobile devices to workstations.$$$Experimental results are poised to show that architectures found by MONAS and DPP-Net achieves Pareto optimality w.r.t the given objectives for various devices.",BACKGROUND OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D03005,"In this paper, we propose a simple but effective method for training neural networks with a limited amount of training data.$$$Our approach inherits the idea of knowledge distillation that transfers knowledge from a deep or wide reference model to a shallow or narrow target model.$$$The proposed method employs this idea to mimic predictions of reference estimators that are more robust against overfitting than the network we want to train.$$$Different from almost all the previous work for knowledge distillation that requires a large amount of labeled training data, the proposed method requires only a small amount of training data.$$$Instead, we introduce pseudo training examples that are optimized as a part of model parameters.$$$Experimental results for several benchmark datasets demonstrate that the proposed method outperformed all the other baselines, such as naive training of the target model and standard knowledge distillation.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D05790,This paper presents a new approach for training artificial neural networks using techniques for solving the constraint satisfaction problem (CSP).$$$The quotient gradient system (QGS) is a trajectory-based method for solving the CSP.$$$This study converts the training set of a neural network into a CSP and uses the QGS to find its solutions.$$$The QGS finds the global minimum of the optimization problem by tracking trajectories of a nonlinear dynamical system and does not stop at a local minimum of the optimization problem.$$$Lyapunov theory is used to prove the asymptotic stability of the solutions with and without the presence of measurement errors.$$$Numerical examples illustrate the effectiveness of the proposed methodology and compare it to a genetic algorithm and error backpropagation.,OBJECTIVES BACKGROUND/METHODS METHODS METHODS RESULTS CONCLUSIONS
D02040,"We present a novel hierarchical graphical model based context-aware hybrid brain-machine interface (hBMI) using probabilistic fusion of electroencephalographic (EEG) and electromyographic (EMG) activities.$$$Based on experimental data collected during stationary executions and subsequent imageries of five different hand gestures with both limbs, we demonstrate feasibility of the proposed hBMI system through within session and online across sessions classification analyses.$$$Furthermore, we investigate the context-aware extent of the model by a simulated probabilistic approach and highlight potential implications of our work in the field of neurophysiologically-driven robotic hand prosthetics.",METHODS METHODS METHODS
D05362,"We evaluated the effectiveness of an automated bird sound identification system in a situation that emulates a realistic, typical application.$$$We trained classification algorithms on a crowd-sourced collection of bird audio recording data and restricted our training methods to be completely free of manual intervention.$$$The approach is hence directly applicable to the analysis of multiple species collections, with labelling provided by crowd-sourced collection.$$$We evaluated the performance of the bird sound recognition system on a realistic number of candidate classes, corresponding to real conditions.$$$We investigated the use of two canonical classification methods, chosen due to their widespread use and ease of interpretation, namely a k Nearest Neighbour (kNN) classifier with histogram-based features and a Support Vector Machine (SVM) with time-summarisation features.$$$We further investigated the use of a certainty measure, derived from the output probabilities of the classifiers, to enhance the interpretability and reliability of the class decisions.$$$Our results demonstrate that both identification methods achieved similar performance, but we argue that the use of the kNN classifier offers somewhat more flexibility.$$$Furthermore, we show that employing an outcome certainty measure provides a valuable and consistent indicator of the reliability of classification results.$$$Our use of generic training data and our investigation of probabilistic classification methodologies that can flexibly address the variable number of candidate species/classes that are expected to be encountered in the field, directly contribute to the development of a practical bird sound identification system with potentially global application.$$$Further, we show that certainty measures associated with identification outcomes can significantly contribute to the practical usability of the overall system.",BACKGROUND/OBJECTIVES METHODS OBJECTIVES BACKGROUND/OBJECTIVES METHODS METHODS RESULTS RESULTS/CONCLUSIONS BACKGROUND BACKGROUND/RESULTS/CONCLUSIONS
D05824,"Due to the increasing number of mobile robots including domestic robots for cleaning and maintenance in developed countries, human activity recognition is inevitable for congruent human-robot interaction.$$$Needless to say that this is indeed a challenging task for robots, it is expedient to learn human activities for autonomous mobile robots (AMR) for navigating in an uncontrolled environment without any guidance.$$$Building a correct classifier for complex human action is non-trivial since simple actions can be combined to recognize a complex human activity.$$$In this paper, we trained a model for human activity recognition using convolutional neural network.$$$We trained and validated the model using the Vicon physical action dataset and also tested the model on our generated dataset (VMCUHK).$$$Our experiment shows that our method performs with high accuracy, human activity recognition task both on the Vicon physical action dataset and VMCUHK dataset.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS CONCLUSIONS
D06801,"With the increasing usage of smartphones, there is a corresponding increase in the phone metadata generated by individuals using these devices.$$$Managing the privacy of personal information on these devices can be a complex task.$$$Recent research has suggested the use of social and behavioral data for automatically recommending privacy settings.$$$This paper is the first effort to connect users' phone use metadata with their privacy attitudes.$$$Based on a 10-week long field study involving phone metadata collection via an app, and a survey on privacy attitudes, we report that an analysis of cell phone metadata may reveal vital clues to a person's privacy attitudes.$$$Specifically, a predictive model based on phone usage metadata significantly outperforms a comparable personality features-based model in predicting individual privacy attitudes.$$$The results motivate a newer direction of automatically inferring a user's privacy attitudes by looking at their phone usage characteristics.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS CONCLUSIONS
D00296,"For extracting meaningful topics from texts, their structures should be considered properly.$$$In this paper, we aim to analyze structured time-series documents such as a collection of news articles and a series of scientific papers, wherein topics evolve along time depending on multiple topics in the past and are also related to each other at each time.$$$To this end, we propose a dynamic and static topic model, which simultaneously considers the dynamic structures of the temporal topic evolution and the static structures of the topic hierarchy at each time.$$$We show the results of experiments on collections of scientific papers, in which the proposed method outperformed conventional models.$$$Moreover, we show an example of extracted topic structures, which we found helpful for analyzing research activities.",BACKGROUND BACKGROUND/OBJECTIVES METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05783,"Micro-blogging services such as Twitter allow anyone to publish anything, anytime.$$$Needless to say, many of the available contents can be diminished as babble or spam.$$$However, given the number and diversity of users, some valuable pieces of information should arise from the stream of tweets.$$$Thus, such services can develop into valuable sources of up-to-date information (the so-called real-time web) provided a way to find the most relevant/trustworthy/authoritative users is available.$$$Hence, this makes a highly pertinent question for which graph centrality methods can provide an answer.$$$In this paper the author offers a comprehensive survey of feasible algorithms for ranking users in social networks, he examines their vulnerabilities to linking malpractice in such networks, and suggests an objective criterion against which to compare such algorithms.$$$Additionally, he suggests a first step towards ""desensitizing"" prestige algorithms against cheating by spammers and other abusive users.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS
D02978,"Assisted text input techniques can save time and effort and improve text quality.$$$In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion.$$$These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word.$$$Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models.$$$Our best system uses both conditioning and grounding, because of their orthogonal benefits.$$$For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%.$$$We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks.$$$We found that at test time numbers have more influence on the document level than on individual word probabilities.",BACKGROUND OBJECTIVES METHODS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D00985,"A key challenge in modern computing is to develop systems that address complex, dynamic problems in a scalable and efficient way, because the increasing complexity of software makes designing and maintaining efficient and flexible systems increasingly difficult.$$$Biological systems are thought to possess robust, scalable processing paradigms that can automatically manage complex, dynamic problem spaces, possessing several properties that may be useful in computer systems.$$$The biological properties of self-organisation, self-replication, self-management, and scalability are addressed in an interesting way by autopoiesis, a descriptive theory of the cell founded on the concept of a system's circular organisation to define its boundary with its environment.$$$In this paper, therefore, we review the main concepts of autopoiesis and then discuss how they could be related to fundamental concepts and theories of computation.$$$The paper is conceptual in nature and the emphasis is on the review of other people's work in this area as part of a longer-term strategy to develop a formal theory of autopoietic computing.",BACKGROUND BACKGROUND METHODS OBJECTIVES METHODS
D00764,"A group of transition probability functions form a Shannon's channel whereas a group of truth functions form a semantic channel.$$$By the third kind of Bayes' theorem, we can directly convert a Shannon's channel into an optimized semantic channel.$$$When a sample is not big enough, we can use a truth function with parameters to produce the likelihood function, then train the truth function by the conditional sampling distribution.$$$The third kind of Bayes' theorem is proved.$$$A semantic information theory is simply introduced.$$$The semantic information measure reflects Popper's hypothesis-testing thought.$$$The Semantic Information Method (SIM) adheres to maximum semantic information criterion which is compatible with maximum likelihood criterion and Regularized Least Squares criterion.$$$It supports Wittgenstein's view: the meaning of a word lies in its use.$$$Letting the two channels mutually match, we obtain the Channels' Matching (CM) algorithm for machine learning.$$$The CM algorithm is used to explain the evolution of the semantic meaning of natural language, such as ""Old age"".$$$The semantic channel for medical tests and the confirmation measures of test-positive and test-negative are discussed.$$$The applications of the CM algorithm to semi-supervised learning and non-supervised learning are simply introduced.$$$As a predictive model, the semantic channel fits variable sources and hence can overcome class-imbalance problem.$$$The SIM strictly distinguishes statistical probability and logical probability and uses both at the same time.$$$This method is compatible with the thoughts of Bayes, Fisher, Shannon, Zadeh, Tarski, Davidson, Wittgenstein, and Popper.It is a competitive alternative to Bayesian inference.",METHODS METHODS METHODS METHODS BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS RESULTS RESULTS METHODS BACKGROUND
D03095,"Human Activity Recognition (HAR) based on motion sensors has drawn a lot of attention over the last few years, since perceiving the human status enables context-aware applications to adapt their services on users' needs.$$$However, motion sensor fusion and feature extraction have not reached their full potentials, remaining still an open issue.$$$In this paper, we introduce PerceptionNet, a deep Convolutional Neural Network (CNN) that applies a late 2D convolution to multimodal time-series sensor data, in order to extract automatically efficient features for HAR.$$$We evaluate our approach on two public available HAR datasets to demonstrate that the proposed model fuses effectively multimodal sensors and improves the performance of HAR.$$$In particular, PerceptionNet surpasses the performance of state-of-the-art HAR methods based on: (i) features extracted from humans, (ii) deep CNNs exploiting early fusion approaches, and (iii) Long Short-Term Memory (LSTM), by an average accuracy of more than 3%.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS
D00087,"Objects may appear at arbitrary scales in perspective images of a scene, posing a challenge for recognition systems that process images at a fixed resolution.$$$We propose a depth-aware gating module that adaptively selects the pooling field size in a convolutional network architecture according to the object scale (inversely proportional to the depth) so that small details are preserved for distant objects while larger receptive fields are used for those nearby.$$$The depth gating signal is provided by stereo disparity or estimated directly from monocular input.$$$We integrate this depth-aware gating into a recurrent convolutional neural network to perform semantic segmentation.$$$Our recurrent module iteratively refines the segmentation results, leveraging the depth and semantic predictions from the previous iterations.$$$Through extensive experiments on four popular large-scale RGB-D datasets, we demonstrate this approach achieves competitive semantic segmentation performance with a model which is substantially more compact.$$$We carry out extensive analysis of this architecture including variants that operate on monocular RGB but use depth as side-information during training, unsupervised gating as a generic attentional mechanism, and multi-resolution gating.$$$We find that gated pooling for joint semantic segmentation and depth yields state-of-the-art results for quantitative monocular depth estimation.",BACKGROUND OBJECTIVES/METHODS METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D00221,"Mobile phone calling is one of the most widely used communication methods in modern society.$$$The records of calls among mobile phone users provide us a valuable proxy for the understanding of human communication patterns embedded in social networks.$$$Mobile phone users call each other forming a directed calling network.$$$If only reciprocal calls are considered, we obtain an undirected mutual calling network.$$$The preferential communication behavior between two connected users can be statistically tested and it results in two Bonferroni networks with statistically validated edges.$$$We perform a comparative analysis of the statistical properties of these four networks, which are constructed from the calling records of more than nine million individuals in Shanghai over a period of 110 days.$$$We find that these networks share many common structural properties and also exhibit idiosyncratic features when compared with previously studied large mobile calling networks.$$$The empirical findings provide us an intriguing picture of a representative large social network that might shed new lights on the modelling of large social networks.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND METHODS OBJECTIVES RESULTS CONCLUSIONS
D01934,"Skin lesion segmentation is one of the first steps towards automatic Computer-Aided Diagnosis of skin cancer.$$$Vast variety in the appearance of the skin lesion makes this task very challenging.$$$The contribution of this paper is to apply a power foreground extraction technique called GrabCut for automatic skin lesion segmentation with minimal human interaction in HSV color space.$$$Preprocessing was performed for removing the outer black border.$$$Jaccard Index was measured to evaluate the performance of the segmentation method.$$$On average, 0.71 Jaccard Index was achieved on 1000 images from ISIC challenge 2017 Training Dataset.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS RESULTS RESULTS
D02667,"We propose a new iterative segmentation model which can be accurately learned from a small dataset.$$$A common approach is to train a model to directly segment an image, requiring a large collection of manually annotated images to capture the anatomical variability in a cohort.$$$In contrast, we develop a segmentation model that recursively evolves a segmentation in several steps, and implement it as a recurrent neural network.$$$We learn model parameters by optimizing the interme- diate steps of the evolution in addition to the final segmentation.$$$To this end, we train our segmentation propagation model by presenting incom- plete and/or inaccurate input segmentations paired with a recommended next step.$$$Our work aims to alleviate challenges in segmenting heart structures from cardiac MRI for patients with congenital heart disease (CHD), which encompasses a range of morphological deformations and topological changes.$$$We demonstrate the advantages of this approach on a dataset of 20 images from CHD patients, learning a model that accurately segments individual heart chambers and great vessels.$$$Com- pared to direct segmentation, the iterative method yields more accurate segmentation for patients with the most severe CHD malformations.",OBJECTIVES BACKGROUND METHODS METHODS METHODS OBJECTIVES METHODS RESULTS
D06723,Relative worst-order analysis is a technique for assessing the relative quality of online algorithms.$$$We survey the most important results obtained with this technique and compare it with other quality measures.,BACKGROUND METHODS
D01507,"For future mmWave mobile communication systems the use of analog/hybrid beamforming is envisioned be a key as- pect.$$$The synthesis of beams is a key technology of enable the best possible operation during beamsearch, data transmission and MU MIMO operation.$$$The developed method for synthesizing beams is based on previous work in radar technology considering only phase array antennas.$$$With this technique it is possible to generate a desired beam of any shape with the constraints of the desired target transceiver antenna frontend.$$$It is not constraint to a certain antenna array geometry, but can handle 1d, 2d and even 3d antenna array geometries like cylindric arrays.$$$The numerical examples show that the method can synthesize beams by considering a user defined tradeoff between gain, transition width and passband ripples.",BACKGROUND BACKGROUND BACKGROUND RESULTS OBJECTIVES RESULTS
D06927,"Learning to optimize - the idea that we can learn from data algorithms that optimize a numerical criterion - has recently been at the heart of a growing number of research efforts.$$$One of the most challenging issues within this approach is to learn a policy that is able to optimize over classes of functions that are fairly different from the ones that it was trained on.$$$We propose a novel way of framing learning to optimize as a problem of learning a good navigation policy on a partially observable loss surface.$$$To this end, we develop Rover Descent, a solution that allows us to learn a fairly broad optimization policy from training on a small set of prototypical two-dimensional surfaces that encompasses the classically hard cases such as valleys, plateaus, cliffs and saddles and by using strictly zero-order information.$$$We show that, without having access to gradient or curvature information, we achieve state-of-the-art convergence speed on optimization problems not presented at training time such as the Rosenbrock function and other hard cases in two dimensions.$$$We extend our framework to optimize over high dimensional landscapes, while still handling only two-dimensional local landscape information and show good preliminary results.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D03264,"3D image processing constitutes nowadays a challenging topic in many scientific fields such as medicine, computational physics and informatics.$$$Therefore, development of suitable tools that guaranty a best treatment is a necessity.$$$Spherical shapes are a big class of 3D images whom processing necessitates adoptable tools.$$$This encourages researchers to develop spherical wavelets and spherical harmonics as special mathematical bases able for 3D spherical shapes.$$$The present work lies in the whole topic of 3D image processing with the special spherical harmonics bases.$$$A spherical harmonics based approach is proposed for the reconstruction of images provided with spherical harmonics Shannon-type entropy to evaluate the order/disorder of the reconstructed image.$$$Efficiency and accuracy of the approach is demonstrated by a simulation study on several spherical models.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D01096,"We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data.$$$In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise.$$$We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers.$$$We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data.$$$The theoretical development and both parametric and nonparametric algorithms proposed here constitutes an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.",OBJECTIVES BACKGROUND METHODS METHODS CONCLUSIONS
D06902,"The smart grid combines the classical power system with information technology leading to a cyber-physical system.$$$In such an environment the malicious injection of data has the potential to cause severe consequences.$$$Classical residual-based methods for bad data detection are unable to detect well designed false data injection (FDI) attacks.$$$Moreover, most work on FDI attack detection is based on the linearized DC model of the power system and fails to detect attacks based on the AC model.$$$The aim of this paper is to address these problems by using the graph structure of the grid and the AC power flow model.$$$We derive an attack detection method that has the ability to detect previously undetectable FDI attacks.$$$This method is based on concepts originating from graph signal processing (GSP).$$$The proposed detection scheme consists of calculating the graph Fourier transform of an estimated grid state and filtering the graph high-frequency components.$$$By comparing the maximum norm of this outcome with a threshold we can detect the presence of FDI attacks.$$$Case studies on the IEEE 14-bus system demonstrate that the proposed method is able to detect a wide range of previously undetectable attacks.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D04159,"Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently attracted much attention in natural language processing and information retrieval tasks.$$$The embedding vectors are typically learned based on term proximity in a large corpus.$$$This means that the objective in well-known word embedding algorithms, e.g., word2vec, is to accurately predict adjacent word(s) for a given word or context.$$$However, this objective is not necessarily equivalent to the goal of many information retrieval (IR) tasks.$$$The primary objective in various IR tasks is to capture relevance instead of term proximity, syntactic, or even semantic similarity.$$$This is the motivation for developing unsupervised relevance-based word embedding models that learn word representations based on query-document relevance information.$$$In this paper, we propose two learning models with different objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classifies each term as belonging to the relevant or non-relevant class for each query.$$$To train our models, we used over six million unique queries and the top ranked documents retrieved in response to each query, which are assumed to be relevant to the query.$$$We extrinsically evaluate our learned word representation models using two IR tasks: query expansion and query classification.$$$Both query expansion experiments on four TREC collections and query classification experiments on the KDD Cup 2005 dataset suggest that the relevance-based word embedding models significantly outperform state-of-the-art proximity-based embedding models, such as word2vec and GloVe.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS
D05469,"In this article we propose a method for measuring internet connection stability which is fast and has negligible overhead for the process of its complexity.$$$This method finds a relative value for representing the stability of internet connections and can also be extended for aggregated internet connections.$$$The method is documented with help of a real time implementation and results are shared.$$$This proposed measurement scheme uses HTTP GET method for each connections.$$$The normalized responses to identified sites like gateways of ISPs, google.com etc are used for calculating current link stability.$$$The novelty of the approach is that historic values are used to calculate overall link stability.$$$In this discussion, we also document a method to use the calculated values as a dynamic threshold metric.$$$This is used in routing decisions and for load-balancing each of the connections in an aggregated bandwidth pipe.$$$This scheme is a very popular practice in aggregated internet connections.",BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS OTHERS
D06690,"A software project has ""Hero Developers"" when 80% of contributions are delivered by 20% of the developers.$$$Are such heroes a good idea?$$$Are too many heroes bad for software quality?$$$Is it better to have more/less heroes for different kinds of projects?$$$To answer these questions, we studied 661 open source projects from Public open source software (OSS) Github and 171 projects from an Enterprise Github.$$$We find that hero projects are very common.$$$In fact, as projects grow in size, nearly all project become hero projects.$$$These findings motivated us to look more closely at the effects of heroes on software development.$$$Analysis shows that the frequency to close issues and bugs are not significantly affected by the presence of project type (Public or Enterprise).$$$Similarly, the time needed to resolve an issue/bug/enhancement is not affected by heroes or project type.$$$This is a surprising result since, before looking at the data, we expected that increasing heroes on a project will slow down howfast that project reacts to change.$$$However, we do find a statistically significant association between heroes, project types, and enhancement resolution rates.$$$Heroes do not affect enhancement resolution rates in Public projects.$$$However, in Enterprise projects, the more heroes increase the rate at which project complete enhancements.$$$In summary, our empirical results call for a revision of a long-held truism in software engineering.$$$Software heroes are far more common and valuable than suggested by the literature, particularly for medium to large Enterprise developments.$$$Organizations should reflect on better ways to find and retain more of these heroes",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS CONCLUSIONS CONCLUSIONS OTHERS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS BACKGROUND RESULTS CONCLUSIONS CONCLUSIONS OTHERS CONCLUSIONS CONCLUSIONS
D06944,"We investigate weak recognizability of deterministic languages of infinite trees.$$$We prove that for deterministic languages the Borel hierarchy and the weak index hierarchy coincide.$$$Furthermore, we propose a procedure computing for a deterministic automaton an equivalent minimal index weak automaton with a quadratic number of states.$$$The algorithm works within the time of solving the emptiness problem.",RESULTS RESULTS RESULTS RESULTS
D04277,"Existing approaches to protect the privacy of Electronic Health Records are either insufficient for existing medical laws or they are too restrictive in their usage.$$$For example, smart card-based encryption systems require the patient to be always present to authorize access to medical records.$$$Questionnaires were administered by 50 medical practitioners to identify and categorize different Electronic Health Records attributes.$$$The system was implemented using multi biometrics of patients to access patient record in pre-hospital care.The software development tools employed were JAVA and MySQL database.$$$The system provides applicable security when patients records are shared either with other practitioners, employers, organizations or research institutes.$$$The result of the system evaluation shows that the average response time of 6 seconds and 11.1 seconds for fingerprint and iris respectively after ten different simulations.$$$The system protects privacy and confidentiality by limiting the amount of data exposed to users.The system also enables emergency medical technicians to gain easy and reliable access to necessary attributes of patients Electronic Health Records while still maintaining the privacy and confidentiality of the data using the patients fingerprint and iris.",BACKGROUND BACKGROUND METHODS METHODS RESULTS RESULTS CONCLUSIONS
D05227,"How many copies of a parallelepiped are needed to ensure that for every point in the parallelepiped a copy of each other point exists, such that the distance between them equals the distance of the pair of points when the opposite sites of the parallelepiped are identified?$$$This question is answered in Euclidean space by constructing the smallest domain that fulfills the above condition.$$$We also describe how to obtain all primitive cells of a lattice (i.e., closures of fundamental domains) that realise the smallest number of copies needed and give them explicitly in 2D and 3D.",OBJECTIVES METHODS/RESULTS RESULTS
D00567,"We consider two-way amplify and forward relaying, where multiple full-duplex user pairs exchange information via a shared full-duplex massive multiple-input multiple-output (MIMO) relay.$$$We derive closed-form lower bound for the spectral efficiency with zero-forcing processing at the relay, by using minimum mean squared error channel estimation.$$$The zero-forcing lower bound for the system model considered herein, which is valid for arbitrary number of antennas, is not yet derived in the massive MIMO relaying literature.$$$We numerically demonstrate the accuracy of the derived lower bound and the performance improvement achieved using zero-forcing processing.$$$We also numerically demonstrate the spectral gains achieved by a full-duplex system over a half-duplex one for various antenna regimes.",BACKGROUND OBJECTIVES/METHODS BACKGROUND/OBJECTIVES RESULTS RESULTS/CONCLUSIONS
D01849,"Live fish recognition is one of the most crucial elements of fisheries survey applications where vast amount of data are rapidly acquired.$$$Different from general scenarios, challenges to underwater image recognition are posted by poor image quality, uncontrolled objects and environment, as well as difficulty in acquiring representative samples.$$$Also, most existing feature extraction techniques are hindered from automation due to involving human supervision.$$$Toward this end, we propose an underwater fish recognition framework that consists of a fully unsupervised feature learning technique and an error-resilient classifier.$$$Object parts are initialized based on saliency and relaxation labeling to match object parts correctly.$$$A non-rigid part model is then learned based on fitness, separation and discrimination criteria.$$$For the classifier, an unsupervised clustering approach generates a binary class hierarchy, where each node is a classifier.$$$To exploit information from ambiguous images, the notion of partial classification is introduced to assign coarse labels by optimizing the ""benefit"" of indecision made by the classifier.$$$Experiments show that the proposed framework achieves high accuracy on both public and self-collected underwater fish images with high uncertainty and class imbalance.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D04014,"The area of Handwritten Signature Verification has been broadly researched in the last decades, but remains an open research problem.$$$The objective of signature verification systems is to discriminate if a given signature is genuine (produced by the claimed individual), or a forgery (produced by an impostor).$$$This has demonstrated to be a challenging task, in particular in the offline (static) scenario, that uses images of scanned signatures, where the dynamic information about the signing process is not available.$$$Many advancements have been proposed in the literature in the last 5-10 years, most notably the application of Deep Learning methods to learn feature representations from signature images.$$$In this paper, we present how the problem has been handled in the past few decades, analyze the recent advancements in the field, and the potential directions for future research.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS/RESULTS/CONCLUSIONS
D00836,"Bimanual gestures are of the utmost importance for the study of motor coordination in humans and in everyday activities.$$$A reliable detection of bimanual gestures in unconstrained environments is fundamental for their clinical study and to assess common activities of daily living.$$$This paper investigates techniques for a reliable, unconstrained detection and classification of bimanual gestures.$$$It assumes the availability of inertial data originating from the two hands/arms, builds upon a previously developed technique for gesture modelling based on Gaussian Mixture Modelling (GMM) and Gaussian Mixture Regression (GMR), and compares different modelling and classification techniques, which are based on a number of assumptions inspired by literature about how bimanual gestures are represented and modelled in the brain.$$$Experiments show results related to 5 everyday bimanual activities, which have been selected on the basis of three main parameters: (not) constraining the two hands by a physical tool, (not) requiring a specific sequence of single-hand gestures, being recursive (or not).$$$In the best performing combination of modeling approach and classification technique, five out of five activities are recognized up to an accuracy of 97%, a precision of 82% and a level of recall of 100%.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS RESULTS
D01925,"In this work, we analyze the performance of the uplink (UL) of a massive MIMO network considering an asymptotically large number of antennas at base stations (BSs).$$$We model the locations of BSs as a homogeneous Poisson point process (PPP) and assume that their service regions are limited to their respective Poisson-Voronoi cells (PVCs).$$$Further, for each PVC, based on a threshold radius, we model the cell center (CC) region as the Johnson-Mehl (JM) cell of its BS while rest of the PVC is deemed as the cell edge (CE) region.$$$The CC and CE users are located uniformly at random independently of each other in the JM cell and CE region, respectively.$$$In addition, we consider a fractional pilot reuse (FPR) scheme where two different sets of pilot sequences are used for CC and CE users with the objective of reducing the interference due to pilot contamination for CE users.$$$Based on the above system model, we derive analytical expressions for the UL signal-to-interference-and-noise ratio (SINR) coverage probability and average spectral efficiency (SE) for randomly selected CC and CE users.$$$In addition, we present an approximate expression for the average cell SE.$$$One of the key intermediate results in our analysis is the approximate but accurate characterization of the distributions of the CC and CE areas of a typical cell.$$$Another key intermediate step is the accurate characterization of the pair correlation functions of the point processes formed by the interfering CC and CE users that subsequently enables the coverage probability analysis.$$$From our system analysis, we present a partitioning rule for the number of pilot sequences to be used for CC and CE users as a function of threshold radius that improves the average CE user SE while achieving similar CC user SE with respect to unity pilot reuse.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS RESULTS CONCLUSIONS
D00023,"In the last decade, social media has evolved as one of the leading platform to create, share, or exchange information; it is commonly used as a way for individuals to maintain social connections.$$$In this online digital world, people use to post texts or pictures to express their views socially and create user-user engagement through discussions and conversations.$$$Thus, social media has established itself to bear signals relating to human behavior.$$$One can easily design user characteristic network by scraping through someone's social media profiles.$$$In this paper, we investigate the potential of social media in characterizing and understanding predominant drunk texters from the perspective of their social, psychological and linguistic behavior as evident from the content generated by them.$$$Our research aims to analyze the behavior of drunk texters on social media and to contrast this with non-drunk texters.$$$We use Twitter social media to obtain the set of drunk texters and non-drunk texters and show that we can classify users into these two respective sets using various psycholinguistic features with an overall average accuracy of 96.78% with very high precision and recall.$$$Note that such an automatic classification can have far-reaching impact - (i) on health research related to addiction prevention and control, and (ii) in eliminating abusive and vulgar contents from Twitter, borne by the tweets of drunk texters.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS/RESULTS/CONCLUSIONS OTHERS
D04251,"The Efficient Global Optimization (EGO) algorithm uses a conditional Gaus-sian Process (GP) to approximate an objective function known at a finite number of observation points and sequentially adds new points which maximize the Expected Improvement criterion according to the GP.$$$The important factor that controls the efficiency of EGO is the GP covariance function (or kernel) which should be chosen according to the objective function.$$$Traditionally, a pa-rameterized family of covariance functions is considered whose parameters are learned through statistical procedures such as maximum likelihood or cross-validation.$$$However, it may be questioned whether statistical procedures for learning covariance functions are the most efficient for optimization as they target a global agreement between the GP and the observations which is not the ultimate goal of optimization.$$$Furthermore, statistical learning procedures are computationally expensive.$$$The main alternative to the statistical learning of the GP is self-adaptation, where the algorithm tunes the kernel parameters based on their contribution to objective function improvement.$$$After questioning the possibility of self-adaptation for kriging based optimizers, this paper proposes a novel approach for tuning the length-scale of the GP in EGO: At each iteration, a small ensemble of kriging models structured by their length-scales is created.$$$All of the models contribute to an iterate in an EGO-like fashion.$$$Then, the set of models is densified around the model whose length-scale yielded the best iterate and further points are produced.$$$Numerical experiments are provided which motivate the use of many length-scales.$$$The tested implementation does not perform better than the classical EGO algorithm in a sequential context but show the potential of the approach for parallel implementations.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS RESULTS
D00942,"Researchers are increasingly incorporating numeric high-order data, i.e., numeric tensors, within their practice.$$$Just like the matrix/vector (MV) paradigm, the development of multi-purpose, but high-performance, sparse data structures and algorithms for arithmetic calculations, e.g., those found in Einstein-like notation, is crucial for the continued adoption of tensors.$$$We use the example of high-order differential operators to illustrate this need.$$$As sparse tensor arithmetic is an emerging research topic, with challenges distinct from the MV paradigm, many aspects require further articulation.$$$We focus on three core facets.$$$First, aligning with prominent voices in the field, we emphasise the importance of data structures able to accommodate the operational complexity of tensor arithmetic.$$$However, we describe a linearised coordinate (LCO) data structure that provides faster and more memory-efficient sorting performance.$$$Second, flexible data structures, like the LCO, rely heavily on sorts and permutations.$$$We introduce an innovative permutation algorithm, based on radix sort, that is tailored to rearrange already-sorted sparse data, producing significant performance gains.$$$Third, we introduce a novel poly-algorithm for sparse tensor products, where hyper-sparsity is a possibility.$$$Different manifestations of hyper-sparsity demand their own approach, which our poly-algorithm is the first to provide.$$$These developments are incorporated within our LibNT and NTToolbox software libraries.$$$Benchmarks, frequently drawn from the high-order differential operators example, demonstrate the practical impact of our routines, with speed-ups of 40% or higher compared to alternative high-performance implementations.$$$Comparisons against the MATLAB Tensor Toolbox show over 10 times speed improvements.$$$Thus, these advancements produce significant practical improvements for sparse tensor arithmetic.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D04149,"Automatic note-level transcription is considered one of the most challenging tasks in music information retrieval.$$$The specific case of flamenco singing transcription poses a particular challenge due to its complex melodic progressions, intonation inaccuracies, the use of a high degree of ornamentation and the presence of guitar accompaniment.$$$In this study, we explore the limitations of existing state of the art transcription systems for the case of flamenco singing and propose a specific solution for this genre: We first extract the predominant melody and apply a novel contour filtering process to eliminate segments of the pitch contour which originate from the guitar accompaniment.$$$We formulate a set of onset detection functions based on volume and pitch characteristics to segment the resulting vocal pitch contour into discrete note events.$$$A quantised pitch label is assigned to each note event by combining global pitch class probabilities with local pitch contour statistics.$$$The proposed system outperforms state of the art singing transcription systems with respect to voicing accuracy, onset detection and overall performance when evaluated on flamenco singing datasets.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D04521,"The rising popularity of social media has radically changed the way news content is propagated, including interactive attempts with new dimensions.$$$To date, traditional news media such as newspapers, television and radio have already adapted their activities to the online news media by utilizing social media, blogs, websites etc.$$$This paper provides some insight into the social media presence of worldwide popular news media outlets.$$$Despite the fact that these large news media propagate content via social media environments to a large extent and very little is known about the news item producers, providers and consumers in the news media community in social media.To better understand these interactions, this work aims to analyze news items in two large social media, Twitter and Facebook.$$$Towards that end, we collected all published posts on Twitter and Facebook from 48 news media to perform descriptive and predictive analyses using the dataset of 152K tweets and 80K Facebook posts.$$$We explored a set of news media that originate content by themselves in social media, those who distribute their news items to other news media and those who consume news content from other news media and/or share replicas.$$$We propose a predictive model to increase news media popularity among readers based on the number of posts, number of followers and number of interactions performed within the news media community.$$$The results manifested that, news media should disperse their own content and they should publish first in social media in order to become a popular news media and receive more attractions to their news items from news readers.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS/RESULTS RESULTS CONCLUSIONS
D01148,"We develop a thermodynamic framework for modeling nonlinear ultrasonic damage sensing and prognosis in materials undergoing progressive damage.$$$The framework is based on the internal variable approach and relies on the construction of a pseudo-elastic strain energy function that captures the energetics associated with the damage progression.$$$The pseudo-elastic strain energy function is composed of two energy functions - one that describes how a material stores energy in an elastic fashion and the other describes how material dissipates energy or stores it in an inelastic fashion.$$$Experimental motivation for the choice of the above two functionals is discussed and some specific choices pertaining to damage progression during fatigue and creep are presented.$$$The thermodynamic framework is employed to model the nonlinear response of material undergoing stress relaxation and creep-like degradation.$$$For each of the above cases, evolution of the nonlinearity parameter with damage as well as with macroscopic measurables like accumulated plastic strain are obtained.",RESULTS METHODS METHODS BACKGROUND RESULTS RESULTS
D06402,"Femtocells have been considered by the wireless industry as a cost-effective solution not only to improve indoor service providing, but also to unload traffic from already overburdened macro networks.$$$Due to spectrum availability and network infrastructure considerations, a macro network may have to share spectrum with overlaid femtocells.$$$In spectrum-sharing macro and femto networks, inter-cell interference caused by different transmission powers of macrocell base stations (MBS) and femtocell access points (FAP), in conjunction with potentially densely deployed femtocells, may create dead spots where reliable services cannot be guaranteed to either macro or femto users.$$$In this paper, based on a thorough analysis of downlink (DL) outage probabilities (OP) of collocated spectrum-sharing orthogonal frequency division multiple access (OFDMA) based macro and femto networks, we devise a decentralized strategy for an FAP to self-regulate its transmission power level and usage of radio resources depending on its distance from the closest MBS.$$$Simulation results show that the derived closed-form lower bounds of DL OPs are tight, and the proposed decentralized femtocell self-regulation strategy is able to guarantee reliable DL services in targeted macro and femto service areas while providing superior spatial reuse, for even a large number of spectrum-sharing femtocells deployed per cell site.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D05368,"Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms.$$$Current systems, however, use simple generalized heuristics and ignore workload characteristics, since developing and tuning a scheduling policy for each workload is infeasible.$$$In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically.$$$Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond a high-level objective such as minimizing average job completion time.$$$Off-the-shelf RL techniques, however, cannot handle the complexity and scale of the scheduling problem.$$$To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent RL training methods for dealing with continuous stochastic job arrivals.$$$Our prototype integration with Spark on a 25-node cluster shows that Decima improves the average job completion time over hand-tuned scheduling heuristics by at least 21%, achieving up to 2x improvement during periods of high cluster load.",BACKGROUND BACKGROUND OBJECTIVES/CONCLUSIONS METHODS/RESULTS BACKGROUND/METHODS METHODS RESULTS
D03443,"In this work, we consider diffusion-based molecular communication with and without drift between two static nano-machines.$$$We employ type-based information encoding, releasing a single molecule per information bit.$$$At the receiver, we consider an asynchronous detection algorithm which exploits the arrival order of the molecules.$$$In such systems, transposition errors fundamentally undermine reliability and capacity.$$$Thus, in this work we study the impact of transpositions on the system performance.$$$Towards this, we present an analytical expression for the exact bit error probability (BEP) caused by transpositions and derive computationally tractable approximations of the BEP for diffusion-based channels with and without drift.$$$Based on these results, we analyze the BEP when background is not negligible and derive the optimal bit interval that minimizes the BEP.$$$Simulation results confirm the theoretical results and show the error and goodput performance for different parameters such as block size or noise generation rate.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D03824,"Selecting the most appropriate data examples to present a deep neural network (DNN) at different stages of training is an unsolved challenge.$$$Though practitioners typically ignore this problem, a non-trivial data scheduling method may result in a significant improvement in both convergence and generalization performance.$$$In this paper, we introduce Self-Paced Learning with Adaptive Deep Visual Embeddings (SPL-ADVisE), a novel end-to-end training protocol that unites self-paced learning (SPL) and deep metric learning (DML).$$$We leverage the Magnet Loss to train an embedding convolutional neural network (CNN) to learn a salient representation space.$$$The student CNN classifier dynamically selects similar instance-level training examples to form a mini-batch, where the easiness from the cross-entropy loss and the true diverseness of examples from the learned metric space serve as sample importance priors.$$$To demonstrate the effectiveness of SPL-ADVisE, we use deep CNN architectures for the task of supervised image classification on several coarse- and fine-grained visual recognition datasets.$$$Results show that, across all datasets, the proposed method converges faster and reaches a higher final accuracy than other SPL variants, particularly on fine-grained classes.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D05564,"Generalized linear models (GLMs) arise in high-dimensional machine learning, statistics, communications and signal processing.$$$In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes or benchmark models in neural networks.$$$We evaluate the mutual information (or ""free entropy"") from which we deduce the Bayes-optimal estimation and generalization errors.$$$Our analysis applies to the high-dimensional limit where both the number of samples and the dimension are large and their ratio is fixed.$$$Non-rigorous predictions for the optimal errors existed for special cases of GLMs, e.g. for the perceptron, in the field of statistical physics based on the so-called replica method.$$$Our present paper rigorously establishes those decades old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm.$$$Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance, and locate the associated sharp phase transitions separating learnable and non-learnable regions.$$$We believe that this random version of GLMs can serve as a challenging benchmark for multi-purpose algorithms.$$$This paper is divided in two parts that can be read independently: The first part (main part) presents the model and main results, discusses some applications and sketches the main ideas of the proof.$$$The second part (supplementary informations) is much more detailed and provides more examples as well as all the proofs.",BACKGROUND OBJECTIVES OBJECTIVES/RESULTS OBJECTIVES BACKGROUND OBJECTIVES/RESULTS OBJECTIVES/RESULTS OBJECTIVES OTHERS OTHERS
D04621,"Domain adaptation (DA) aims to generalize a learning model across training and testing data despite the mismatch of their data distributions.$$$In light of a theoretical estimation of upper error bound, we argue in this paper that an effective DA method should 1) search a shared feature subspace where source and target data are not only aligned in terms of distributions as most state of the art DA methods do, but also discriminative in that instances of different classes are well separated; 2) account for the geometric structure of the underlying data manifold when inferring data labels on the target domain.$$$In comparison with a baseline DA method which only cares about data distribution alignment between source and target, we derive three different DA models, namely CDDA, GA-DA, and DGA-DA, to highlight the contribution of Close yet Discriminative DA(CDDA) based on 1), Geometry Aware DA (GA-DA) based on 2), and finally Discriminative and Geometry Aware DA (DGA-DA) implementing jointly 1) and 2).$$$Using both synthetic and real data, we show the effectiveness of the proposed approach which consistently outperforms state of the art DA methods over 36 image classification DA tasks through 6 popular benchmarks.$$$We further carry out in-depth analysis of the proposed DA method in quantifying the contribution of each term of our DA model and provide insights into the proposed DA methods in visualizing both real and synthetic data.",BACKGROUND OBJECTIVES METHODS RESULTS/CONCLUSIONS CONCLUSIONS
D03460,"With the rapid increasing of software project size and maintenance cost, adherence to coding standards especially by managing identifier naming, is attracting a pressing concern from both computer science educators and software managers.$$$Software developers mainly use identifier names to represent the knowledge recorded in source code.$$$However, the popularity and adoption consistency of identifier naming conventions have not been revealed yet in this field.$$$Taking forty-eight popular open source projects written in three top-ranking programming languages Java, C and C++ as examples, an identifier extraction tool based on regular expression matching is developed.$$$In the subsequent investigation, some interesting findings are obtained.$$$For the identifier naming popularity, it is found that Camel and Pascal naming conventions are leading the road while Hungarian notation is vanishing.$$$For the identifier naming consistency, we have found that the projects written in Java have a much better performance than those written in C and C++.$$$Finally, academia and software industry are urged to adopt the most popular naming conventions consistently in their practices so as to lead the identifier naming to a standard, unified and high-quality road.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS RESULTS CONCLUSIONS
D06569,"HIV/AIDS spread depends upon complex patterns of interaction among various sub-sets emerging at population level.$$$This added complexity makes it difficult to study and model AIDS and its dynamics.$$$AIDS is therefore a natural candidate to be modeled using agent-based modeling, a paradigm well-known for modeling Complex Adaptive Systems (CAS).$$$While agent-based models are also well-known to effectively model CAS, often times models can tend to be ambiguous and the use of purely text-based specifications (such as ODD) can make models difficult to be replicated.$$$Previous work has shown how formal specification may be used in conjunction with agent-based modeling to develop models of various CAS.$$$However, to the best of our knowledge, no such model has been developed in conjunction with AIDS.$$$In this paper, we present a Formal Agent-Based Simulation modeling framework (FABS-AIDS) for an AIDS-based CAS.$$$FABS-AIDS employs the use of a formal specification model in conjunction with an agent-based model to reduce ambiguity as well as improve clarity in the model definition.$$$The proposed model demonstrates the effectiveness of using formal specification in conjunction with agent-based simulation for developing models of CAS in general and, social network-based agent-based models, in particular.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D05694,"We present three major transitions that occur on the way to the elaborate and diverse societies of the modern era.$$$Our account links the worlds of social animals such as pigtail macaques and monk parakeets to examples from human history, including 18th Century London and the contemporary online phenomenon of Wikipedia.$$$From the first awareness and use of group-level social facts to the emergence of norms and their self-assembly into normative bundles, each transition represents a new relationship between the individual and the group.$$$At the center of this relationship is the use of coarse-grained information gained via lossy compression.$$$The role of top-down causation in the origin of society parallels that conjectured to occur in the origin and evolution of life itself.",OBJECTIVES OBJECTIVES RESULTS RESULTS CONCLUSIONS
D00148,"In this paper a secret message/image transmission technique has been proposed through (2, 2) visual cryptographic share which is non-interpretable in general.$$$A binary image is taken as cover image and authenticating message/image has been fabricated into it through a hash function where two bits in each pixel within four bits from LSB of the pixel is embedded and as a result it converts the binary image to gray scale one.$$$(2,2) visual cryptographic shares are generated from this converted gray scale image.$$$During decoding shares are combined to regenerate the authenticated image from where the secret message/image is obtained through the same hash function along with reduction of noise.$$$Noise reduction is also done on regenerated authenticated image to regenerate original cover image at destination.",BACKGROUND/OBJECTIVES METHODS METHODS METHODS METHODS
D04461,"Near-miss experiences are one of the main sources of intense emotions.$$$Despite people's consistency when judging near-miss situations and when communicating about them, there is no integrated theoretical account of the phenomenon.$$$In particular, individuals' reaction to near-miss situations is not correctly predicted by rationality-based or probability-based optimization.$$$The present study suggests that emotional intensity in the case of near-miss is in part predicted by Simplicity Theory.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES
D05259,"In this paper, we present a consensus-based framework for decentralized estimation of deterministic parameters in wireless sensor networks (WSNs).$$$In particular, we propose an optimization algorithm to design (possibly complex) sensor gains in order to achieve an estimate of the parameter of interest that is as accurate as possible.$$$The proposed design algorithm employs a cyclic approach capable of handling various sensor gain constraints.$$$In addition, each iteration of the proposed design framework is comprised of the Gram-Schmidt process and power-method like iterations, and as a result, enjoys a low-computational cost.",OBJECTIVES OBJECTIVES/METHODS METHODS METHODS
D00028,"Software quality in use comprises quality from the user's perspective.$$$It has gained its importance in e-government applications, mobile-based applications, embedded systems, and even business process development.$$$User's decisions on software acquisitions are often ad hoc or based on preference due to difficulty in quantitatively measuring software quality in use.$$$But, why is quality-in-use measurement difficult?$$$Although there are many software quality models, to the authors' knowledge no works survey the challenges related to software quality-in-use measurement.$$$This article has two main contributions: 1) it identifies and explains major issues and challenges in measuring software quality in use in the context of the ISO SQuaRE series and related software quality models and highlights open research areas; and 2) it sheds light on a research direction that can be used to predict software quality in use.$$$In short, the quality-in-use measurement issues are related to the complexity of the current standard models and the limitations and incompleteness of the customized software quality models.$$$A sentiment analysis of software reviews is proposed to deal with these issues.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES RESULTS RESULTS/CONCLUSIONS
D00625,"Most network-based speech recognition methods are based on the assumption that the labels of two adjacent speech samples in the network are likely to be the same.$$$However, assuming the pairwise relationship between speech samples is not complete.$$$The information a group of speech samples that show very similar patterns and tend to have similar labels is missed.$$$The natural way overcoming the information loss of the above assumption is to represent the feature data of speech samples as the hypergraph.$$$Thus, in this paper, the three un-normalized, random walk, and symmetric normalized hypergraph Laplacian based semi-supervised learning methods applied to hypergraph constructed from the feature data of speech samples in order to predict the labels of speech samples are introduced.$$$Experiment results show that the sensitivity performance measures of these three hypergraph Laplacian based semi-supervised learning methods are greater than the sensitivity performance measures of the Hidden Markov Model method (the current state of the art method applied to speech recognition problem) and graph based semi-supervised learning methods (i.e. the current state of the art network-based method for classification problems) applied to network created from the feature data of speech samples.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES CONCLUSIONS
D00823,"A good measure of similarity between data points is crucial to many tasks in machine learning.$$$Similarity and metric learning methods learn such measures automatically from data, but they do not scale well respect to the dimensionality of the data.$$$In this paper, we propose a method that can learn efficiently similarity measure from high-dimensional sparse data.$$$The core idea is to parameterize the similarity measure as a convex combination of rank-one matrices with specific sparsity structures.$$$The parameters are then optimized with an approximate Frank-Wolfe procedure to maximally satisfy relative similarity constraints on the training data.$$$Our algorithm greedily incorporates one pair of features at a time into the similarity measure, providing an efficient way to control the number of active features and thus reduce overfitting.$$$It enjoys very appealing convergence guarantees and its time and memory complexity depends on the sparsity of the data instead of the dimension of the feature space.$$$Our experiments on real-world high-dimensional datasets demonstrate its potential for classification, dimensionality reduction and data exploration.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS/RESULTS RESULTS RESULTS
D02281,"The overwhelming amount and rate of information update in online social media is making it increasingly difficult for users to allocate their attention to their topics of interest, thus there is a strong need for prioritizing news feeds.$$$The attractiveness of a post to a user depends on many complex contextual and temporal features of the post.$$$For instance, the contents of the post, the responsiveness of a third user, and the age of the post may all have impact.$$$So far, these static and dynamic features has not been incorporated in a unified framework to tackle the post prioritization problem.$$$In this paper, we propose a novel approach for prioritizing posts based on a feature modulated multi-dimensional point process.$$$Our model is able to simultaneously capture textual and sentiment features, and temporal features such as self-excitation, mutual-excitation and bursty nature of social interaction.$$$As an evaluation, we also curated a real-world conversational benchmark dataset crawled from Facebook.$$$In our experiments, we demonstrate that our algorithm is able to achieve the-state-of-the-art performance in terms of analyzing, predicting, and prioritizing events.$$$In terms of interpretability of our method, we observe that features indicating individual user profile and linguistic characteristics of the events work best for prediction and prioritization of new events.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D01234,"A few decades of work in the AI field have focused efforts on developing a new generation of systems which can acquire knowledge via interaction with the world.$$$Yet, until very recently, most such attempts were underpinned by research which predominantly regarded linguistic phenomena as separated from the brain and body.$$$This could lead one into believing that to emulate linguistic behaviour, it suffices to develop 'software' operating on abstract representations that will work on any computational machine.$$$This picture is inaccurate for several reasons, which are elucidated in this paper and extend beyond sensorimotor and semantic resonance.$$$Beginning with a review of research, I list several heterogeneous arguments against disembodied language, in an attempt to draw conclusions for developing embodied multisensory agents which communicate verbally and non-verbally with their environment.$$$Without taking into account both the architecture of the human brain, and embodiment, it is unrealistic to replicate accurately the processes which take place during language acquisition, comprehension, production, or during non-linguistic actions.$$$While robots are far from isomorphic with humans, they could benefit from strengthened associative connections in the optimization of their processes and their reactivity and sensitivity to environmental stimuli, and in situated human-machine interaction.$$$The concept of multisensory integration should be extended to cover linguistic input and the complementary information combined from temporally coincident sensory impressions.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS OBJECTIVES RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05172,"DynamicGEM is an open-source Python library for learning node representations of dynamic graphs.$$$It consists of state-of-the-art algorithms for defining embeddings of nodes whose connections evolve over time.$$$The library also contains the evaluation framework for four downstream tasks on the network: graph reconstruction, static and temporal link prediction, node classification, and temporal visualization.$$$We have implemented various metrics to evaluate the state-of-the-art methods, and examples of evolving networks from various domains.$$$We have easy-to-use functions to call and evaluate the methods and have extensive usage documentation.$$$Furthermore, DynamicGEM provides a template to add new algorithms with ease to facilitate further research on the topic.",OBJECTIVES OBJECTIVES/METHODS METHODS METHODS OBJECTIVES RESULTS
D00259,"We address the issue of incorporating a particular yet expressive form of integrity constraints (namely, denial constraints) into probabilistic databases.$$$To this aim, we move away from the common way of giving semantics to probabilistic databases, which relies on considering a unique interpretation of the data, and address two fundamental problems: consistency checking and query evaluation.$$$The former consists in verifying whether there is an interpretation which conforms to both the marginal probabilities of the tuples and the integrity constraints.$$$The latter is the problem of answering queries under a ""cautious"" paradigm, taking into account all interpretations of the data in accordance with the constraints.$$$In this setting, we investigate the complexity of the above-mentioned problems, and identify several tractable cases of practical relevance.",OBJECTIVES BACKGROUND METHODS METHODS RESULTS
D06201,"Deep Learning methods employ multiple processing layers to learn hierarchial representations of data.$$$They have already been deployed in a humongous number of applications and have produced state-of-the-art results.$$$Recently with the growth in processing power of computers to be able to do high dimensional tensor calculations, Natural Language Processing (NLP) applications have been given a significant boost in terms of efficiency as well as accuracy.$$$In this paper, we will take a look at various signal processing techniques and then application of them to produce a speech-to-text system using Deep Recurrent Neural Networks.",BACKGROUND/METHODS BACKGROUND METHODS OBJECTIVES
D04844,"On a constant quest for inspiration, designers can become more effective with tools that facilitate their creative process and let them overcome design fixation.$$$This paper explores the practicality of applying neural style transfer as an emerging design tool for generating creative digital content.$$$To this aim, the present work explores a well-documented neural style transfer algorithm (Johnson 2016) in four experiments on four relevant visual parameters: number of iterations, learning rate, total variation, content vs. style weight.$$$The results allow a pragmatic recommendation of parameter configuration (number of iterations: 200 to 300, learning rate: 2e-1 to 4e-1, total variation: 1e-4 to 1e-8, content weights vs. style weights: 50:100 to 200:100) that saves extensive experimentation time and lowers the technical entry barrier.$$$With this rule-of-thumb insight, visual designers can effectively apply deep learning to create artistic visual variations of digital content.$$$This could enable designers to leverage AI for creating design works as state-of-the-art.",BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS CONCLUSIONS
D06596,"Modern large-scale computing deployments consist of complex applications running over machine clusters.$$$An important issue in these is the offering of elasticity, i.e., the dynamic allocation of resources to applications to meet fluctuating workload demands.$$$Threshold based approaches are typically employed, yet they are difficult to configure and optimize.$$$Approaches based on reinforcement learning have been proposed, but they require a large number of states in order to model complex application behavior.$$$Methods that adaptively partition the state space have been proposed, but their partitioning criteria and strategies are sub-optimal.$$$In this work we present MDP_DT, a novel full-model based reinforcement learning algorithm for elastic resource management that employs adaptive state space partitioning.$$$We propose two novel statistical criteria and three strategies and we experimentally prove that they correctly decide both where and when to partition, outperforming existing approaches.$$$We experimentally evaluate MDP_DT in a real large scale cluster over variable not-encountered workloads and we show that it takes more informed decisions compared to static and model-free approaches, while requiring a minimal amount of training data.",BACKGROUND OBJECTIVES BACKGROUND BACKGROUND BACKGROUND METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D06486,"End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases.$$$In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue.$$$Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network.$$$We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories.$$$In addition, our model is quite general without complicated task-specific designs.$$$As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D01258,"Hierarchical Classification (HC) is a supervised learning problem where unlabeled instances are classified into a taxonomy of classes.$$$Several methods that utilize the hierarchical structure have been developed to improve the HC performance.$$$However, in most cases apriori defined hierarchical structure by domain experts is inconsistent; as a consequence performance improvement is not noticeable in comparison to flat classification methods.$$$We propose a scalable data-driven filter based rewiring approach to modify an expert-defined hierarchy.$$$Experimental comparisons of top-down HC with our modified hierarchy, on a wide range of datasets shows classification performance improvement over the baseline hierarchy (i:e:, defined by expert), clustered hierarchy and flattening based hierarchy modification approaches.$$$In comparison to existing rewiring approaches, our developed method (rewHier) is computationally efficient, enabling it to scale to datasets with large numbers of classes, instances and features.$$$We also show that our modified hierarchy leads to improved classification performance for classes with few training samples in comparison to flat and state-of-the-art HC approaches.",OBJECTIVES BACKGROUND BACKGROUND METHODS RESULTS RESULTS CONCLUSIONS
D06141,"We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images.$$$Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image.$$$We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations.$$$Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics.$$$We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.",OBJECTIVES METHODS METHODS METHODS RESULTS
D01688,"The concept of the augmented coaching ecosystem for non-obtrusive adaptive personalized elderly care is proposed on the basis of the integration of new and available ICT approaches.$$$They include the multimodal user interface (MMUI), augmented reality (AR), machine learning (ML), Internet of Things (IoT), and machine-to-machine (M2M) interactions.$$$The ecosystem is based on the Cloud-Fog-Dew computing paradigm services, providing a full symbiosis by integrating the whole range from low-level sensors up to high-level services using integration efficiency inherent in synergistic use of applied technologies.$$$Inside of this ecosystem, all of them are encapsulated in the following network layers: Dew, Fog, and Cloud computing layer.$$$Instead of the ""spaghetti connections"", ""mosaic of buttons"", ""puzzles of output data"", etc., the proposed ecosystem provides the strict division in the following dataflow channels: consumer interaction channel, machine interaction channel, and caregiver interaction channel.$$$This concept allows to decrease the physical, cognitive, and mental load on elderly care stakeholders by decreasing the secondary human-to-human (H2H), human-to-machine (H2M), and machine-to-human (M2H) interactions in favor of M2M interactions and distributed Dew Computing services environment.$$$It allows to apply this non-obtrusive augmented reality ecosystem for effective personalized elderly care to preserve their physical, cognitive, mental and social well-being.",OBJECTIVES METHODS METHODS METHODS/RESULTS RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D04625,"Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve.$$$Yet, current neural machine translation training focuses on expensive human-generated reference translations.$$$We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback.$$$Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoder-decoder architecture (Luong et al., 2015).$$$This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS
D04255,Machine learning is used to compute achievable information rates (AIRs) for a simplified fiber channel.$$$The approach jointly optimizes the input distribution (constellation shaping) and the auxiliary channel distribution to compute AIRs without explicit channel knowledge in an end-to-end fashion.,OBJECTIVES/METHODS CONCLUSIONS
D04577,"In order to improve the performances of recently-presented improved normalized subband adaptive filter (INSAF) and proportionate INSAF algorithms for highly noisy system, this paper proposes their set-membership versions by exploiting the theory of set-membership filtering.$$$Apart from obtaining smaller steady-state error, the proposed algorithms significantly reduce the overall computational complexity.$$$In addition, to further improve the steady-state performance for the algorithms, their smooth variants are developed by using the smoothed absolute subband output errors to update the step sizes.$$$Simulation results in the context of acoustic echo cancellation have demonstrated the superiority of the proposed algorithms.",BACKGROUND/METHODS RESULTS METHODS RESULTS
D03769,"Due to the severe multipath effect, no satisfactory device-free methods have ever been found for indoor speed estimation problem, especially in non-line-of-sight scenarios, where the direct path between the source and observer is blocked.$$$In this paper, we present WiSpeed, a universal low-complexity indoor speed estimation system leveraging radio signals, such as commercial WiFi, LTE, 5G, etc., which can work in both device-free and device-based situations.$$$By exploiting the statistical theory of electromagnetic waves, we establish a link between the autocorrelation function of the physical layer channel state information and the speed of a moving object, which lays the foundation of WiSpeed.$$$WiSpeed differs from the other schemes requiring strong line-of-sight conditions between the source and observer in that it embraces the rich-scattering environment typical for indoors to facilitate highly accurate speed estimation.$$$Moreover, as a calibration-free system, WiSpeed saves the users' efforts from large-scale training and fine-tuning of system parameters.$$$In addition, WiSpeed could extract the stride length as well as detect abnormal activities such as falling down, a major threat to seniors that leads to a large number of fatalities every year.$$$Extensive experiments show that WiSpeed achieves a mean absolute percentage error of 4.85% for device-free human walking speed estimation and 4.62% for device-based speed estimation, and a detection rate of 95% without false alarms for fall detection.",BACKGROUND RESULTS METHODS OBJECTIVES/RESULTS OBJECTIVES/RESULTS OBJECTIVES RESULTS
D03765,"In the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks.$$$However, most of the deep learning architectures are vulnerable to so called adversarial examples.$$$This questions the security of deep neural networks (DNN) for many security- and trust-sensitive domains.$$$The majority of the proposed existing adversarial attacks are based on the differentiability of the DNN cost function.Defence strategies are mostly based on machine learning and signal processing principles that either try to detect-reject or filter out the adversarial perturbations and completely neglect the classical cryptographic component in the defence.$$$In this work, we propose a new defence mechanism based on the second Kerckhoffs's cryptographic principle which states that the defence and classification algorithm are supposed to be known, but not the key.$$$To be compliant with the assumption that the attacker does not have access to the secret key, we will primarily focus on a gray-box scenario and do not address a white-box one.$$$More particularly, we assume that the attacker does not have direct access to the secret block, but (a) he completely knows the system architecture, (b) he has access to the data used for training and testing and (c) he can observe the output of the classifier for each given input.$$$We show empirically that our system is efficient against most famous state-of-the-art attacks in black-box and gray-box scenarios.",OTHERS BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS METHODS CONCLUSIONS
D00777,"The issue of representing attacks to attacks in argumentation is receiving an increasing attention as a useful conceptual modelling tool in several contexts.$$$In this paper we present AFRA, a formalism encompassing unlimited recursive attacks within argumentation frameworks.$$$AFRA satisfies the basic requirements of definition simplicity and rigorous compatibility with Dung's theory of argumentation.$$$This paper provides a complete development of the AFRA formalism complemented by illustrative examples and a detailed comparison with other recursive attack formalizations.",BACKGROUND OBJECTIVES METHODS RESULTS
D01552,"Complex models are commonly used in predictive modeling.$$$In this paper we present R packages that can be used to explain predictions from complex black box models and attribute parts of these predictions to input features.$$$We introduce two new approaches and corresponding packages for such attribution, namely live and breakDown.$$$We also compare their results with existing implementations of state of the art solutions, namely lime that implements Locally Interpretable Model-agnostic Explanations and ShapleyR that implements Shapley values.",BACKGROUND OBJECTIVES RESULTS OBJECTIVES
D03537,"New ideas in distributed systems (algorithms or protocols) are commonly tested by simulation, because experimenting with a prototype deployed on a realistic platform is cumbersome.$$$However, a prototype not only measures performance but also verifies assumptions about the underlying system.$$$We developed dfuntest - a testing framework for distributed applications that defines abstractions and test structure, and automates experiments on distributed platforms.$$$Dfuntest aims to be jUnit's analogue for distributed applications; a framework that enables the programmer to write robust and flexible scenarios of experiments.$$$Dfuntest requires minimal bindings that specify how to deploy and interact with the application.$$$Dfuntest's abstractions allow execution of a scenario on a single machine, a cluster, a cloud, or any other distributed infrastructure, e.g. on PlanetLab.$$$A scenario is a procedure; thus, our framework can be used both for functional tests and for performance measurements.$$$We show how to use dfuntest to deploy our DHT prototype on 60 PlanetLab nodes and verify whether the prototype maintains a correct topology.",BACKGROUND BACKGROUND RESULTS OBJECTIVES METHODS METHODS OTHERS OTHERS
D04485,"The notes which play the most important and second most important roles in expressing a raga are called Vadi and Samvadi swars respectively in (North) Indian Classical music.$$$Like Bageshree, Bhairavi, Shankara, Hamir and Kalingra, Rageshree is another controversial raga so far as the choice of Vadi-Samvadi selection is concerned where there are two different opinions.$$$In the present work, a two minute vocal recording of raga Rageshree is subjected to a careful statistical analysis.$$$Our analysis is broken into three phases: first half, middle half and last half.$$$Under a multinomial model set up holding appreciably in the first two phases, only one opinion is found acceptable.$$$In the last phase the distribution seems to be quasi multinomial, characterized by an unstable nature of relative occurrence of pitch of all the notes and although the note whose relative occurrence of pitch suddenly shoots is the Vadi swar selected from our analysis of the first two phases, we take it as an outlier demanding a separate treatment like any other in statistics.$$$Selection of Vadi-Samvadi notes in a quasi-multinomial set up is still an open research problem.$$$An interesting musical cocktail is proposed, however, embedding several ideas like melodic property of notes, note combinations and pitch movements between notes, using some weighted combination of psychological and statistical stability of notes along with watching carefully the sudden shoot of one or more notes whenever there is enough evidence that multinomial model has broken down.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS RESULTS OTHERS RESULTS/CONCLUSIONS
D05136,"The problem of landmark recognition has achieved excellent results in small-scale datasets.$$$When dealing with large-scale retrieval, issues that were irrelevant with small amount of data, quickly become fundamental for an efficient retrieval phase.$$$In particular, computational time needs to be kept as low as possible, whilst the retrieval accuracy has to be preserved as much as possible.$$$In this paper we propose a novel multi-index hashing method called Bag of Indexes (BoI) for Approximate Nearest Neighbors (ANN) search.$$$It allows to drastically reduce the query time and outperforms the accuracy results compared to the state-of-the-art methods for large-scale landmark recognition.$$$It has been demonstrated that this family of algorithms can be applied on different embedding techniques like VLAD and R-MAC obtaining excellent results in very short times on different public datasets: Holidays+Flickr1M, Oxford105k and Paris106k.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS RESULTS CONCLUSIONS
D06885,"We establish exact recovery for the Least Unsquared Deviations (LUD) algorithm of Ozyesil and Singer.$$$More precisely, we show that for sufficiently many cameras with given corrupted pairwise directions, where both camera locations and pairwise directions are generated by a special probabilistic model, the LUD algorithm exactly recovers the camera locations with high probability.$$$A similar exact recovery guarantee was established for the ShapeFit algorithm by Hand, Lee and Voroninski, but with typically less corruption.",OBJECTIVES RESULTS BACKGROUND
D03004,"Calculi of string diagrams are increasingly used to present the syntax and algebraic structure of various families of circuits, including signal flow graphs, electrical circuits and quantum processes.$$$In many such approaches, the semantic interpretation for diagrams is given in terms of relations or corelations (generalised equivalence relations) of some kind.$$$In this paper we show how semantic categories of both relations and corelations can be characterised as colimits of simpler categories.$$$This modular perspective is important as it simplifies the task of giving a complete axiomatisation for semantic equivalence of string diagrams.$$$Moreover, our general result unifies various theorems that are independently found in literature and are relevant for program semantics, quantum computation and control theory.",BACKGROUND BACKGROUND/METHODS OBJECTIVES/METHODS CONCLUSIONS RESULTS
D04116,"XML query can be modeled by twig pattern query (TPQ) specifying predicates on XML nodes and XPath relationships satisfied between them.$$$A lot of TPQ types have been proposed; this paper takes into account a TPQ model extended by a specification of output and non-output query nodes since it complies with the XQuery semantics and, in many cases, it leads to a more efficient query processing.$$$In general, there are two approaches to process the TPQ: holistic joins and binary joins.$$$Whereas the binary join approach builds a query plan as a tree of interconnected binary operators, the holistic join approach evaluates a whole query using one operator (i.e., using one complex algorithm).$$$Surprisingly, a thorough analytical and experimental comparison is still missing despite an enormous research effort in this area.$$$In this paper, we try to fill this gap; we analytically and experimentally show that the binary joins used in a fully-pipelined plan (i.e., the plan where each join operation does not wait for the complete result of the previous operation and no explicit sorting is used) can often outperform the holistic joins, especially for TPQs with a higher ratio of non-output query nodes.$$$The main contributions of this paper can be summarized as follows: (i) we introduce several improvements of existing binary join approaches allowing to build a fully-pipelined plan for a TPQ considering non-output query nodes, (ii) we prove that for a certain class of TPQs such a plan has the linear time complexity with respect to the size of the input and output as well as the linear space complexity with respect to the XML document depth (i.e., the same complexity as the holistic join approaches), (iii) we show that our improved binary join approach outperforms the holistic join approaches in many situations, and (iv) we propose a simple combined approach that uses advantages of both types of approaches.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D00909,"Recent advances in saliency detection have utilized deep learning to obtain high level features to detect salient regions in a scene.$$$These advances have demonstrated superior results over previous works that utilize hand-crafted low level features for saliency detection.$$$In this paper, we demonstrate that hand-crafted features can provide complementary information to enhance performance of saliency detection that utilizes only high level features.$$$Our method utilizes both high level and low level features for saliency detection under a unified deep learning framework.$$$The high level features are extracted using the VGG-net, and the low level features are compared with other parts of an image to form a low level distance map.$$$The low level distance map is then encoded using a convolutional neural network(CNN) with multiple 1X1 convolutional and ReLU layers.$$$We concatenate the encoded low level distance map and the high level features, and connect them to a fully connected neural network classifier to evaluate the saliency of a query region.$$$Our experiments show that our method can further improve the performance of state-of-the-art deep learning-based saliency detection methods.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D05082,"A formal theory based on a binary operator of directional associative relation is constructed in the article and an understanding of an associative normal form of image constructions is introduced.$$$A model of a commutative semigroup, which provides a presentation of a sentence as three components of an interrogative linguistic image construction, is considered.",METHODS/RESULTS METHODS/RESULTS/CONCLUSIONS
D05590,"There are now several large scale deployments of differential privacy used to collect statistical information about users.$$$However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use.$$$As a result, these systems do not provide meaningful privacy guarantees over long time scales.$$$Moreover, existing techniques to mitigate this effect do not apply in the ""local model"" of differential privacy that these systems use.$$$In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods.$$$We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common (but changing) group-specific distribution.$$$We also provide an application to frequency and heavy-hitter estimation.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS
D01905,"The paper presents a study of an inter-stimulus interval (ISI) influence on a tactile point-pressure stimulus-based brain-computer interface's (tpBCI) classification accuracy.$$$A novel tactile pressure generating tpBCI stimulator is also discussed, which is based on a three-by-three pins' matrix prototype.$$$The six pin-linear patterns are presented to the user's palm during the online tpBCI experiments in an oddball style paradigm allowing for ""the aha-responses"" elucidation, within the event related potential (ERP).$$$A subsequent classification accuracies' comparison is discussed based on two ISI settings in an online tpBCI application.$$$A research hypothesis of classification accuracies' non-significant differences with various ISIs is confirmed based on the two settings of 120 ms and 300 ms, as well as with various numbers of ERP response averaging scenarios.",OBJECTIVES OBJECTIVES METHODS RESULTS RESULTS/CONCLUSIONS
D06821,"Information about external world is delivered to the brain in the form of structured in time spike trains.$$$During further processing in higher areas, information is subjected to a certain condensation process, which results in formation of abstract conceptual images of external world, apparently, represented as certain uniform spiking activity partially independent on the input spike trains details.$$$Possible physical mechanism of condensation at the level of individual neuron was discussed recently.$$$In a reverberating spiking neural network, due to this mechanism the dynamics should settle down to the same uniform/periodic activity in response to a set of various inputs.$$$Since the same periodic activity may correspond to different input spike trains, we interpret this as possible candidate for information condensation mechanism in a network.$$$Our purpose is to test this possibility in a network model consisting of five fully connected neurons, particularly, the influence of geometric size of the network, on its ability to condense information.$$$Dynamics of 20 spiking neural networks of different geometric sizes are modelled by means of computer simulation.$$$Each network was propelled into reverberating dynamics by applying various initial input spike trains.$$$We run the dynamics until it becomes periodic.$$$The Shannon's formula is used to calculate the amount of information in any input spike train and in any periodic state found.$$$As a result, we obtain explicit estimate of the degree of information condensation in the networks, and conclude that it depends strongly on the net's geometric size.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D00105,"The assumption that training and testing samples are generated from the same distribution does not always hold for real-world machine-learning applications.$$$The procedure of tackling this discrepancy between the training (source) and testing (target) domains is known as domain adaptation.$$$We propose an unsupervised version of domain adaptation that considers the presence of only unlabelled data in the target domain.$$$Our approach centers on finding correspondences between samples of each domain.$$$The correspondences are obtained by treating the source and target samples as graphs and using a convex criterion to match them.$$$The criteria used are first-order and second-order similarities between the graphs as well as a class-based regularization.$$$We have also developed a computationally efficient routine for the convex optimization, thus allowing the proposed method to be used widely.$$$To verify the effectiveness of the proposed method, computer simulations were conducted on synthetic, image classification and sentiment classification datasets.$$$Results validated that the proposed local sample-to-sample matching method out-performs traditional moment-matching methods and is competitive with respect to current local domain-adaptation methods.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS
D00774,We propose a novel framework for the analysis of learning algorithms that allows us to say when such algorithms can and cannot generalize certain patterns from training data to test data.$$$In particular we focus on situations where the rule that must be learned concerns two components of a stimulus being identical.$$$We call such a basis for discrimination an identity-based rule.$$$Identity-based rules have proven to be difficult or impossible for certain types of learning algorithms to acquire from limited datasets.$$$This is in contrast to human behaviour on similar tasks.$$$Here we provide a framework for rigorously establishing which learning algorithms will fail at generalizing identity-based rules to novel stimuli.$$$We use this framework to show that such algorithms are unable to generalize identity-based rules to novel inputs unless trained on virtually all possible inputs.$$$We demonstrate these results computationally with a multilayer feedforward neural network.,OBJECTIVES OBJECTIVES BACKGROUND BACKGROUND BACKGROUND METHODS RESULTS/CONCLUSIONS OTHERS
D04595,"Face sketches are able to capture the spatial topology of a face while lacking some facial attributes such as race, skin, or hair color.$$$Existing sketch-photo recognition approaches have mostly ignored the importance of facial attributes.$$$In this paper, we propose a new loss function, called attribute-centered loss, to train a Deep Coupled Convolutional Neural Network (DCCNN) for the facial attribute guided sketch to photo matching.$$$Specifically, an attribute-centered loss is proposed which learns several distinct centers, in a shared embedding space, for photos and sketches with different combinations of attributes.$$$The DCCNN simultaneously is trained to map photos and pairs of testified attributes and corresponding forensic sketches around their associated centers, while preserving the spatial topology information.$$$Importantly, the centers learn to keep a relative distance from each other, related to their number of contradictory attributes.$$$Extensive experiments are performed on composite (E-PRIP) and semi-forensic (IIIT-D Semi-forensic) databases.$$$The proposed method significantly outperforms the state-of-the-art.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D01293,"In this paper, we propose a novel neural approach for paraphrase generation.$$$Conventional para- phrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles.$$$To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation.$$$Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers.$$$This allows for efficient training of deep LSTMs.$$$We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers and MSCOCO.$$$Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based and bi- directional LSTM models on BLEU, METEOR, TER and an embedding-based sentence similarity metric.",OBJECTIVES BACKGROUND CONCLUSIONS METHODS METHODS METHODS RESULTS
D02619,"Conventional seq2seq chatbot models only try to find the sentences with the highest probabilities conditioned on the input sequences, without considering the sentiment of the output sentences.$$$Some research works trying to modify the sentiment of the output sequences were reported.$$$In this paper, we propose five models to scale or adjust the sentiment of the chatbot response: persona-based model, reinforcement learning, plug and play model, sentiment transformation network and cycleGAN, all based on the conventional seq2seq model.$$$We also develop two evaluation metrics to estimate if the responses are reasonable given the input.$$$These metrics together with other two popularly used metrics were used to analyze the performance of the five proposed models on different aspects, and reinforcement learning and cycleGAN were shown to be very attractive.$$$The evaluation metrics were also found to be well correlated with human evaluation.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS
D05512,"Small variance asymptotics is emerging as a useful technique for inference in large scale Bayesian non-parametric mixture models.$$$This paper analyses the online learning of robot manipulation tasks with Bayesian non-parametric mixture models under small variance asymptotics.$$$The analysis yields a scalable online sequence clustering (SOSC) algorithm that is non-parametric in the number of clusters and the subspace dimension of each cluster.$$$SOSC groups the new datapoint in its low dimensional subspace by online inference in a non-parametric mixture of probabilistic principal component analyzers (MPPCA) based on Dirichlet process, and captures the state transition and state duration information online in a hidden semi-Markov model (HSMM) based on hierarchical Dirichlet process.$$$A task-parameterized formulation of our approach autonomously adapts the model to changing environmental situations during manipulation.$$$We apply the algorithm in a teleoperation setting to recognize the intention of the operator and remotely adjust the movement of the robot using the learned model.$$$The generative model is used to synthesize both time-independent and time-dependent behaviours by relying on the principles of shared and autonomous control.$$$Experiments with the Baxter robot yield parsimonious clusters that adapt online with new demonstrations and assist the operator in performing remote manipulation tasks.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS OBJECTIVES METHODS/RESULTS RESULTS/CONCLUSIONS
D05207,"Previous approaches to model and analyze facial expression analysis use three different techniques: facial action units, geometric features and graph based modelling.$$$However, previous approaches have treated these technique separately.$$$There is an interrelationship between these techniques.$$$The facial expression analysis is significantly improved by utilizing these mappings between major geometric features involved in facial expressions and the subset of facial action units whose presence or absence are unique to a facial expression.$$$This paper combines dimension reduction techniques and image classification with search space pruning achieved by this unique subset of facial action units to significantly prune the search space.$$$The performance results on the publicly facial expression database shows an improvement in performance by 70% over time while maintaining the emotion recognition correctness.",BACKGROUND OTHERS OBJECTIVES CONCLUSIONS METHODS RESULTS
D01429,"In robust optimization, the uncertainty set is used to model all possible outcomes of uncertain parameters.$$$In the classic setting, one assumes that this set is provided by the decision maker based on the data available to her.$$$Only recently it has been recognized that the process of building useful uncertainty sets is in itself a challenging task that requires mathematical support.$$$In this paper, we propose an approach to go beyond the classic setting, by assuming multiple uncertainty sets to be prepared, each with a weight showing the degree of belief that the set is a ""true"" model of uncertainty.$$$We consider theoretical aspects of this approach and show that it is as easy to model as the classic setting.$$$In an extensive computational study using a shortest path problem based on real-world data, we auto-tune uncertainty sets to the available data, and show that with regard to out-sample performance, the combination of multiple sets can give better results than each set on its own.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/RESULTS METHODS/RESULTS
D02843,"We present a simple yet effective approach for linking entities in queries.$$$The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query.$$$Then, we employ a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework.$$$The advantages of our approach lie in two aspects, which contribute to the ranking process and final linking result.$$$First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query.$$$Second, we can obtain the query sensitive prior probability in addition to the static link-probability derived from all Wikipedia articles.$$$We conduct experiments on two benchmark datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ dataset.$$$Experimental results show that our method outperforms state-of-the-art systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ dataset.",BACKGROUND METHODS METHODS CONCLUSIONS CONCLUSIONS CONCLUSIONS RESULTS RESULTS
D05841,"Classification of multivariate time series (MTS) has been tackled with a large variety of methodologies and applied to a wide range of scenarios.$$$Among the existing approaches, reservoir computing (RC) techniques, which implement a fixed and high-dimensional recurrent network to process sequential data, are computationally efficient tools to generate a vectorial, fixed-size representation of the MTS that can be further processed by standard classifiers.$$$Despite their unrivaled training speed, MTS classifiers based on a standard RC architecture fail to achieve the same accuracy of other classifiers, such as those exploiting fully trainable recurrent networks.$$$In this paper we introduce the reservoir model space, an RC approach to learn vectorial representations of MTS in an unsupervised fashion.$$$Each MTS is encoded within the parameters of a linear model trained to predict a low-dimensional embedding of the reservoir dynamics.$$$Our model space yields a powerful representation of the MTS and, thanks to an intermediate dimensionality reduction procedure, attains computational performance comparable to other RC methods.$$$As a second contribution we propose a modular RC framework for MTS classification, with an associated open source Python library.$$$By combining the different modules it is possible to seamlessly implement advanced RC architectures, including our proposed unsupervised representation, bidirectional reservoirs, and non-linear readouts, such as deep neural networks with both fixed and flexible activation functions.$$$Results obtained on benchmark and real-world MTS datasets show that RC classifiers are dramatically faster and, when implemented using our proposed representation, also achieve superior classification accuracy.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS OBJECTIVES METHODS RESULTS/CONCLUSIONS
D03854,Overlapping of cervical cells and poor contrast of cell cytoplasm are the major issues in accurate detection and segmentation of cervical cells.$$$An unsupervised cell segmentation approach is presented here.$$$Cell clump segmentation was carried out using the extended depth of field (EDF) image created from the images of different focal planes.$$$A modified Otsu method with prior class weights is proposed for accurate segmentation of nuclei from the cell clumps.$$$The cell cytoplasm was further segmented from cell clump depending upon the number of nucleus detected in that cell clump.$$$Level set model was used for cytoplasm segmentation.,BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS
D03966,"We consider the problem of mapping digital data encoded on a quantum register to analog amplitudes in parallel.$$$It is shown to be unlikely that a fully unitary polynomial-time quantum algorithm exists for this problem; NP becomes a subset of BQP if it exists.$$$In the practical point of view, we propose a nonunitary linear-time algorithm using quantum decoherence.$$$It tacitly uses an exponentially large physical resource, which is typically a huge number of identical molecules.$$$Quantumness of correlation appearing in the process of the algorithm is also discussed.",OBJECTIVES RESULTS METHODS/CONCLUSIONS METHODS/RESULTS OBJECTIVES
D03101,"In this paper, we propose a novel CS approach in which the acquisition of non-visible information is also avoided.",OBJECTIVES
D02580,"Almost all known secret sharing schemes work on numbers.$$$Such methods will have difficulty in sharing graphs since the number of graphs increases exponentially with the number of nodes.$$$We propose a secret sharing scheme for graphs where we use graph intersection for reconstructing the secret which is hidden as a sub graph in the shares.$$$Our method does not rely on heavy computational operations such as modular arithmetic or polynomial interpolation but makes use of very basic operations like assignment and checking for equality, and graph intersection can also be performed visually.$$$In certain cases, the secret could be reconstructed using just pencil and paper by authorised parties but cannot be broken by an adversary even with unbounded computational power.$$$The method achieves perfect secrecy for (2, n) scheme and requires far fewer operations compared to Shamir's algorithm.$$$The proposed method could be used to share objects such as matrices, sets, plain text and even a heterogeneous collection of these.$$$Since we do not require a previously agreed upon encoding scheme, the method is very suitable for sharing heterogeneous collection of objects in a dynamic fashion.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS
D03195,"As an ubiquitous method in natural language processing, word embeddings are extensively employed to map semantic properties of words into a dense vector representation.$$$They capture semantic and syntactic relations among words but the vector corresponding to the words are only meaningful relative to each other.$$$Neither the vector nor its dimensions have any absolute, interpretable meaning.$$$We introduce an additive modification to the objective function of the embedding learning algorithm that encourages the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected.$$$In other words, we align words that are already determined to be related, along predefined concepts.$$$Therefore, we impart interpretability to the word embedding by assigning meaning to its vector dimensions.$$$The predefined concepts are derived from an external lexical resource, which in this paper is chosen as Roget's Thesaurus.$$$We observe that alignment along the chosen concepts is not limited to words in the Thesaurus and extends to other related words as well.$$$We quantify the extent of interpretability and assignment of meaning from our experimental results.$$$We also demonstrate the preservation of semantic coherence of the resulting vector space by using word-analogy and word-similarity tests.$$$These tests show that the interpretability-imparted word embeddings that are obtained by the proposed framework do not sacrifice performances in common benchmark tests.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS OBJECTIVES/METHODS OBJECTIVES METHODS RESULTS METHODS/RESULTS RESULTS RESULTS/CONCLUSIONS
D06946,"In this work, we propose a novel framework for privacy-preserving client-distributed machine learning.$$$It is motivated by the desire to achieve differential privacy guarantees in the local model of privacy in a way that satisfies all systems constraints using asynchronous client-server communication and provides attractive model learning properties.$$$We call it ""Draw and Discard"" because it relies on random sampling of models for load distribution (scalability), which also provides additional server-side privacy protections and improved model quality through averaging.$$$We present the mechanics of client and server components of ""Draw and Discard"" and demonstrate how the framework can be applied to learning Generalized Linear models.$$$We then analyze the privacy guarantees provided by our approach against several types of adversaries and showcase experimental results that provide evidence for the framework's viability in practical deployments.",OBJECTIVES OBJECTIVES METHODS RESULTS RESULTS
D00716,"Unsupervised learning for visual perception of 3D geometry is of great interest to autonomous systems.$$$Recent works on unsupervised learning have made considerable progress on geometry perception; however, they perform poorly on dynamic objects and scenarios with dark and noisy environments.$$$In contrast, supervised learning algorithms, which are robust, require large labeled geometric data-set.$$$This paper introduces SIGNet, a novel framework that provides robust geometry perception without requiring geometrically informative labels.$$$Specifically, SIGNet integrates semantic information to make unsupervised robust geometric predictions for dynamic objects in low lighting and noisy environments.$$$SIGNet is shown to improve upon the state of art unsupervised learning for geometry perception by 30% (in squared relative error for depth prediction).$$$In particular, SIGNet improves the dynamic object class performance by 39% in depth prediction and 29% in flow prediction.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D03742,"Text segmentation (TS) aims at dividing long text into coherent segments which reflect the subtopic structure of the text.$$$It is beneficial to many natural language processing tasks, such as Information Retrieval (IR) and document summarisation.$$$Current approaches to text segmentation are similar in that they all use word-frequency metrics to measure the similarity between two regions of text, so that a document is segmented based on the lexical cohesion between its words.$$$Various NLP tasks are now moving towards the semantic web and ontologies, such as ontology-based IR systems, to capture the conceptualizations associated with user needs and contents.$$$Text segmentation based on lexical cohesion between words is hence not sufficient anymore for such tasks.$$$This paper proposes OntoSeg, a novel approach to text segmentation based on the ontological similarity between text blocks.$$$The proposed method uses ontological similarity to explore conceptual relations between text segments and a Hierarchical Agglomerative Clustering (HAC) algorithm to represent the text as a tree-like hierarchy that is conceptually structured.$$$The rich structure of the created tree further allows the segmentation of text in a linear fashion at various levels of granularity.$$$The proposed method was evaluated on a wellknown dataset, and the results show that using ontological similarity in text segmentation is very promising.$$$Also we enhance the proposed method by combining ontological similarity with lexical similarity and the results show an enhancement of the segmentation quality.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS METHODS/RESULTS
D04541,"An important issue with oversampled FIR analysis filter banks (FBs) is to determine inverse synthesis FBs, when they exist.$$$Given any complex oversampled FIR analysis FB, we first provide an algorithm to determine whether there exists an inverse FIR synthesis system.$$$We also provide a method to ensure the Hermitian symmetry property on the synthesis side, which is serviceable to processing real-valued signals.$$$As an invertible analysis scheme corresponds to a redundant decomposition, there is no unique inverse FB.$$$Given a particular solution, we parameterize the whole family of inverses through a null space projection.$$$The resulting reduced parameter set simplifies design procedures, since the perfect reconstruction constrained optimization problem is recast as an unconstrained optimization problem.$$$The design of optimized synthesis FBs based on time or frequency localization criteria is then investigated, using a simple yet efficient gradient algorithm.",BACKGROUND/OBJECTIVES OBJECTIVES/RESULTS RESULTS BACKGROUND METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS
D00044,"We propose a mechanism that incorporates network coding into TCP with only minor changes to the protocol stack, thereby allowing incremental deployment.$$$In our scheme, the source transmits random linear combinations of packets currently in the congestion window.$$$At the heart of our scheme is a new interpretation of ACKs - the sink acknowledges every degree of freedom (i.e., a linear combination that reveals one unit of new information) even if it does not reveal an original packet immediately.$$$Such ACKs enable a TCP-like sliding-window approach to network coding.$$$Our scheme has the nice property that packet losses are essentially masked from the congestion control algorithm.$$$Our algorithm therefore reacts to packet drops in a smooth manner, resulting in a novel and effective approach for congestion control over networks involving lossy links such as wireless links.$$$Our experiments show that our algorithm achieves higher throughput compared to TCP in the presence of lossy wireless links.$$$We also establish the soundness and fairness properties of our algorithm.",OBJECTIVES/RESULTS METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D02526,"Generative adversarial networks (GANs) are powerful tools for learning generative models.$$$In practice, the training may suffer from lack of convergence.$$$GANs are commonly viewed as a two-player zero-sum game between two neural networks.$$$Here, we leverage this game theoretic view to study the convergence behavior of the training process.$$$Inspired by the fictitious play learning process, a novel training method, referred to as Fictitious GAN, is introduced.$$$Fictitious GAN trains the deep neural networks using a mixture of historical models.$$$Specifically, the discriminator (resp. generator) is updated according to the best-response to the mixture outputs from a sequence of previously trained generators (resp. discriminators).$$$It is shown that Fictitious GAN can effectively resolve some convergence issues that cannot be resolved by the standard training approach.$$$It is proved that asymptotically the average of the generator outputs has the same distribution as the data samples.",BACKGROUND OBJECTIVES BACKGROUND METHODS/CONCLUSIONS METHODS/RESULTS METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05352,"Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses.$$$Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years.$$$Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit.$$$Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category).$$$Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79% recall@30, significantly higher than several baselines.$$$Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.",METHODS METHODS METHODS METHODS RESULTS RESULTS
D04793,"Novel scientific knowledge is constantly produced by the scientific community.$$$Understanding the level of novelty characterized by scientific literature is key for modeling scientific dynamics and analyzing the growth mechanisms of scientific knowledge.$$$Metrics derived from bibliometrics and citation analysis were effectively used to characterize the novelty in scientific development.$$$However, time is required before we can observe links between documents such as citation links or patterns derived from the links, which makes these techniques more effective for retrospective analysis than predictive analysis.$$$In this study, we present a new approach to measuring the novelty of a research topic in a scientific community over a specific period by tracking semantic changes of the terms and characterizing the research topic in their usage context.$$$The semantic changes are derived from the text data of scientific literature by temporal embedding learning techniques.$$$We validated the effects of the proposed novelty metric on predicting the future growth of scientific publications and investigated the relations between novelty and growth by panel data analysis applied in a large-scale publication dataset (MEDLINE/PubMed).$$$Key findings based on the statistical investigation indicate that the novelty metric has significant predictive effects on the growth of scientific literature and the predictive effects may last for more than ten years.$$$We demonstrated the effectiveness and practical implications of the novelty metric in three case studies.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS CONCLUSIONS CONCLUSIONS
D05413,"Voice disguise, purposeful modification of one's speaker identity with the aim of avoiding being identified as oneself, is a low-effort way to fool speaker recognition, whether performed by a human or an automatic speaker verification (ASV) system.$$$We present an evaluation of the effectiveness of age stereotypes as a voice disguise strategy, as a follow up to our recent work where 60 native Finnish speakers attempted to sound like an elderly and like a child.$$$In that study, we presented evidence that both ASV and human observers could easily miss the target speaker but we did not address how believable the presented vocal age stereotypes were; this study serves to fill that gap.$$$The interesting cases would be speakers who succeed in being missed by the ASV system, and which a typical listener cannot detect as being a disguise.$$$We carry out a perceptual test to study the quality of the disguised speech samples.$$$The listening test was carried out both locally and with the help of Amazon's Mechanical Turk (MT) crowd-workers.$$$A total of 91 listeners participated in the test and were instructed to estimate both the speaker's chronological and intended age.$$$The results indicate that age estimations for the intended old and child voices for female speakers were towards the target age groups, while for male speakers, the age estimations corresponded to the direction of the target voice only for elderly voices.$$$In the case of intended child's voice, listeners estimated the age of male speakers to be older than their chronological age for most of the speakers and not the intended target age.",BACKGROUND OBJECTIVES BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06367,"We present a generic and automated approach to re-identifying nodes in anonymized social networks which enables novel anonymization techniques to be quickly evaluated.$$$It uses machine learning (decision forests) to matching pairs of nodes in disparate anonymized sub-graphs.$$$The technique uncovers artefacts and invariants of any black-box anonymization scheme from a small set of examples.$$$Despite a high degree of automation, classification succeeds with significant true positive rates even when small false positive rates are sought.$$$Our evaluation uses publicly available real world datasets to study the performance of our approach against real-world anonymization strategies, namely the schemes used to protect datasets of The Data for Development (D4D) Challenge.$$$We show that the technique is effective even when only small numbers of samples are used for training.$$$Further, since it detects weaknesses in the black-box anonymization scheme it can re-identify nodes in one social network when trained on another.",BACKGROUND METHODS RESULTS CONCLUSIONS BACKGROUND RESULTS RESULTS
D05210,"We present a novel cross-view classification algorithm where the gallery and probe data come from different views.$$$A popular approach to tackle this problem is the multi-view subspace learning (MvSL) that aims to learn a latent subspace shared by multi-view data.$$$Despite promising results obtained on some applications, the performance of existing methods deteriorates dramatically when the multi-view data is sampled from nonlinear manifolds or suffers from heavy outliers.$$$To circumvent this drawback, motivated by the Divide-and-Conquer strategy, we propose Multi-view Hybrid Embedding (MvHE), a unique method of dividing the problem of cross-view classification into three subproblems and building one model for each subproblem.$$$Specifically, the first model is designed to remove view discrepancy, whereas the second and third models attempt to discover the intrinsic nonlinear structure and to increase discriminability in intra-view and inter-view samples respectively.$$$The kernel extension is conducted to further boost the representation power of MvHE.$$$Extensive experiments are conducted on four benchmark datasets.$$$Our methods demonstrate overwhelming advantages against the state-of-the-art MvSL based cross-view classification approaches in terms of classification accuracy and robustness.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS METHODS METHODS OTHERS RESULTS/CONCLUSIONS
D01238,"Estimating the influence of a given feature to a model prediction is challenging.$$$We introduce ROAR, RemOve And Retrain, a benchmark to evaluate the accuracy of interpretability methods that estimate input feature importance in deep neural networks.$$$We remove a fraction of input features deemed to be most important according to each estimator and measure the change to the model accuracy upon retraining.$$$The most accurate estimator will identify inputs as important whose removal causes the most damage to model performance relative to all other estimators.$$$This evaluation produces thought-provoking results -- we find that several estimators are less accurate than a random assignment of feature importance.$$$However, averaging a set of squared noisy estimators (a variant of a technique proposed by Smilkov et al.$$$(2017)), leads to significant gains in accuracy for each method considered and far outperforms such a random guess.",BACKGROUND METHODS METHODS METHODS RESULTS RESULTS RESULTS
D00451,"The paper aims to show how an application can be developed that converts the English language into the Punjabi Language, and the same application can convert the Text to Speech(TTS) i.e. pronounce the text.$$$This application can be really beneficial for those with special needs.",OBJECTIVES OBJECTIVES
D00115,"Extreme learning machine (ELM) is an extremely fast learning method and has a powerful performance for pattern recognition tasks proven by enormous researches and engineers.$$$However, its good generalization ability is built on large numbers of hidden neurons, which is not beneficial to real time response in the test process.$$$In this paper, we proposed new ways, named ""constrained extreme learning machines"" (CELMs), to randomly select hidden neurons based on sample distribution.$$$Compared to completely random selection of hidden nodes in ELM, the CELMs randomly select hidden nodes from the constrained vector space containing some basic combinations of original sample vectors.$$$The experimental results show that the CELMs have better generalization ability than traditional ELM, SVM and some other related methods.$$$Additionally, the CELMs have a similar fast learning speed as ELM.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D03207,"Optical Character Recognition (OCR) has been a topic of interest for many years.$$$It is defined as the process of digitizing a document image into its constituent characters.$$$Despite decades of intense research, developing OCR with capabilities comparable to that of human still remains an open challenge.$$$Due to this challenging nature, researchers from industry and academic circles have directed their attentions towards Optical Character Recognition.$$$Over the last few years, the number of academic laboratories and companies involved in research on Character Recognition has increased dramatically.$$$This research aims at summarizing the research so far done in the field of OCR.$$$It provides an overview of different aspects of OCR and discusses corresponding proposals aimed at resolving issues of OCR.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES
D04420,"Stochastic behaviors of resistive random access memory (RRAM) play an important role in the design of cross-point memory arrays.$$$A Monte Carlo compact model of oxide RRAM is developed and calibrated with experiments on various device stack configurations.$$$With Monte Carlo SPICE simulations, we show that an increase in array size and interconnect wire resistance will statistically deteriorate write functionality.$$$Write failure probability (WFP) has an exponential dependency on device uniformity and supply voltage (VDD), and the array bias scheme is a key knob.$$$Lowering array VDD leads to higher effective energy consumption (EEC) due to the increase in WFP when the variation statistics are included in the analysis.$$$Random-access simulations indicate that data sparsity statistically benefits write functionality and energy consumption.$$$Finally, we show that a pseudo-sub-array topology with uniformly distributed pre-forming cells in the pristine high resistance state is able to reduce both WFP and EEC, enabling higher net capacity for memory circuits due to improved variation tolerance.",BACKGROUND METHODS RESULTS RESULTS RESULTS RESULTS RESULTS
D06361,"Estimating statistical models within sensor networks requires distributed algorithms, in which both data and computation are distributed across the nodes of the network.$$$We propose a general approach for distributed learning based on combining local estimators defined by pseudo-likelihood components, encompassing a number of combination methods, and provide both theoretical and experimental analysis.$$$We show that simple linear combination or max-voting methods, when combined with second-order information, are statistically competitive with more advanced and costly joint optimization.$$$Our algorithms have many attractive properties including low communication and computational cost and ""any-time"" behavior.",BACKGROUND OBJECTIVES METHODS METHODS
D01062,"In this paper, we review multi-agent collective behavior algorithms in the literature and classify them according to their underlying mathematical structure.$$$For each mathematical technique, we identify the multi-agent coordination tasks it can be applied to, and we analyze its scalability, bandwidth use, and demonstrated maturity.$$$We highlight how versatile techniques such as artificial potential functions can be used for applications ranging from low-level position control to high-level coordination and task allocation, we discuss possible reasons for the slow adoption of complex distributed coordination algorithms in the field, and we highlight areas for further research and development.",OBJECTIVES METHODS RESULTS
D06315,"Humans have an unparalleled visual intelligence and can overcome visual ambiguities that machines currently cannot.$$$Recent works have shown that incorporating guidance from humans during inference for monocular viewpoint-estimation can help overcome difficult cases in which the computer-alone would have otherwise failed.$$$These hybrid intelligence approaches are hence gaining traction.$$$However, deciding what question to ask the human at inference time remains an unknown for these problems.$$$We address this question by formulating it as an Adviser Problem: can we learn a mapping from the input to a specific question to ask the human to maximize the expected positive impact to the overall task?$$$We formulate a solution to the adviser problem for viewpoint estimation using a deep network where the question asks for the location of a keypoint in the input image.$$$We show that by using the Adviser Network's recommendations, the model and the human outperforms the previous hybrid-intelligence state-of-the-art by 3.7%, and the computer-only state-of-the-art by 5.28% absolute.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D03804,"Many real world applications can be framed as multi-objective optimization problems, where we wish to simultaneously optimize for multiple criteria.$$$Bayesian optimization techniques for the multi-objective setting are pertinent when the evaluation of the functions in question are expensive.$$$Traditional methods for multi-objective optimization, both Bayesian and otherwise, are aimed at recovering the Pareto front of these objectives.$$$However, in certain cases a practitioner might desire to identify Pareto optimal points only in a particular region of the Pareto front due to external considerations.$$$In this work, we propose a strategy based on random scalarizations of the objectives that addresses this problem.$$$While being computationally similar or cheaper than other approaches, our approach is flexible enough to sample from specified subsets of the Pareto front or the whole of it.$$$We also introduce a novel notion of regret in the multi-objective setting and show that our strategy achieves sublinear regret.$$$We experiment with both synthetic and real-life problems, and demonstrate superior performance of our proposed algorithm in terms of flexibility, scalability and regret.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS METHODS RESULTS/CONCLUSIONS
D05693,"Dynamic ensemble selection (DES) techniques work by estimating the level of competence of each classifier from a pool of classifiers.$$$Only the most competent ones are selected to classify a given test sample.$$$Hence, the key issue in DES is the criterion used to estimate the level of competence of the classifiers in predicting the label of a given test sample.$$$In order to perform a more robust ensemble selection, we proposed the META-DES framework using meta-learning, where multiple criteria are encoded as meta-features and are passed down to a meta-classifier that is trained to estimate the competence level of a given classifier.$$$In this technical report, we present a step-by-step analysis of each phase of the framework during training and test.$$$We show how each set of meta-features is extracted as well as their impact on the estimation of the competence level of the base classifier.$$$Moreover, an analysis of the impact of several factors in the system performance, such as the number of classifiers in the pool, the use of different linear base classifiers, as well as the size of the validation data.$$$We show that using the dynamic selection of linear classifiers through the META-DES framework, we can solve complex non-linear classification problems where other combination techniques such as AdaBoost cannot.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES/METHODS RESULTS CONCLUSIONS
D00819,"Sparse connectivity is an important factor behind the success of convolutional neural networks and recurrent neural networks.$$$In this paper, we consider the problem of learning sparse connectivity for feedforward neural networks (FNNs).$$$The key idea is that a unit should be connected to a small number of units at the next level below that are strongly correlated.$$$We use Chow-Liu's algorithm to learn a tree-structured probabilistic model for the units at the current level, use the tree to identify subsets of units that are strongly correlated, and introduce a new unit with receptive field over the subsets.$$$The procedure is repeated on the new units to build multiple layers of hidden units.$$$The resulting model is called a TRF-net.$$$Empirical results show that, when compared to dense FNNs, TRF-net achieves better or comparable classification performance with much fewer parameters and sparser structures.$$$They are also more interpretable.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS RESULTS
D04142,"In 2016, 2017, and 2018 at the IEEE Conference on Computational Intelligence in Games, the authors of this paper ran a competition for agents that can play classic text-based adventure games.$$$This competition fills a gap in existing game AI competitions that have typically focussed on traditional card/board games or modern video games with graphical interfaces.$$$By providing a platform for evaluating agents in text-based adventures, the competition provides a novel benchmark for game AI with unique challenges for natural language understanding and generation.$$$This paper summarises the three competitions ran in 2016, 2017, and 2018 (including details of open source implementations of both the competition framework and our competitors) and presents the results of an improved evaluation of these competitors across 20 games.",BACKGROUND BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES METHODS/RESULTS
D03379,"An optimal data partitioning in parallel & distributed implementation of clustering algorithms is a necessary computation as it ensures independent task completion, fair distribution, less number of affected points and better & faster merging.$$$Though partitioning using Kd Tree is being conventionally used in academia, it suffers from performance drenches and bias (non equal distribution) as dimensionality of data increases and hence is not suitable for practical use in industry where dimensionality can be of order of 100s to 1000s.$$$To address these issues we propose two new partitioning techniques using existing mathematical models & study their feasibility, performance (bias and partitioning speed) & possible variants in choosing initial seeds.$$$First method uses an n dimensional hashed grid based approach which is based on mapping the points in space to a set of cubes which hashes the points.$$$Second method uses a tree of voronoi planes where each plane corresponds to a partition.$$$We found that grid based approach was computationally impractical, while using a tree of voronoi planes (using scalable K-Means++ initial seeds) drastically outperformed the Kd-tree tree method as dimensionality increased.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D04057,"In this paper, we present a new method for detecting road users in an urban environment which leads to an improvement in multiple object tracking.$$$Our method takes as an input a foreground image and improves the object detection and segmentation.$$$This new image can be used as an input to trackers that use foreground blobs from background subtraction.$$$The first step is to create foreground images for all the frames in an urban video.$$$Then, starting from the original blobs of the foreground image, we merge the blobs that are close to one another and that have similar optical flow.$$$The next step is extracting the edges of the different objects to detect multiple objects that might be very close (and be merged in the same blob) and to adjust the size of the original blobs.$$$At the same time, we use the optical flow to detect occlusion of objects that are moving in opposite directions.$$$Finally, we make a decision on which information we keep in order to construct a new foreground image with blobs that can be used for tracking.$$$The system is validated on four videos of an urban traffic dataset.$$$Our method improves the recall and precision metrics for the object detection task compared to the vanilla background subtraction method and improves the CLEAR MOT metrics in the tracking tasks for most videos.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D02651,"The metrics play increasingly fundamental role in the design, development, deployment and operation of telecommunication systems.$$$Despite their importance, the studies of metrics are usually limited to a narrow area or a well-defined objective.$$$Our study aims to more broadly survey the metrics that are commonly used for analyzing, developing and managing telecommunication networks in order to facilitate understanding of the current metrics landscape.$$$The metrics are simple abstractions of systems, and they directly influence how the systems are perceived by different stakeholders.$$$However, defining and using metrics for telecommunication systems with ever increasing complexity is a complicated matter which has not been so far systematically and comprehensively considered in the literature.$$$The common metrics sources are identified, and how the metrics are used and selected is discussed.$$$The most commonly used metrics for telecommunication systems are categorized and presented as energy and power metrics, quality-of-service metrics, quality-of-experience metrics, security metrics, and reliability and resilience metrics.$$$Finally, the research directions and recommendations how the metrics can evolve, and be defined and used more effectively are outlined.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND BACKGROUND RESULTS RESULTS RESULTS
D03662,"Content-Centric Networking (CCN) is an internetworking paradigm that offers an alternative to today's IP-based Internet Architecture.$$$Instead of focusing on hosts and their locations, CCN emphasizes addressable named content.$$$By decoupling content from its location, CCN allows opportunistic in-network content caching, thus enabling better network utilization, at least for scalable content distribution.$$$However, in order to be considered seriously, CCN must support basic security services, including content authenticity, integrity, confidentiality, authorization and access control.$$$Current approaches rely on content producers to perform authorization and access control.$$$This general approach has several disadvantages.$$$First, consumer privacy vis-a-vis producers is not preserved.$$$Second, identity management and access control impose high computational overhead on producers.$$$Also, unnecessary repeated authentication and access control decisions must be made for each content request.$$$These issues motivate our design of KRB-CCN - a complete authorization and access control system for private CCNs.$$$Inspired by Kerberos in IP-based networks, KRB-CCN involves distinct authentication and authorization authorities.$$$By doing so, KRB-CCN obviates the need for producers to make consumer authentication and access control decisions.$$$KRB-CCN preserves consumer privacy since producers are unaware of consumer identities.$$$Producers are also not required to keep any hard state and only need to perform two symmetric key operations to guarantee that sensitive content is confidentially delivered only to authenticated and authorized consumers.$$$Most importantly, unlike prior designs, KRB-CCN leaves the network (i.e., CCN routers) out of any authorization, access control or confidentiality issues.$$$We describe KRB-CCN design and implementation, analyze its security, and report on its performance.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS RESULTS CONCLUSIONS
D04818,"cryptographic hash function is a deterministic procedure that compresses an arbitrary block of numerical data and returns a fixed-size bit string.$$$There exist many hash functions: MD5, HAVAL, SHA, ...$$$It was reported that these hash functions are not longer secure.$$$Our work is focused in the construction of a new hash function based on composition of functions.$$$The construction used the NP-completeness of Three-dimensional contingency tables and the relaxation of the constraint that a hash function should also be a compression function.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS/RESULTS/CONCLUSIONS
D05164,"Due to the rapidly rising popularity of Massive Open Online Courses (MOOCs), there is a growing demand for scalable automated support technologies for student learning.$$$Transferring traditional educational resources to online contexts has become an increasingly relevant problem in recent years.$$$For learning science theories to be applicable, educators need a way to identify learning behaviors of students which contribute to learning outcomes, and use them to design and provide personalized intervention support to the students.$$$Click logs are an important source of information about students' learning behaviors, however current literature has limited understanding of how these behaviors are represented within click logs.$$$In this project, we have exploited the temporal dynamics of student behaviors both to do behavior modeling via graphical modeling approaches and to do performance prediction via recurrent neural network approaches in order to first identify student behaviors and then use them to predict their final outcome in the course.$$$Our experiments showed that the long short-term memory (LSTM) model is capable of learning long-term dependencies in a sequence and outperforms other strong baselines in the prediction task.$$$Further, these sequential approaches to click log analysis can be successfully imported to other courses when used with results obtained from graphical model behavior modeling.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D03612,"Improving patient care safety is an ultimate objective for medical cyber-physical systems.$$$A recent study shows that the patients' death rate can be significantly reduced by computerizing medical best practice guidelines.$$$To facilitate the development of computerized medical best practice guidelines, statecharts are often used as a modeling tool because of their high resemblances to disease and treatment models and their capabilities to provide rapid prototyping and simulation for clinical validations.$$$However, some implementations of statecharts, such as Yakindu statecharts, are priority-based and have synchronous execution semantics which makes it difficult to model certain functionalities that are essential in modeling medical guidelines, such as two-way communications and configurable execution orders.$$$Rather than introducing new statechart elements or changing the statechart implementation's underline semantics, we use existing basic statechart elements to design model patterns for the commonly occurring issues.$$$In particular, we show the design of model patterns for two-way communications and configurable execution orders and formally prove the correctness of these model patterns.$$$We further use a simplified airway laser surgery scenario as a case study to demonstrate how the developed model patterns address the two-way communication and configurable execution order issues and their impact on validation and verification of medical safety properties.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D01883,"Comtraces (combined traces) are extensions of Mazurkiewicz traces that can model the ""not later than"" relationship.$$$In this paper, we first introduce the novel notion of generalized comtraces, extensions of comtraces that can additionally model the ""non-simultaneously"" relationship.$$$Then we study some basic algebraic properties and canonical reprentations of comtraces and generalized comtraces.$$$Finally we analyze the relationship between generalized comtraces and generalized stratified order structures.$$$The major technical contribution of this paper is a proof showing that generalized comtraces can be represented by generalized stratified order structures.",BACKGROUND RESULTS METHODS RESULTS RESULTS/CONCLUSIONS
D05686,"This paper proposes a novel framework for the use of eye movement patterns for biometric applications.$$$Eye movements contain abundant information about cognitive brain functions, neural pathways, etc.$$$In the proposed method, eye movement data is classified into fixations and saccades.$$$Features extracted from fixations and saccades are used by a Gaussian Radial Basis Function Network (GRBFN) based method for biometric authentication.$$$A score fusion approach is adopted to classify the data in the output layer.$$$In the evaluation stage, the algorithm has been tested using two types of stimuli: random dot following on a screen and text reading.$$$The results indicate the strength of eye movement pattern as a biometric modality.$$$The algorithm has been evaluated on BioEye 2015 database and found to outperform all the other methods.$$$Eye movements are generated by a complex oculomotor plant which is very hard to spoof by mechanical replicas.$$$Use of eye movement dynamics along with iris recognition technology may lead to a robust counterfeit-resistant person identification system.",OBJECTIVES BACKGROUND METHODS METHODS METHODS METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D06682,"We propose a DC proximal Newton algorithm for solving nonconvex regularized sparse learning problems in high dimensions.$$$Our proposed algorithm integrates the proximal Newton algorithm with multi-stage convex relaxation based on the difference of convex (DC) programming, and enjoys both strong computational and statistical guarantees.$$$Specifically, by leveraging a sophisticated characterization of sparse modeling structures/assumptions (i.e., local restricted strong convexity and Hessian smoothness), we prove that within each stage of convex relaxation, our proposed algorithm achieves (local) quadratic convergence, and eventually obtains a sparse approximate local optimum with optimal statistical properties after only a few convex relaxations.$$$Numerical experiments are provided to support our theory.",OBJECTIVES METHODS METHODS RESULTS
D01016,"Proliferation of touch-based devices has made sketch-based image retrieval practical.$$$While many methods exist for sketch-based object detection/image retrieval on small datasets, relatively less work has been done on large (web)-scale image retrieval.$$$In this paper, we present an efficient approach for image retrieval from millions of images based on user-drawn sketches.$$$Unlike existing methods for this problem which are sensitive to even translation or scale variations, our method handles rotation, translation, scale (i.e. a similarity transformation) and small deformations.$$$The object boundaries are represented as chains of connected segments and the database images are pre-processed to obtain such chains that have a high chance of containing the object.$$$This is accomplished using two approaches in this work: a) extracting long chains in contour segment networks and b) extracting boundaries of segmented object proposals.$$$These chains are then represented by similarity-invariant variable length descriptors.$$$Descriptor similarities are computed by a fast Dynamic Programming-based partial matching algorithm.$$$This matching mechanism is used to generate a hierarchical k-medoids based indexing structure for the extracted chains of all database images in an offline process which is used to efficiently retrieve a small set of possible matched images for query chains.$$$Finally, a geometric verification step is employed to test geometric consistency of multiple chain matches to improve results.$$$Qualitative and quantitative results clearly demonstrate superiority of the approach over existing methods.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS METHODS RESULTS
D00461,"We propose a framework, named Aggregated Wasserstein, for computing a dissimilarity measure or distance between two Hidden Markov Models with state conditional distributions being Gaussian.$$$For such HMMs, the marginal distribution at any time spot follows a Gaussian mixture distribution, a fact exploited to softly match, aka register, the states in two HMMs.$$$We refer to such HMMs as Gaussian mixture model-HMM (GMM-HMM).$$$The registration of states is inspired by the intrinsic relationship of optimal transport and the Wasserstein metric between distributions.$$$Specifically, the components of the marginal GMMs are matched by solving an optimal transport problem where the cost between components is the Wasserstein metric for Gaussian distributions.$$$The solution of the optimization problem is a fast approximation to the Wasserstein metric between two GMMs.$$$The new Aggregated Wasserstein distance is a semi-metric and can be computed without generating Monte Carlo samples.$$$It is invariant to relabeling or permutation of the states.$$$This distance quantifies the dissimilarity of GMM-HMMs by measuring both the difference between the two marginal GMMs and the difference between the two transition matrices.$$$Our new distance is tested on the tasks of retrieval and classification of time series.$$$Experiments on both synthetic data and real data have demonstrated its advantages in terms of accuracy as well as efficiency in comparison with existing distances based on the Kullback-Leibler divergence.",METHODS BACKGROUND BACKGROUND METHODS METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D01344,We outline a program in the area of formalization of mathematics to automate theorem proving in algebra and algebraic geometry.$$$We propose a construction of a dictionary between automated theorem provers and (La)TeX exploiting syntactic parsers.$$$We describe its application to a repository of human-written facts and definitions in algebraic geometry (The Stacks Project).$$$We use deep learning techniques.,OBJECTIVES/RESULTS OBJECTIVES/RESULTS CONCLUSIONS METHODS
D03896,"We address the problem of learning hierarchical deep neural network policies for reinforcement learning.$$$In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective.$$$Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer.$$$The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space.$$$Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior.$$$Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D04205,"The traction force of a kite can be used to drive a cyclic motion for extracting wind energy from the atmosphere.$$$This paper presents a novel quasi-steady modelling framework for predicting the power generated over a full pumping cycle.$$$The cycle is divided into traction, retraction and transition phases, each described by an individual set of analytic equations.$$$The effect of gravity on the airborne system components is included in the framework.$$$A trade-off is made between modelling accuracy and computation speed such that the model is specifically useful for system optimisation and scaling in economic feasibility studies.$$$Simulation results are compared to experimental measurements of a 20 kW kite power system operated up to a tether length of 720 m. Simulation and experiment agree reasonably well, both for moderate and for strong wind conditions, indicating that the effect of gravity has to be taken into account for a predictive performance simulation.",BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS
D06290,"This work studies the problem of content-based image retrieval, specifically, texture retrieval.$$$It focuses on feature extraction and similarity measure for texture images.$$$Our approach employs a recently developed method, the so-called Scattering transform, for the process of feature extraction in texture retrieval.$$$It shares a distinctive property of providing a robust representation, which is stable with respect to spatial deformations.$$$Recent work has demonstrated its capability for texture classification, and hence as a promising candidate for the problem of texture retrieval.$$$Moreover, we adopt a common approach of measuring the similarity of textures by comparing the subband histograms of a filterbank transform.$$$To this end we derive a similarity measure based on the popular Bhattacharyya Kernel.$$$Despite the popularity of describing histograms using parametrized probability density functions, such as the Generalized Gaussian Distribution, it is unfortunately not applicable for describing most of the Scattering transform subbands, due to the complex modulus performed on each one of them.$$$In this work, we propose to use the Weibull distribution to model the Scattering subbands of descendant layers.$$$Our numerical experiments demonstrated the effectiveness of the proposed approach, in comparison with several state of the arts.",BACKGROUND/OBJECTIVES OBJECTIVES METHODS BACKGROUND BACKGROUND METHODS METHODS BACKGROUND/OBJECTIVES OBJECTIVES RESULTS
D00082,"We focus on adversarial patrolling games on arbitrary graphs, where the Defender can control a mobile resource, the targets are alarmed by an alarm system, and the Attacker can observe the actions of the mobile resource of the Defender and perform different attacks exploiting multiple resources.$$$This scenario can be modeled as a zero-sum extensive-form game in which each player can play multiple times.$$$The game tree is exponentially large both in the size of the graph and in the number of attacking resources.$$$We show that when the number of the Attacker's resources is free, the problem of computing the equilibrium path is NP-hard, while when the number of resources is fixed, the equilibrium path can be computed in poly-time.$$$We provide a dynamic-programming algorithm that, given the number of the Attacker's resources, computes the equilibrium path requiring poly-time in the size of the graph and exponential time in the number of the resources.$$$Furthermore, since in real-world scenarios it is implausible that the Defender knows the number of attacking resources, we study the robustness of the Defender's strategy when she makes a wrong guess about that number.$$$We show that even the error of just a single resource can lead to an arbitrary inefficiency, when the inefficiency is defined as the ratio of the Defender's utilities obtained with a wrong guess and a correct guess.$$$However, a more suitable definition of inefficiency is given by the difference of the Defender's utilities: this way, we observe that the higher the error in the estimation, the higher the loss for the Defender.$$$Then, we investigate the performance of online algorithms when no information about the Attacker's resources is available.$$$Finally, we resort to randomized online algorithms showing that we can obtain a competitive factor that is twice better than the one that can be achieved by any deterministic online algorithm.",BACKGROUND BACKGROUND/METHODS RESULTS OBJECTIVES/RESULTS METHODS/RESULTS BACKGROUND/OBJECTIVES RESULTS BACKGROUND OBJECTIVES METHODS/RESULTS/CONCLUSIONS
D04228,"We introduce new diversification methods for zero-one optimization that significantly extend strategies previously introduced in the setting of metaheuristic search.$$$Our methods incorporate easily implemented strategies for partitioning assignments of values to variables, accompanied by processes called augmentation and shifting which create greater flexibility and generality.$$$We then show how the resulting collection of diversified solutions can be further diversified by means of permutation mappings, which equally can be used to generate diversified collections of permutations for applications such as scheduling and routing.$$$These methods can be applied to non-binary vectors by the use of binarization procedures and by Diversification-Based Learning (DBL) procedures which also provide connections to applications in clustering and machine learning.$$$Detailed pseudocode and numerical illustrations are provided to show the operation of our methods and the collections of solutions they create.",OBJECTIVES/RESULTS METHODS RESULTS RESULTS/CONCLUSIONS RESULTS
D01203,"Information transmission over channels with transceiver distortion is investigated via generalized mutual information (GMI) under Gaussian input distribution and nearest-neighbor decoding.$$$A canonical transceiver structure in which the channel output is processed by a minimum mean-squared error estimator before decoding is established to maximize the GMI, and the well-known Bussgang's decomposition is shown to be a heuristic that is consistent with the GMI under linear output processing.",OBJECTIVES/METHODS METHODS/RESULTS/CONCLUSIONS
D03728,"Methods from computational topology are becoming more and more popular in computer vision and have shown to improve the state-of-the-art in several tasks.$$$In this paper, we investigate the applicability of topological descriptors in the context of 3D surface analysis for the classification of different surface textures.$$$We present a comprehensive study on topological descriptors, investigate their robustness and expressiveness and compare them with state-of-the-art methods including Convolutional Neural Networks (CNNs).$$$Results show that class-specific information is reflected well in topological descriptors.$$$The investigated descriptors can directly compete with non-topological descriptors and capture complementary information.$$$As a consequence they improve the state-of-the-art when combined with non-topological descriptors.",BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS CONCLUSIONS
D01674,"An examination of object recognition challenge leaderboards (ILSVRC, PASCAL-VOC) reveals that the top-performing classifiers typically exhibit small differences amongst themselves in terms of error rate/mAP.$$$To better differentiate the top performers, additional criteria are required.$$$Moreover, the (test) images, on which the performance scores are based, predominantly contain fully visible objects.$$$Therefore, `harder' test images, mimicking the challenging conditions (e.g. occlusion) in which humans routinely recognize objects, need to be utilized for benchmarking.$$$To address the concerns mentioned above, we make two contributions.$$$First, we systematically vary the level of local object-part content, global detail and spatial context in images from PASCAL VOC 2010 to create a new benchmarking dataset dubbed PPSS-12.$$$Second, we propose an object-part based benchmarking procedure which quantifies classifiers' robustness to a range of visibility and contextual settings.$$$The benchmarking procedure relies on a semantic similarity measure that naturally addresses potential semantic granularity differences between the category labels in training and test datasets, thus eliminating manual mapping.$$$We use our procedure on the PPSS-12 dataset to benchmark top-performing classifiers trained on the ILSVRC-2012 dataset.$$$Our results show that the proposed benchmarking procedure enables additional differentiation among state-of-the-art object classifiers in terms of their ability to handle missing content and insufficient object detail.$$$Given this capability for additional differentiation, our approach can potentially supplement existing benchmarking procedures used in object recognition challenge leaderboards.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OTHERS OBJECTIVES OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D04576,"We propose a novel reflection color model consisting of body essence and (mixed) neuter, and present an effective method for separating dichromatic reflection components using a single image.$$$Body essence is an entity invariant to interface reflection, and has two degrees of freedom unlike hue and maximum chromaticity.$$$As a result, the proposed method is insensitive to noise and proper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red, green, and blue), contrary to the maximum chromaticity-based methods.$$$Interface reflection is separated by using a Gaussian function, which removes a critical thresholding problem.$$$Furthermore, the method does not require any region segmentation.$$$Experimental results show the efficacy of the proposed model and method.",OBJECTIVES/CONCLUSIONS CONCLUSIONS RESULTS/CONCLUSIONS METHODS/CONCLUSIONS RESULTS RESULTS
D00024,"This paper explores the potential of extreme learning machine based supervised classification algorithm for land cover classification.$$$In comparison to a backpropagation neural network, which requires setting of several user-defined parameters and may produce local minima, extreme learning machine require setting of one parameter and produce a unique solution.$$$ETM+ multispectral data set (England) was used to judge the suitability of extreme learning machine for remote sensing classifications.$$$A back propagation neural network was used to compare its performance in term of classification accuracy and computational cost.$$$Results suggest that the extreme learning machine perform equally well to back propagation neural network in term of classification accuracy with this data set.$$$The computational cost using extreme learning machine is very small in comparison to back propagation neural network.",BACKGROUND BACKGROUND/OBJECTIVES/METHODS OBJECTIVES OBJECTIVES/METHODS RESULTS CONCLUSIONS
D04668,"Opportunistic detection rules (ODRs) are variants of fixed-sample-size detection rules in which the statistician is allowed to make an early decision on the alternative hypothesis opportunistically based on the sequentially observed samples.$$$From a sequential decision perspective, ODRs are also mixtures of one-sided and truncated sequential detection rules.$$$Several results regarding ODRs are established in this paper.$$$In the finite regime, the maximum sample size is modeled either as a fixed finite number, or a geometric random variable with a fixed finite mean.$$$For both cases, the corresponding Bayesian formulations are investigated.$$$The former case is a slight variation of the well-known finite-length sequential hypothesis testing procedure in the literature, whereas the latter case is new, for which the Bayesian optimal ODR is shown to be a sequence of likelihood ratio threshold tests with two different thresholds: a running threshold, which is determined by solving a stationary state equation, is used when future samples are still available, and a terminal threshold (simply the ratio between the priors scaled by costs) is used when the statistician reaches the final sample and thus has to make a decision immediately.$$$In the asymptotic regime, the tradeoff among the exponents of the (false alarm and miss) error probabilities and the normalized expected stopping time under the alternative hypothesis is completely characterized and proved to be tight, via an information-theoretic argument.$$$Within the tradeoff region, one noteworthy fact is that the performance of the Stein-Chernoff Lemma is attainable by ODRs.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05780,"The recovery type error estimators introduced by Zienkiewicz and Zhu use a recovered stress field evaluated from the Finite Element (FE) solution.$$$Their accuracy depends on the quality of the recovered field.$$$In this sense, accurate results are obtained using recovery procedures based on the Superconvergent Patch recovery technique (SPR).$$$These error estimators can be easily implemented and provide accurate estimates.$$$Another important feature is that the recovered solution is of a better quality than the FE solution and can therefore be used as an enhanced solution.$$$We have developed an SPR-type recovery technique that considers equilibrium and displacements constraints to obtain a very accurate recovered displacements field from which a recovered stress field can also be evaluated.$$$We propose the use of these recovered fields as the standard output of the FE code instead of the raw FE solution.$$$Techniques to quantify the error of the recovered solution are therefore needed.$$$In this report we present an error estimation technique that accurately evaluates the error of the recovered solution both at global and local levels in the FEM and XFEM frameworks.$$$We have also developed an h-adaptive mesh refinement strategy based on the error of the recovered solution.$$$As the converge rate of the error of the recovered solution is higher than that of the FE one, the computational cost required to obtain a solution with a prescribed accuracy is smaller than for traditional h-adaptive processes.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND RESULTS METHODS/RESULTS CONCLUSIONS OBJECTIVES RESULTS/CONCLUSIONS RESULTS RESULTS/CONCLUSIONS
D01006,"The blooming availability of traces for social, biological, and communication networks opens up unprecedented opportunities in analyzing diffusion processes in networks.$$$However, the sheer sizes of the nowadays networks raise serious challenges in computational efficiency and scalability.$$$In this paper, we propose a new hyper-graph sketching framework for inflence dynamics in networks.$$$The central of our sketching framework, called SKIS, is an efficient importance sampling algorithm that returns only non-singular reverse cascades in the network.$$$Comparing to previously developed sketches like RIS and SKIM, our sketch significantly enhances estimation quality while substantially reducing processing time and memory-footprint.$$$Further, we present general strategies of using SKIS to enhance existing algorithms for influence estimation and influence maximization which are motivated by practical applications like viral marketing.$$$Using SKIS, we design high-quality influence oracle for seed sets with average estimation error up to 10x times smaller than those using RIS and 6x times smaller than SKIM.$$$In addition, our influence maximization using SKIS substantially improves the quality of solutions for greedy algorithms.$$$It achieves up to 10x times speed-up and 4x memory reduction for the fastest RIS-based DSSA algorithm, while maintaining the same theoretical guarantees.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS METHODS METHODS/RESULTS RESULTS RESULTS
D06769,"We took part in the YouTube-8M Video Understanding Challenge hosted on Kaggle, and achieved the 10th place within less than one month's time.$$$In this paper, we present an extensive analysis and solution to the underlying machine-learning problem based on frame-level data, where major challenges are identified and corresponding preliminary methods are proposed.$$$It's noteworthy that, with merely the proposed strategies and uniformly-averaging multi-crop ensemble was it sufficient for us to reach our ranking.$$$We also report the methods we believe to be promising but didn't have enough time to train to convergence.$$$We hope this paper could serve, to some extent, as a review and guideline of the YouTube-8M multi-label video classification benchmark, inspiring future attempts and research.",BACKGROUND/RESULTS OBJECTIVES/METHODS METHODS METHODS CONCLUSIONS
D02393,"Dedicated Short Range Communication (DSRC) was designed to provide reliable wireless communication for intelligent transportation system applications.$$$Sharing information among cars and between cars and the infrastructure, pedestrians, or ""the cloud"" has great potential to improve safety, mobility and fuel economy.$$$DSRC is being considered by the US Department of Transportation to be required for ground vehicles.$$$In the past, their performance has been assessed thoroughly in the labs and limited field testing, but not on a large fleet.$$$In this paper, we present the analysis of DSRC performance using data from the world's largest connected vehicle test program - Safety Pilot Model Deployment lead by the University of Michigan.$$$We first investigate their maximum and effective range, and then study the effect of environmental factors, such as trees/foliage, weather, buildings, vehicle travel direction, and road elevation.$$$The results can be used to guide future DSRC equipment placement and installation, and can be used to develop DSRC communication models for numerical simulations.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS CONCLUSIONS
D02425,In this paper we study the MOR cryptosystem using finite classical Chevalley groups over a finite field of odd characteristic.$$$In the process we develop an algorithm for these Chevalley groups in the same spirit as the row-column operation for special linear group.$$$We focus our study on orthogonal and symplectic groups.$$$We find the hardness of the proposed MOR cryptosystem for these groups.,OBJECTIVES METHODS RESULTS CONCLUSIONS
D00654,"Optimization is becoming a crucial element in industrial applications involving sustainable alternative energy systems.$$$During the design of such systems, the engineer/decision maker would often encounter noise factors (e.g. solar insolation and ambient temperature fluctuations) when their system interacts with the environment.$$$In this chapter, the sizing and design optimization of the solar powered irrigation system was considered.$$$This problem is multivariate, noisy, nonlinear and multiobjective.$$$This design problem was tackled by first using the Fuzzy Type II approach to model the noise factors.$$$Consequently, the Bacterial Foraging Algorithm (BFA) (in the context of a weighted sum framework) was employed to solve this multiobjective fuzzy design problem.$$$This method was then used to construct the approximate Pareto frontier as well as to identify the best solution option in a fuzzy setting.$$$Comprehensive analyses and discussions were performed on the generated numerical results with respect to the implemented solution methods.",BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES BACKGROUND METHODS METHODS METHODS RESULTS/CONCLUSIONS
D01618,"Boltzmann-Gibbs distribution arises as the statistical equilibrium probability distribution of money among the agents of a closed economic system where random and undirected exchanges are allowed.$$$When considering a model with uniform savings in the exchanges, the final distribution is close to the gamma family.$$$In this work, we implement these exchange rules on networks and we find that these stationary probability distributions are robust and they are not affected by the topology of the underlying network.$$$We introduce a new family of interactions: random but directed ones.$$$In this case, it is found the topology to be determinant and the mean money per economic agent is related to the degree of the node representing the agent in the network.$$$The relation between the mean money per economic agent and its degree is shown to be linear.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS
D06979,"The SINTAGMA information integration system is an infrastructure for accessing several different information sources together.$$$Besides providing a uniform interface to the information sources (databases, web services, web sites, RDF resources, XML files), semantic integration is also needed.$$$Semantic integration is carried out by providing a high-level model and the mappings to the models of the sources.$$$When executing a query of the high level model, a query is transformed to a low-level query plan, which is a piece of Prolog code that answers the high-level query.$$$This transformation is done in two phases.$$$First, the Query Planner produces a plan as a logic formula expressing the low-level query.$$$Next, the Query Optimizer transforms this formula to executable Prolog code and optimizes it according to structural and statistical information about the information sources.$$$This article discusses the main ideas of the optimization algorithm and its implementation.",RESULTS OBJECTIVES METHODS METHODS OTHERS METHODS METHODS RESULTS
D03028,"Modal analysis is the process of estimating a system's modal parameters such as its natural frequencies and mode shapes.$$$One application of modal analysis is in structural health monitoring (SHM), where a network of sensors may be used to collect vibration data from a physical structure such as a building or bridge.$$$There is a growing interest in developing automated techniques for SHM based on data collected in a wireless sensor network.$$$In order to conserve power and extend battery life, however, it is desirable to minimize the amount of data that must be collected and transmitted in such a sensor network.$$$In this paper, we highlight the fact that modal analysis can be formulated as an atomic norm minimization (ANM) problem, which can be solved efficiently and in some cases recover perfectly a structure's mode shapes and frequencies.$$$We survey a broad class of sampling and compression strategies that one might consider in a physical sensor network, and we provide bounds on the sample complexity of these compressive schemes in order to recover a structure's mode shapes and frequencies via ANM.$$$A main contribution of our paper is to establish a bound on the sample complexity of modal analysis with random temporal compression, and in this scenario we prove that the samples per sensor can actually decrease as the number of sensors increases.$$$We also extend an atomic norm denoising problem to the multiple measurement vector (MMV) setting in the case of uniform sampling.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS/RESULTS METHODS/RESULTS RESULTS/CONCLUSIONS METHODS/RESULTS/CONCLUSIONS
D02977,"Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics.$$$The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts.$$$In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data.$$$Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks.$$$In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner.$$$We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D01177,"We study robust distributed learning that involves minimizing a non-convex loss function with saddle points.$$$We consider the Byzantine setting where some worker machines have abnormal or even arbitrary and adversarial behavior.$$$In this setting, the Byzantine machines may create fake local minima near a saddle point that is far away from any true local minimum, even when robust gradient estimators are used.$$$We develop ByzantinePGD, a robust first-order algorithm that can provably escape saddle points and fake local minima, and converge to an approximate true local minimizer with low iteration complexity.$$$As a by-product, we give a simpler algorithm and analysis for escaping saddle points in the usual non-Byzantine setting.$$$We further discuss three robust gradient estimators that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering.$$$We characterize their performance in concrete statistical settings, and argue for their near-optimality in low and high dimensional regimes.",BACKGROUND OBJECTIVES OTHERS METHODS/RESULTS RESULTS RESULTS RESULTS
D01894,"Rapport, the close and harmonious relationship in which interaction partners are ""in sync"" with each other, was shown to result in smoother social interactions, improved collaboration, and improved interpersonal outcomes.$$$In this work, we are first to investigate automatic prediction of low rapport during natural interactions within small groups.$$$This task is challenging given that rapport only manifests in subtle non-verbal signals that are, in addition, subject to influences of group dynamics as well as inter-personal idiosyncrasies.$$$We record videos of unscripted discussions of three to four people using a multi-view camera system and microphones.$$$We analyse a rich set of non-verbal signals for rapport detection, namely facial expressions, hand motion, gaze, speaker turns, and speech prosody.$$$Using facial features, we can detect low rapport with an average precision of 0.7 (chance level at 0.25), while incorporating prior knowledge of participants' personalities can even achieve early prediction without a drop in performance.$$$We further provide a detailed analysis of different feature sets and the amount of information contained in different temporal segments of the interactions.",BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS METHODS/RESULTS OBJECTIVES/RESULTS
D00077,"The residual neural network (ResNet) is a popular deep network architecture which has the ability to obtain high-accuracy results on several image processing problems.$$$In order to analyze the behavior and structure of ResNet, recent work has been on establishing connections between ResNets and continuous-time optimal control problems.$$$In this work, we show that the post-activation ResNet is related to an optimal control problem with differential inclusions, and provide continuous-time stability results for the differential inclusion associated with ResNet.$$$Motivated by the stability conditions, we show that alterations of either the architecture or the optimization problem can generate variants of ResNet which improve the theoretical stability bounds.$$$In addition, we establish stability bounds for the full (discrete) network associated with two variants of ResNet, in particular, bounds on the growth of the features and a measure of the sensitivity of the features with respect to perturbations.$$$These results also help to show the relationship between the depth, regularization, and stability of the feature space.$$$Computational experiments on the proposed variants show that the accuracy of ResNet is preserved and that the accuracy seems to be monotone with respect to the depth and various corruptions.",BACKGROUND BACKGROUND RESULTS RESULTS RESULTS OTHERS OTHERS
D00324,"One major challenge in training Deep Neural Networks is preventing overfitting.$$$Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data.$$$In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization.$$$Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations.$$$This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning.$$$Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.",BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES METHODS/RESULTS METHODS/RESULTS BACKGROUND RESULTS/CONCLUSIONS
D05510,"When the training data in a two-class classification problem is overwhelmed by one class, most classification techniques fail to correctly identify the data points belonging to the underrepresented class.$$$We propose Similarity-based Imbalanced Classification (SBIC) that learns patterns in the training data based on an empirical similarity function.$$$To take the imbalanced structure of the training data into account, SBIC utilizes the concept of absent data, i.e. data from the minority class which can help better find the boundary between the two classes.$$$SBIC simultaneously optimizes the weights of the empirical similarity function and finds the locations of absent data points.$$$As such, SBIC uses an embedded mechanism for synthetic data generation which does not modify the training dataset, but alters the algorithm to suit imbalanced datasets.$$$Therefore, SBIC uses the ideas of both major schools of thoughts in imbalanced classification: Like cost-sensitive approaches SBIC operates on an algorithm level to handle imbalanced structures; and similar to synthetic data generation approaches, it utilizes the properties of unobserved data points from the minority class.$$$The application of SBIC to imbalanced datasets suggests it is comparable to, and in some cases outperforms, other commonly used classification techniques for imbalanced datasets.",BACKGROUND OBJECTIVES/METHODS METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D03471,"Fundamental experimental measurements of quantities such as ignition delay times, laminar flame speeds, and species profiles (among others) serve important roles in understanding fuel chemistry and validating chemical kinetic models.$$$However, despite both the importance and abundance of such information in the literature, the community lacks a widely adopted standard format for this data.$$$This impedes both sharing and wide use by the community.$$$Here we introduce a new chemical kinetics experimental data format, ChemKED, and the related Python-based package for validating and working with ChemKED-formatted files called PyKED.$$$We also review past and related efforts, and motivate the need for a new solution.$$$ChemKED currently supports the representation of autoignition delay time measurements from shock tubes and rapid compression machines.$$$ChemKED-formatted files contain all of the information needed to simulate experimental data points, including the uncertainty of the data.$$$ChemKED is based on the YAML data serialization language, and is intended as a human- and machine-readable standard for easy creation and automated use.$$$Development of ChemKED and PyKED occurs openly on GitHub under the BSD 3-clause license, and contributions from the community are welcome.$$$Plans for future development include support for experimental data from laminar flame, jet stirred reactor, and speciation measurements.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES BACKGROUND METHODS METHODS METHODS METHODS CONCLUSIONS
D05131,"Recently, merging signal processing techniques with information security services has found a lot of attention.$$$Steganography and steganalysis are among those trends.$$$Like their counterparts in cryptology, steganography and steganalysis are in a constant battle.$$$Steganography methods try to hide the presence of covert messages in innocuous-looking data, whereas steganalysis methods try to reveal existence of such messages and to break steganography methods.$$$The stream nature of audio signals, their popularity, and their wide spread usage make them very suitable media for steganography.$$$This has led to a very rich literature on both steganography and steganalysis of audio signals.$$$This paper intends to conduct a comprehensive review of audio steganalysis methods aggregated over near fifteen years.$$$Furthermore, we implement some of the most recent audio steganalysis methods and conduct a comparative analysis on their performances.$$$Finally, the paper provides some possible directions for future researches on audio steganalysis.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS METHODS
D02591,"We introduce a broad class of random graph models: the generalised hypergeometric ensemble (GHypEG).$$$This class enables to solve some long standing problems in random graph theory.$$$First, GHypEG provides an elegant and compact formulation of the well-known configuration model in terms of an urn problem.$$$Second, GHypEG allows to incorporate arbitrary tendencies to connect different vertex pairs.$$$Third, we present the closed-form expressions of the associated probability distribution ensures the analytical tractability of our formulation.$$$This is in stark contrast with the previous state-of-the-art, which is to implement the configuration model by means of computationally expensive procedures.",OBJECTIVES/METHODS BACKGROUND/CONCLUSIONS RESULTS RESULTS RESULTS BACKGROUND/RESULTS/CONCLUSIONS
D03190,"We equip dynamic geometry software (DGS) with a user-friendly method that enables massively parallel calculations on the graphics processing unit (GPU).$$$This interplay of DGS and GPU opens up various applications in education and mathematical research.$$$The GPU-aided discovery of mathematical properties, interactive visualizations of algebraic surfaces (raycasting), the mathematical deformation of images and footage in real-time, and computationally demanding numerical simulations of PDEs are examples from the long and versatile list of new domains that our approach makes accessible within a DGS.$$$We ease the development of complex (mathematical) visualizations and provide a rapid-prototyping scheme for general-purpose computations (GPGPU).$$$The possibility to program both CPU and GPU with the use of only one high-level (scripting) programming language is a crucial aspect of our concept.$$$We embed shader programming seamlessly within a high-level (scripting) programming environment.$$$The aforementioned requires the symbolic process of the transcompilation of a high-level programming language into shader programming language for GPU and, in this article, we address the challenge of the automatic translation of a high-level programming language to a shader language of the GPU.$$$To maintain platform independence and the possibility to use our technology on modern devices, we focus on a realization through WebGL.",OBJECTIVES RESULTS RESULTS OBJECTIVES METHODS METHODS METHODS METHODS
D01250,"While deep learning models and techniques have achieved great empirical success, our understanding of the source of success in many aspects remains very limited.$$$In an attempt to bridge the gap, we investigate the decision boundary of a production deep learning architecture with weak assumptions on both the training data and the model.$$$We demonstrate, both theoretically and empirically, that the last weight layer of a neural network converges to a linear SVM trained on the output of the last hidden layer, for both the binary case and the multi-class case with the commonly used cross-entropy loss.$$$Furthermore, we show empirically that training a neural network as a whole, instead of only fine-tuning the last weight layer, may result in better bias constant for the last weight layer, which is important for generalization.$$$In addition to facilitating the understanding of deep learning, our result can be helpful for solving a broad range of practical problems of deep learning, such as catastrophic forgetting and adversarial attacking.$$$The experiment codes are available at https://github.com/lykaust15/NN_decision_boundary",BACKGROUND OBJECTIVES RESULTS RESULTS CONCLUSIONS OTHERS
D05699,"This paper describes LIUM submissions to WMT17 News Translation Task for English-German, English-Turkish, English-Czech and English-Latvian language pairs.$$$We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework.$$$Competitive scores were obtained by ensembling various systems and exploiting the availability of target monolingual corpora for back-translation.$$$The impact of back-translation quantity and quality is also analyzed for English-Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.",BACKGROUND METHODS METHODS/RESULTS/CONCLUSIONS RESULTS
D04247,"Wireless networking allows users to access information and services regardless of location and physical infrastructure.$$$It is a fast growing technology due to its availability of wireless devices, flexibility, ease of installation and configuration.$$$With this rapid expansion of information and Communication Technology (ICT), the consumption of energy is also increasing.$$$In the early age of wireless technology, computing infrastructure focused on everywhere access, capacity and speed of technology.$$$But now computing infrastructure should be energy efficient because, in wireless networking, devices are mostly powered by a battery that is a limited source of energy and is a challenge for the researchers.$$$In computing infrastructure energy saving and environmental protection has become a global demand.$$$This paper proposed a computing infrastructure based on green computing for energy efficient wireless networking.$$$Further, some challenges and techniques like power consumption in network architecture, algorithm efficiency, virtualization, and dynamic power saving will be discussed to make energy efficient computing infrastructure.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OTHERS OBJECTIVES OBJECTIVES/RESULTS/CONCLUSIONS OBJECTIVES/RESULTS/CONCLUSIONS
D02082,"We present a Few-Shot Relation Classification Dataset (FewRel), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers.$$$The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers.$$$We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct a thorough evaluation of these methods.$$$Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans.$$$We also show that a range of different reasoning skills are needed to solve our task.$$$These results indicate that few-shot relation classification remains an open problem and still requires further research.$$$Our detailed analysis points multiple directions for future research.$$$All details and resources about the dataset and baselines are released on http://zhuhao.me/fewrel.",OBJECTIVES METHODS METHODS RESULTS RESULTS CONCLUSIONS CONCLUSIONS OTHERS
D05507,"Over the last few decades, the player recruitment process in professional football has evolved into a multi-billion industry and has thus become of vital importance.$$$To gain insights into the general level of their candidate reinforcements, many professional football clubs have access to extensive video footage and advanced statistics.$$$However, the question whether a given player would fit the team's playing style often still remains unanswered.$$$In this paper, we aim to bridge that gap by proposing a set of 21 player roles and introducing a method for automatically identifying the most applicable roles for each player from play-by-play event data collected during matches.",BACKGROUND BACKGROUND OBJECTIVES METHODS
D06943,"A critical analysis of the feasibility of reversible computing is performed.$$$The key question is: Is it possible to build a completely reversible computer?$$$A closer look into the internal aspects of the reversible computing as well as the external constraints such as the second law of thermodynamics has demonstrated that several difficulties would have to be solved before reversible computer is being built.$$$It is shown that a conventional reversible computer would require energy for setting up the reversible inputs from irreversible signals, for the reading out of the reversible outputs, for the transport of the information between logic elements and finally for the control signals that will require more energy dissipating into the environment.$$$A loose bound on the minimum amount of energy required to be dissipated during the physical implementation of a reversible computer is obtained and a generalization of the principles for reversible computing is provided.",BACKGROUND RESULTS OBJECTIVES METHODS CONCLUSIONS
D04016,"In this article we show the duality between tensor networks and undirected graphical models with discrete variables.$$$We study tensor networks on hypergraphs, which we call tensor hypernetworks.$$$We show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph.$$$We translate various notions under duality.$$$For example, marginalization in a graphical model is dual to contraction in the tensor network.$$$Algorithms also translate under duality.$$$We show that belief propagation corresponds to a known algorithm for tensor network contraction.$$$This article is a reminder that the research areas of graphical models and tensor networks can benefit from interaction.",OBJECTIVES BACKGROUND RESULTS METHODS RESULTS METHODS RESULTS CONCLUSIONS
D01237,"Learning about the social structure of hidden and hard-to-reach populations --- such as drug users and sex workers --- is a major goal of epidemiological and public health research on risk behaviors and disease prevention.$$$Respondent-driven sampling (RDS) is a peer-referral process widely used by many health organizations, where research subjects recruit other subjects from their social network.$$$In such surveys, researchers observe who recruited whom, along with the time of recruitment and the total number of acquaintances (network degree) of respondents.$$$However, due to privacy concerns, the identities of acquaintances are not disclosed.$$$In this work, we show how to reconstruct the underlying network structure through which the subjects are recruited.$$$We formulate the dynamics of RDS as a continuous-time diffusion process over the underlying graph and derive the likelihood for the recruitment time series under an arbitrary recruitment time distribution.$$$We develop an efficient stochastic optimization algorithm called RENDER (REspoNdent-Driven nEtwork Reconstruction) that finds the network that best explains the collected data.$$$We support our analytical results through an exhaustive set of experiments on both synthetic and real data.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D04279,"This paper considers the task of thorax disease classification on chest X-ray images.$$$Existing methods generally use the global image as input for network learning.$$$Such a strategy is limited in two aspects.$$$1) A thorax disease usually happens in (small) localized areas which are disease specific.$$$Training CNNs using global image may be affected by the (excessive) irrelevant noisy areas.$$$2) Due to the poor alignment of some CXR images, the existence of irregular borders hinders the network performance.$$$In this paper, we address the above problems by proposing a three-branch attention guided convolution neural network (AG-CNN).$$$AG-CNN 1) learns from disease-specific regions to avoid noise and improve alignment, 2) also integrates a global branch to compensate the lost discriminative cues by local branch.$$$Specifically, we first learn a global CNN branch using global images.$$$Then, guided by the attention heat map generated from the global branch, we inference a mask to crop a discriminative region from the global image.$$$The local region is used for training a local CNN branch.$$$Lastly, we concatenate the last pooling layers of both the global and local branches for fine-tuning the fusion branch.$$$The Comprehensive experiment is conducted on the ChestX-ray14 dataset.$$$We first report a strong global baseline producing an average AUC of 0.841 with ResNet-50 as backbone.$$$After combining the local cues with the global information, AG-CNN improves the average AUC to 0.868.$$$While DenseNet-121 is used, the average AUC achieves 0.871, which is a new state of the art in the community.",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D02438,We give a description of the weighted Reed-Muller codes over a prime field in a modular algebra.$$$A description of the homogeneous Reed-Muller codes in the same ambient space is presented for the binary case.$$$A decoding procedure using the Landrock-Manz method is developed.,OBJECTIVES OBJECTIVES OBJECTIVES/METHODS
D00265,"This is the preprint version of our paper on ICWL2015.$$$A virtual reality based enhanced technology for learning primary geography is proposed, which synthesizes several latest information technologies including virtual reality(VR), 3D geographical information system(GIS), 3D visualization and multimodal human-computer-interaction (HCI).$$$The main functions of the proposed system are introduced, i.e.$$$Buffer analysis, Overlay analysis, Space convex hull calculation, Space convex decomposition, 3D topology analysis and 3D space intersection detection.$$$The multimodal technologies are employed in the system to enhance the immersive perception of the users.",BACKGROUND METHODS OBJECTIVES OBJECTIVES RESULTS
D03510,"This work presents a supervised learning based approach to the computer vision problem of frame interpolation.$$$The presented technique could also be used in the cartoon animations since drawing each individual frame consumes a noticeable amount of time.$$$The most existing solutions to this problem use unsupervised methods and focus only on real life videos with already high frame rate.$$$However, the experiments show that such methods do not work as well when the frame rate becomes low and object displacements between frames becomes large.$$$This is due to the fact that interpolation of the large displacement motion requires knowledge of the motion structure thus the simple techniques such as frame averaging start to fail.$$$In this work the deep convolutional neural network is used to solve the frame interpolation problem.$$$In addition, it is shown that incorporating the prior information such as optical flow improves the interpolation quality significantly.",BACKGROUND BACKGROUND BACKGROUND RESULTS RESULTS OBJECTIVES METHODS
D02352,"This paper addresses the problem of automated vehicle tracking and recognition from aerial image sequences.$$$Motivated by its successes in the existing literature focus on the use of linear appearance subspaces to describe multi-view object appearance and highlight the challenges involved in their application as a part of a practical system.$$$A working solution which includes steps for data extraction and normalization is described.$$$In experiments on real-world data the proposed methodology achieved promising results with a high correct recognition rate and few, meaningful errors (type II errors whereby genuinely similar targets are sometimes being confused with one another).$$$Directions for future research and possible improvements of the proposed method are discussed.",OBJECTIVES BACKGROUND/METHODS METHODS RESULTS/CONCLUSIONS OTHERS
D02677,"Attention distributions of the generated translations are a useful bi-product of attention-based recurrent neural network translation models and can be treated as soft alignments between the input and output tokens.$$$In this work, we use attention distributions as a confidence metric for output translations.$$$We present two strategies of using the attention distributions: filtering out bad translations from a large back-translated corpus, and selecting the best translation in a hybrid setup of two different translation systems.$$$While manual evaluation indicated only a weak correlation between our confidence score and human judgments, the use-cases showed improvements of up to 2.22 BLEU points for filtering and 0.99 points for hybrid translation, tested on English<->German and English<->Latvian translation.",BACKGROUND METHODS METHODS RESULTS
D00085,"Feedback mechanism based algorithms are frequently used to solve network optimization problems.$$$These schemes involve users and network exchanging information (e.g. requests for bandwidth allocation and pricing) to achieve convergence towards an optimal solution.$$$However, in the implementation, these algorithms do not guarantee that messages will be delivered to the destination when network congestion occurs.$$$This in turn often results in packet drops, which may cause information loss, and this condition may lead to algorithm failing to converge.$$$To prevent this failure, we propose least square (LS) estimation algorithm to recover the missing information when packets are dropped from the network.$$$The simulation results involving several scenarios demonstrate that LS estimation can provide the convergence for feedback mechanism based algorithm.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D03377,"We propose a new learning-based method for estimating 2D human pose from a single image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN).$$$Recently, many methods have been developed to estimate human pose by using pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective.$$$In this paper, we propose to integrate both the local (body) part appearance and the holistic view of each local part for more accurate human pose estimation.$$$Specifically, the proposed DS-CNN takes a set of image patches (category-independent object proposals for training and multi-scale sliding windows for testing) as the input and then learns the appearance of each local part by considering their holistic views in the full body.$$$Using DS-CNN, we achieve both joint detection, which determines whether an image patch contains a body joint, and joint localization, which finds the exact location of the joint in the image patch.$$$Finally, we develop an algorithm to combine these joint detection/localization results from all the image patches for estimating the human pose.$$$The experimental results show the effectiveness of the proposed method by comparing to the state-of-the-art human-pose estimation methods based on pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective.",OBJECTIVES/METHODS BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS/CONCLUSIONS
D02444,"Most companies' new business practices are based on customer data.$$$These practices have raised privacy concerns because of the associated risks.$$$Privacy laws require companies to gain customer consent before using their information, which stands as the biggest roadblock to monetise this asset.$$$Privacy literature suggests that reducing privacy concerns and building trust may increase individuals' intention to authorise the use of personal information.$$$Fair information practices (FIPs) are potential means to achieve this goal.$$$However, there is lack of empirical evidence on the mechanisms through which the FIPs affect privacy concerns and trust.$$$This research argues that FIPs load individuals with control, which has been found to influence privacy concerns and trust level.$$$We will use an experimental design methodology to conduct the study.$$$The results are expected to have both theoretical and managerial implications.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS METHODS/CONCLUSIONS OTHERS
D04546,"The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared.$$$Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms.$$$The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation.",BACKGROUND BACKGROUND METHODS
D06078,"We study Doob's martingale convergence theorem for computable continuous time martingales on Brownian motion, in the context of algorithmic randomness.$$$A characterization of the class of sample points for which the theorem holds is given.$$$Such points are given the name of Doob random points.$$$It is shown that a point is Doob random if its tail is computably random in a certain sense.$$$Moreover, Doob randomness is strictly weaker than computable randomness and is incomparable with Schnorr randomness.",BACKGROUND/OBJECTIVES RESULTS OTHERS RESULTS RESULTS
D02242,"As the amount of textual data has been rapidly increasing over the past decade, efficient similarity search methods have become a crucial component of large-scale information retrieval systems.$$$A popular strategy is to represent original data samples by compact binary codes through hashing.$$$A spectrum of machine learning methods have been utilized, but they often lack expressiveness and flexibility in modeling to learn effective representations.$$$The recent advances of deep learning in a wide range of applications has demonstrated its capability to learn robust and powerful feature representations for complex data.$$$Especially, deep generative models naturally combine the expressiveness of probabilistic generative models with the high capacity of deep neural networks, which is very suitable for text modeling.$$$However, little work has leveraged the recent progress in deep learning for text hashing.$$$In this paper, we propose a series of novel deep document generative models for text hashing.$$$The first proposed model is unsupervised while the second one is supervised by utilizing document labels/tags for hashing.$$$The third model further considers document-specific factors that affect the generation of words.$$$The probabilistic generative formulation of the proposed models provides a principled framework for model extension, uncertainty estimation, simulation, and interpretability.$$$Based on variational inference and reparameterization, the proposed models can be interpreted as encoder-decoder deep neural networks and thus they are capable of learning complex nonlinear distributed representations of the original documents.$$$We conduct a comprehensive set of experiments on four public testbeds.$$$The experimental results have demonstrated the effectiveness of the proposed supervised learning models for text hashing.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS RESULTS RESULTS
D05466,"As a data-centric cache-enabled architecture, Named Data Networking (NDN) is considered to be an appropriate alternative to the current host-centric IP-based Internet infrastructure.$$$Leveraging in-network caching, name-based routing, and receiver-driven sessions, NDN can greatly enhance the way Internet resources are being used.$$$A critical issue in NDN is the procedure of cache allocation and management.$$$Our main contribution in this research is the analysis of memory requirements to allocate suitable Content-Store size to NDN routers, with respect to combined impacts of long-term centrality-based metric and Exponential Weighted Moving Average (EWMA) of short-term parameters such as users behaviors and outgoing traffic.$$$To determine correlations in such large data sets, data mining methods can prove valuable to researchers.$$$In this paper, we apply a data-fusion approach, namely Principal Component Analysis (PCA), to discover relations from short- and long-term parameters of the router.$$$The output of PCA, exploited to mine out raw data sets, is used to allocate a proper cache size to the router.$$$Evaluation results show an increase in the hit ratio of Content-Stores in sources, and NDN routers.$$$Moreover, for the proposed cache size allocation scheme, the number of unsatisfied and pending Interests in NDN routers is smaller than the Degree-Centrality cache size scheme.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS CONCLUSIONS CONCLUSIONS
D01431,"A growing issue in the modern cyberspace world is the direct identification of malicious activity over network connections.$$$The boom of the machine learning industry in the past few years has led to the increasing usage of machine learning technologies, which are especially prevalent in the network intrusion detection research community.$$$When utilizing these fairly contemporary techniques, the community has realized that datasets are pivotal for identifying malicious packets and connections, particularly ones associated with information concerning labeling in order to construct learning models.$$$However, there exists a shortage of publicly available, relevant datasets to researchers in the network intrusion detection community.$$$Thus, in this paper, we introduce a method to construct labeled flow data by combining the packet meta-information with IDS logs to infer labels for intrusion detection research.$$$Specifically, we designed a NetFlow-compatible format due to the capability of a a large body of network devices, such as routers and switches, to export NetFlow records from raw traffic.$$$In doing so, the introduced method at hand would aid researchers to access relevant network flow datasets along with label information.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS CONCLUSIONS
D06593,"Cyber attacks and malware are now more prevalent than ever and the trend is ever upward.$$$There have been several approaches to attack detection including resident software applications at the root or user level, e.g., virus detection, and modifications to the OS, e.g., encryption, application signing, etc.$$$Some approaches have moved to lower level detection and preven- tion, e.g., Data Execution Prevention.$$$An emerging approach in countermeasure development is the use of hardware performance counters existing in the micro-architecture of modern processors.$$$These are at the lowest level, implemented in processor hardware, and the wealth of data collected by these counters affords some very promising countermeasures with minimal overhead as well as protection from being sabotaged themselves by attackers.$$$Here, we conduct a survey of recent techniques in realizing effective countermeasures for cyber attack detection from these hardware performance counters.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS
D04697,"Ant Colony System (ACS) is a distributed (agent- based) algorithm which has been widely studied on the Symmetric Travelling Salesman Problem (TSP).$$$The optimum parameters for this algorithm have to be found by trial and error.$$$We use a Particle Swarm Optimization algorithm (PSO) to optimize the ACS parameters working in a designed subset of TSP instances.$$$First goal is to perform the hybrid PSO-ACS algorithm on a single instance to find the optimum parameters and optimum solutions for the instance.$$$Second goal is to analyze those sets of optimum parameters, in relation to instance characteristics.$$$Computational results have shown good quality solutions for single instances though with high computational times, and that there may be sets of parameters that work optimally for a majority of instances.",BACKGROUND BACKGROUND OBJECTIVES METHODS/RESULTS METHODS/RESULTS CONCLUSIONS
D00014,"In this paper, we propose a design solution for the implementation of Virtualized Network Coding Functionality (VNCF) over a service coverage area.$$$Network Function Virtualization (NFV) and Network Coding (NC) architectural designs are integrated as a toolbox of NC design domains so that NC can be implemented over different underlying physical networks including satellite or hybrid networks.$$$The design includes identifying theoretical limits of NC over wireless networks in terms of achievable rate region and optimizing coding rates for nodes that implement VNCF.$$$The overall design target is to achieve a given multicast transmission target reliability at receiver sides.$$$In addition, the optimization problem uses databases with geo-tagged link statistics and geo-location information of network nodes in the deployment area for some computational complexity/energy constraints.$$$Numerical results provide validation of our design solution on how network conditions and system constraints impact the design and implementation of NC and how VNCF allows reliable communication over wireless networks with reliability and connectivity up to theoretical limits.",OBJECTIVES OBJECTIVES METHODS OBJECTIVES METHODS RESULTS/CONCLUSIONS
D05252,"In this paper we give an exponential lower bound for Cunningham's least recently considered (round-robin) rule as applied to parity games, Markhov decision processes and linear programs.$$$This improves a recent subexponential bound of Friedmann for this rule on these problems.$$$The round-robin rule fixes a cyclical order of the variables and chooses the next pivot variable starting from the previously chosen variable and proceeding in the given circular order.$$$It is perhaps the simplest example from the class of history-based pivot rules.$$$Our results are based on a new lower bound construction for parity games.$$$Due to the nature of the construction we are also able to obtain an exponential lower bound for the round-robin rule applied to acyclic unique sink orientations of hypercubes (AUSOs).$$$Furthermore these AUSOs are realizable as polytopes.$$$We believe these are the first such results for history based rules for AUSOs, realizable or not.$$$The paper is self-contained and requires no previous knowledge of parity games.",OBJECTIVES/RESULTS BACKGROUND/RESULTS BACKGROUND BACKGROUND METHODS RESULTS RESULTS CONCLUSIONS OTHERS
D04384,"Arabic word segmentation is essential for a variety of NLP applications such as machine translation and information retrieval.$$$Segmentation entails breaking words into their constituent stems, affixes and clitics.$$$In this paper, we compare two approaches for segmenting four major Arabic dialects using only several thousand training examples for each dialect.$$$The two approaches involve posing the problem as a ranking problem, where an SVM ranker picks the best segmentation, and as a sequence labeling problem, where a bi-LSTM RNN coupled with CRF determines where best to segment words.$$$We are able to achieve solid segmentation results for all dialects using rather limited training data.$$$We also show that employing Modern Standard Arabic data for domain adaptation and assuming context independence improve overall results.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS CONCLUSIONS
D05250,"Given n red and n blue points in general position in the plane, it is well-known that there is a perfect matching formed by non-crossing line segments.$$$We characterize the bichromatic point sets which admit exactly one non-crossing matching.$$$We give several geometric descriptions of such sets, and find an O(nlogn) algorithm that checks whether a given bichromatic set has this property.",BACKGROUND RESULTS RESULTS
D01120,"We consider the problem of learning from a similarity matrix (such as spectral clustering and lowd imensional embedding), when computing pairwise similarities are costly, and only a limited number of entries can be observed.$$$We provide a theoretical analysis using standard notions of graph approximation, significantly generalizing previous results (which focused on spectral clustering with two clusters).$$$We also propose a new algorithmic approach based on adaptive sampling, which experimentally matches or improves on previous methods, while being considerably more general and computationally cheaper.",OBJECTIVES RESULTS RESULTS
D00141,"Previous research has pointed that software applications should not depend on programmers to provide security for end-users as majority of programmers are not experts of computer security.$$$On the other hand, some studies have revealed that security experts believe programmers have a major role to play in ensuring the end-users' security.$$$However, there has been no investigation on what programmers perceive about their responsibility for the end-users' security of applications they develop.$$$In this work, by conducting a qualitative experimental study with 40 software developers, we attempted to understand the programmer's perception on who is responsible for ensuring end-users' security of the applications they develop.$$$Results revealed majority of programmers perceive that they are responsible for the end-users' security of applications they develop.$$$Furthermore, results showed that even though programmers aware of things they need to do to ensure end-users' security, they do not often follow them.$$$We believe these results would change the current view on the role that different stakeholders of the software development process (i.e. researchers, security experts, programmers and Application Programming Interface (API) developers) have to play in order to ensure the security of software applications.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS RESULTS CONCLUSIONS
D03229,"Modelling, simulation and optimization form an integrated part of modern design practice in engineering and industry.$$$Tremendous progress has been observed for all three components over the last few decades.$$$However, many challenging issues remain unresolved, and the current trends tend to use nature-inspired algorithms and surrogate-based techniques for modelling and optimization.$$$This 4th workshop on Computational Optimization, Modelling and Simulation (COMS 2013) at ICCS 2013 will further summarize the latest developments of optimization and modelling and their applications in science, engineering and industry.$$$In this review paper, we will analyse the recent trends in modelling and optimization, and their associated challenges.$$$We will discuss important topics for further research, including parameter-tuning, large-scale problems, and the gaps between theory and applications.",BACKGROUND BACKGROUND METHODS OBJECTIVES RESULTS/CONCLUSIONS CONCLUSIONS
D02129,"Graphics Processing Units allow for running massively parallel applications offloading the CPU from computationally intensive resources, however GPUs have a limited amount of memory.$$$In this paper a trie compression algorithm for massively parallel pattern matching is presented demonstrating 85% less space requirements than the original highly efficient parallel failure-less aho-corasick, whilst demonstrating over 22 Gbps throughput.$$$The algorithm presented takes advantage of compressed row storage matrices as well as shared and texture memory on the GPU.",BACKGROUND RESULTS RESULTS/CONCLUSIONS
D06981,"Hash-based message authentication codes are an extremely simple yet hugely effective construction for producing keyed message digests using shared secrets.$$$HMACs have seen widespread use as ad-hoc digital signatures in many Internet applications.$$$While messages signed with an HMAC are secure against sender impersonation and tampering in transit, if used alone they are susceptible to replay attacks.$$$We propose a construction that extends HMACs to produce a keyed message digest that has a finite validity period.$$$We then propose a message signature scheme that uses this time-dependent MAC along with an unique message identifier to calculate a set of authentication factors using which a recipient can readily detect and ignore replayed messages, thus providing perfect resistance against replay attacks.$$$We further analyse time-based message authentication codes and show that they provide stronger security guarantees than plain HMACs, even when used independently of the aforementioned replay attack resistant message signature scheme.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS
D02750,"The hierarchy of classical Chinese poetry has been broadly acknowledged by a number of studies in Chinese literature.$$$However, quantitative investigations about the evolution of classical Chinese poetry are limited.$$$The primary goal of this study is to provide quantitative evidence of the evolutionary linkages, with emphasis on word usage, among different period genres for classical Chinese poetry.$$$Specifically, various statistical analyses were performed to find and compare the patterns of word usage in the poems of nine period genres, including shi jing, chu ci, Han shi , Jin shi, Tang shi, Song shi, Yuan shi, Ming shi, and Qing shi.$$$The result of analysis indicates that each of nine period genres has unique patterns of word usage, with some Chinese characters being preferably used by the poems of a particular period genre.$$$The analysis on the general pattern of word preference implies a decreasing trend in the use of ancient Chinese characters along the timeline of dynastic types of classical Chinese poetry.$$$The phylogenetic analysis based on the distance matrix suggests that the evolution of different types of classical Chinese poetry is congruent with their chronological order, suggesting that word frequencies contain useful phylogenetic information and thus can be used to infer evolutionary linkages among various types of classical Chinese poetry.$$$The statistical analyses conducted in this study can be applied to the data sets of general Chinese literature.$$$Such analyses can provide quantitative insights about the evolution of general Chinese literature.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS RESULTS CONCLUSIONS CONCLUSIONS
D04219,"Quantum query complexity is known to be characterized by the so-called quantum adversary bound.$$$While this result has been proved in the standard discrete-time model of quantum computation, it also holds for continuous-time (or Hamiltonian-based) quantum computation, due to a known equivalence between these two query complexity models.$$$In this work, we revisit this result by providing a direct proof in the continuous-time model.$$$One originality of our proof is that it draws new connections between the adversary bound, a modern technique of theoretical computer science, and early theorems of quantum mechanics.$$$Indeed, the proof of the lower bound is based on Ehrenfest's theorem, while the upper bound relies on the adiabatic theorem, as it goes by constructing a universal adiabatic quantum query algorithm.$$$Another originality is that we use for the first time in the context of quantum computation a version of the adiabatic theorem that does not require a spectral gap.",BACKGROUND BACKGROUND RESULTS METHODS METHODS METHODS
D05596,"Named Entity Disambiguation (NED) is the task of linking a named-entity mention to an instance in a knowledge-base, typically Wikipedia.$$$This task is closely related to word-sense disambiguation (WSD), where the supervised word-expert approach has prevailed.$$$In this work we present the results of the word-expert approach to NED, where one classifier is built for each target entity mention string.$$$The resources necessary to build the system, a dictionary and a set of training instances, have been automatically derived from Wikipedia.$$$We provide empirical evidence of the value of this approach, as well as a study of the differences between WSD and NED, including ambiguity and synonymy statistics.",BACKGROUND BACKGROUND METHODS METHODS OBJECTIVES/RESULTS/CONCLUSIONS
D01029,"There have been numerous studies on the problem of flocking control for multiagent systems whose simplified models are presented in terms of point-mass elements.$$$Meanwhile, full dynamic models pose some challenging problems in addressing the flocking control problem of mobile robots due to their nonholonomic dynamic properties.$$$Taking practical constraints into consideration, we propose a novel approach to distributed flocking control of nonholonomic mobile robots by bounded feedback.$$$The flocking control objectives consist of velocity consensus, collision avoidance, and cohesion maintenance among mobile robots.$$$A flocking control protocol which is based on the information of neighbor mobile robots is constructed.$$$The theoretical analysis is conducted with the help of a Lyapunov-like function and graph theory.$$$Simulation results are shown to demonstrate the efficacy of the proposed distributed flocking control scheme.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS/RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS RESULTS
D06174,"Human-human joint-action in short-cycle repetitive handover tasks was investigated for a bottle handover task using a three-fold approach: work-methods field studies in multiple supermarkets, simulation analysis using an ergonomics software package and by conducting an in-house lab experiment on human-human collaboration by re-creating the environment and conditions of a supermarket.$$$Evaluation included both objective and subjective measures.$$$Subjective evaluation was done taking a psychological perspective and showcases among other things, the differences in the way a common joint-action is being perceived by individual team partners depending upon their role (giver or receiver).$$$The proposed approach can provide a systematic method to analyze similar tasks.$$$Combining the results of all the three analyses, this research gives insight into the science of joint-action for short-cycle repetitive tasks and its implications for human-robot collaborative system design.",OBJECTIVES/METHODS METHODS METHODS CONCLUSIONS CONCLUSIONS
D05378,"Building on the Ethernet Passive Optical Network (EPON) and Gigabit PON (GPON) standards, Next-Generation (NG) PONs (i) provide increased data rates, split ratios, wavelengths counts, and fiber lengths, as well as (ii) allow for all-optical integration of access and metro networks.$$$In this paper we provide a comprehensive probabilistic analysis of the capacity (maximum mean packet throughput) and packet delay of subnetworks that can be used to form NG-PONs.$$$Our analysis can cover a wide range of NG-PONs through taking the minimum capacity of the subnetworks making up the NG-PON and weighing the packet delays of the subnetworks.$$$Our numerical and simulation results indicate that our analysis quite accurately characterizes the throughput-delay performance of EPON/GPON tree networks, including networks upgraded with higher data rates and wavelength counts.$$$Our analysis also characterizes the trade-offs and bottlenecks when integrating EPON/GPON tree networks across a metro area with a ring, a Passive Star Coupler (PSC), or an Arrayed Waveguide Grating (AWG) for uniform and non-uniform traffic.$$$To the best of our knowledge, the presented analysis is the first to consider multiple PONs interconnected via a metro network.",BACKGROUND OBJECTIVES OBJECTIVES METHODS/RESULTS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D06065,"The proposed Earth observation (EO) based value adding system (EO VAS), hereafter identified as AutoCloud+, consists of an innovative EO image understanding system (EO IUS) design and implementation capable of automatic spatial context sensitive cloud/cloud shadow detection in multi source multi spectral (MS) EO imagery, whether or not radiometrically calibrated, acquired by multiple platforms, either spaceborne or airborne, including unmanned aerial vehicles (UAVs).$$$It is worth mentioning that the same EO IUS architecture is suitable for a large variety of EO based value adding products and services, including: (i) low level image enhancement applications, such as automatic MS image topographic correction, co registration, mosaicking and compositing, (ii) high level MS image land cover (LC) and LC change (LCC) classification and (iii) content based image storage/retrieval in massive multi source EO image databases (big data mining).",OBJECTIVES/CONCLUSIONS CONCLUSIONS
D00011,"Mobile agent networks, such as multi-UAV systems, are constrained by limited resources.$$$In particular, limited energy affects system performance directly, such as system lifetime.$$$It has been demonstrated in the wireless sensor network literature that the communication energy consumption dominates the computational and the sensing energy consumption.$$$Hence, the lifetime of the multi-UAV systems can be extended significantly by optimizing the amount of communication data, at the expense of increasing computational cost.$$$In this work, we aim at attaining an optimal trade-off between the communication and the computational energy.$$$Specifically, we propose a mixed-integer optimization formulation for a multi-hop hierarchical clustering-based self-organizing UAV network incorporating data aggregation, to obtain an energy-efficient information routing scheme.$$$The proposed framework is tested on two applications, namely target tracking and area mapping.$$$Based on simulation results, our method can significantly save energy compared to a baseline strategy, where there is no data aggregation and clustering scheme.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS/RESULTS RESULTS/CONCLUSIONS
D03167,"Recently deep neural networks based on tanh activation function have shown their impressive power in image denoising.$$$In this letter, we try to use rectifier function instead of tanh and propose a dual-pathway rectifier neural network by combining two rectifier neurons with reversed input and output weights in the same hidden layer.$$$We drive the equivalent activation function and compare it to some typical activation functions for image denoising under the same network architecture.$$$The experimental results show that our model achieves superior performances faster especially when the noise is small.",BACKGROUND OBJECTIVES/METHODS OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D05792,"Clinical medical data, especially in the intensive care unit (ICU), consist of multivariate time series of observations.$$$For each patient visit (or episode), sensor data and lab test results are recorded in the patient's Electronic Health Record (EHR).$$$While potentially containing a wealth of insights, the data is difficult to mine effectively, owing to varying length, irregular sampling and missing data.$$$Recurrent Neural Networks (RNNs), particularly those using Long Short-Term Memory (LSTM) hidden units, are powerful and increasingly popular models for learning from sequence data.$$$They effectively model varying length sequences and capture long range dependencies.$$$We present the first study to empirically evaluate the ability of LSTMs to recognize patterns in multivariate time series of clinical measurements.$$$Specifically, we consider multilabel classification of diagnoses, training a model to classify 128 diagnoses given 13 frequently but irregularly sampled clinical measurements.$$$First, we establish the effectiveness of a simple LSTM network for modeling clinical data.$$$Then we demonstrate a straightforward and effective training strategy in which we replicate targets at each sequence step.$$$Trained only on raw time series, our models outperform several strong baselines, including a multilayer perceptron trained on hand-engineered features.",BACKGROUND BACKGROUND BACKGROUND METHODS METHODS METHODS METHODS METHODS METHODS RESULTS
D01423,"The problem of visual tracking evaluation is sporting a large variety of performance measures, and largely suffers from lack of consensus about which measures should be used in experiments.$$$This makes the cross-paper tracker comparison difficult.$$$Furthermore, as some measures may be less effective than others, the tracking results may be skewed or biased towards particular tracking aspects.$$$In this paper we revisit the popular performance measures and tracker performance visualizations and analyze them theoretically and experimentally.$$$We show that several measures are equivalent from the point of information they provide for tracker comparison and, crucially, that some are more brittle than the others.$$$Based on our analysis we narrow down the set of potential measures to only two complementary ones, describing accuracy and robustness, thus pushing towards homogenization of the tracker evaluation methodology.$$$These two measures can be intuitively interpreted and visualized and have been employed by the recent Visual Object Tracking (VOT) challenges as the foundation for the evaluation methodology.",BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS CONCLUSIONS
D03044,"We review the work on data-driven grasp synthesis and the methodologies for sampling and ranking candidate grasps.$$$We divide the approaches into three groups based on whether they synthesize grasps for known, familiar or unknown objects.$$$This structure allows us to identify common object representations and perceptual processes that facilitate the employed data-driven grasp synthesis technique.$$$In the case of known objects, we concentrate on the approaches that are based on object recognition and pose estimation.$$$In the case of familiar objects, the techniques use some form of a similarity matching to a set of previously encountered objects.$$$Finally for the approaches dealing with unknown objects, the core part is the extraction of specific features that are indicative of good grasps.$$$Our survey provides an overview of the different methodologies and discusses open problems in the area of robot grasping.$$$We also draw a parallel to the classical approaches that rely on analytic formulations.",OBJECTIVES METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS
D04726,"Consider a network of k parties, each holding a long sequence of n entries (a database), with minimum vertex-cut greater than t. We show that any empirical statistic across the network of databases can be computed by each party with perfect privacy, against any set of t < k/2 passively colluding parties, such that the worst-case distortion and communication cost (in bits per database entry) both go to zero as n, the number of entries in the databases, goes to infinity.$$$This is based on combining a striking dimensionality reduction result for random sampling with unconditionally secure multi-party computation protocols.",BACKGROUND/RESULTS METHODS
D01859,"Bilinear pooling has been recently proposed as a feature encoding layer, which can be used after the convolutional layers of a deep network, to improve performance in multiple vision tasks.$$$Different from conventional global average pooling or fully connected layer, bilinear pooling gathers 2nd order information in a translation invariant fashion.$$$However, a serious drawback of this family of pooling layers is their dimensionality explosion.$$$Approximate pooling methods with compact properties have been explored towards resolving this weakness.$$$Additionally, recent results have shown that significant performance gains can be achieved by adding 1st order information and applying matrix normalization to regularize unstable higher order information.$$$However, combining compact pooling with matrix normalization and other order information has not been explored until now.$$$In this paper, we unify bilinear pooling and the global Gaussian embedding layers through the empirical moment matrix.$$$In addition, we propose a novel sub-matrix square-root layer, which can be used to normalize the output of the convolution layer directly and mitigate the dimensionality problem with off-the-shelf compact pooling methods.$$$Our experiments on three widely used fine-grained classification datasets illustrate that our proposed architecture, MoNet, can achieve similar or better performance than with the state-of-art G2DeNet.$$$Furthermore, when combined with compact pooling technique, MoNet obtains comparable performance with encoded features with 96% less dimensions.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS
D05554,"In this paper, efficient resource allocation for the uplink transmission of wireless powered IoT networks is investigated.$$$We adopt LoRa technology as an example in the IoT network, but this work is still suitable for other communication technologies.$$$Allocating limited resources, like spectrum and energy resources, among a massive number of users faces critical challenges.$$$We consider grouping wireless powered IoT users into available channels first and then investigate power allocation for users grouped in the same channel to improve the network throughput.$$$Specifically, the user grouping problem is formulated as a many to one matching game.$$$It is achieved by considering IoT users and channels as selfish players which belong to two disjoint sets.$$$Both selfish players focus on maximizing their own utilities.$$$Then we propose an efficient channel allocation algorithm (ECAA) with low complexity for user grouping.$$$Additionally, a Markov Decision Process (MDP) is used to model unpredictable energy arrival and channel conditions uncertainty at each user, and a power allocation algorithm is proposed to maximize the accumulative network throughput over a finite-horizon of time slots.$$$By doing so, we can distribute the channel access and dynamic power allocation local to IoT users.$$$Numerical results demonstrate that our proposed ECAA algorithm achieves near-optimal performance and is superior to random channel assignment, but has much lower computational complexity.$$$Moreover, simulations show that the distributed power allocation policy for each user is obtained with better performance than a centralized offline scheme.",OBJECTIVES METHODS BACKGROUND METHODS METHODS METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS/CONCLUSIONS
D04280,"Data diversity is critical to success when training deep learning models.$$$Medical imaging data sets are often imbalanced as pathologic findings are generally rare, which introduces significant challenges when training deep learning models.$$$In this work, we propose a method to generate synthetic abnormal MRI images with brain tumors by training a generative adversarial network using two publicly available data sets of brain MRI.$$$We demonstrate two unique benefits that the synthetic images provide.$$$First, we illustrate improved performance on tumor segmentation by leveraging the synthetic images as a form of data augmentation.$$$Second, we demonstrate the value of generative models as an anonymization tool, achieving comparable tumor segmentation results when trained on the synthetic data versus when trained on real subject data.$$$Together, these results offer a potential solution to two of the largest challenges facing machine learning in medical imaging, namely the small incidence of pathological findings, and the restrictions around sharing of patient data.",BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS RESULTS/CONCLUSIONS
D03025,"We slightly improve the known lower bound on the asymptotic competitive ratio for online bin packing of rectangles.$$$We present a complete proof for the new lower bound, whose value is above 1.91.",BACKGROUND/RESULTS RESULTS/CONCLUSIONS
D02072,"Reeb graphs are structural descriptors that capture shape properties of a topological space from the perspective of a chosen function.$$$In this work we define a combinatorial metric for Reeb graphs of orientable surfaces in terms of the cost necessary to transform one graph into another by edit operations.$$$The main contributions of this paper are the stability property and the optimality of this edit distance.$$$More precisely, the stability result states that changes in the functions, measured by the maximum norm, imply not greater changes in the corresponding Reeb graphs, measured by the edit distance.$$$The optimality result states that our edit distance discriminates Reeb graphs better than any other metric for Reeb graphs of surfaces satisfying the stability property.",BACKGROUND METHODS RESULTS RESULTS RESULTS
D02214,"A common model for question answering (QA) is that a good answer is one that is closely related to the question, where relatedness is often determined using general-purpose lexical models such as word embeddings.$$$We argue that a better approach is to look for answers that are related to the question in a relevant way, according to the information need of the question, which may be determined through task-specific embeddings.$$$With causality as a use case, we implement this insight in three steps.$$$First, we generate causal embeddings cost-effectively by bootstrapping cause-effect pairs extracted from free text using a small set of seed patterns.$$$Second, we train dedicated embeddings over this data, by using task-specific contexts, i.e., the context of a cause is its effect.$$$Finally, we extend a state-of-the-art reranking approach for QA to incorporate these causal embeddings.$$$We evaluate the causal embedding models both directly with a casual implication task, and indirectly, in a downstream causal QA task using data from Yahoo!$$$Answers.$$$We show that explicitly modeling causality improves performance in both tasks.$$$In the QA task our best model achieves 37.3% P@1, significantly outperforming a strong baseline by 7.7% (relative).",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS OTHERS RESULTS RESULTS
D06362,"This work deals with non-native children's speech and investigates both multi-task and transfer learning approaches to adapt a multi-language Deep Neural Network (DNN) to speakers, specifically children, learning a foreign language.$$$The application scenario is characterized by young students learning English and German and reading sentences in these second-languages, as well as in their mother language.$$$The paper analyzes and discusses techniques for training effective DNN-based acoustic models starting from children native speech and performing adaptation with limited non-native audio material.$$$A multi-lingual model is adopted as baseline, where a common phonetic lexicon, defined in terms of the units of the International Phonetic Alphabet (IPA), is shared across the three languages at hand (Italian, German and English); DNN adaptation methods based on transfer learning are evaluated on significant non-native evaluation sets.$$$Results show that the resulting non-native models allow a significant improvement with respect to a mono-lingual system adapted to speakers of the target language.",BACKGROUND METHODS OBJECTIVES METHODS RESULTS
D03163,"Evaluating human-computer interaction is essential as a broadening population uses machines, sometimes in sensitive contexts.$$$However, traditional evaluation methods may fail to combine real-time measures, an ""objective"" approach and data contextualization.$$$In this review we look at how adding neuroimaging techniques can respond to such needs.$$$We focus on electroencephalography (EEG), as it could be handled effectively during a dedicated evaluation phase.$$$We identify workload, attention, vigilance, fatigue, error recognition, emotions, engagement, flow and immersion as being recognizable by EEG.$$$We find that workload, attention and emotions assessments would benefit the most from EEG.$$$Moreover, we advocate to study further error recognition through neuroimaging to enhance usability and increase user experience.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS RESULTS OTHERS
D01143,"A long-standing practical challenge in the optimization of higher-order languages is inlining functions with free variables.$$$Inlining code statically at a function call site is safe if the compiler can guarantee that the free variables have the same bindings at the inlining point as they do at the point where the function is bound as a closure (code and free variables).$$$There have been many attempts to create a heuristic to check this correctness condition, from Shivers' kCFA-based reflow analysis to Might's Delta-CFA and anodization, but all of those have performance unsuitable for practical compiler implementations.$$$In practice, modern language implementations rely on a series of tricks to capture some common cases (e.g., closures whose free variables are only top-level identifiers such as +) and rely on hand-inlining by the programmer for anything more complicated.$$$This work provides the first practical, general approach for inlining functions with free variables.$$$We also provide a proof of correctness, an evaluation of both the execution time and performance impact of this optimization, and some tips and tricks for implementing an efficient and precise control-flow analysis.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND RESULTS CONCLUSIONS
D01004,"This paper introduces a generalization of Convolutional Neural Networks (CNNs) to graphs with irregular linkage structures, especially heterogeneous graphs with typed nodes and schemas.$$$We propose a novel spatial convolution operation to model the key properties of local connectivity and translation invariance, using high-order connection patterns or motifs.$$$We develop a novel deep architecture Motif-CNN that employs an attention model to combine the features extracted from multiple patterns, thus effectively capturing high-order structural and feature information.$$$Our experiments on semi-supervised node classification on real-world social networks and multiple representative heterogeneous graph datasets indicate significant gains of 6-21% over existing graph CNNs and other state-of-the-art techniques.",OBJECTIVES METHODS METHODS RESULTS
D02623,"Modeling data with multivariate count responses is a challenging problem due to the discrete nature of the responses.$$$Existing methods for univariate count responses cannot be easily extended to the multivariate case since the dependency among multiple responses needs to be properly accommodated.$$$In this paper, we propose a multivariate Poisson log-normal regression model for multivariate data with count responses.$$$By simultaneously estimating the regression coefficients and inverse covariance matrix over the latent variables with an efficient Monte Carlo EM algorithm, the proposed regression model takes advantages of association among multiple count responses to improve the model prediction performance.$$$Simulation studies and applications to real world data are conducted to systematically evaluate the performance of the proposed method in comparison with conventional methods.",OBJECTIVES BACKGROUND METHODS METHODS RESULTS
D04838,"We develop a static complexity analysis for a higher-order functional language with structural list recursion.$$$The complexity of an expression is a pair consisting of a cost and a potential.$$$The former is defined to be the size of the expression's evaluation derivation in a standard big-step operational semantics.$$$The latter is a measure of the ""future"" cost of using the value of that expression.$$$A translation function tr maps target expressions to complexities.$$$Our main result is the following Soundness Theorem: If t is a term in the target language, then the cost component of tr(t) is an upper bound on the cost of evaluating t. The proof of the Soundness Theorem is formalized in Coq, providing certified upper bounds on the cost of any expression in the target language.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D06250,"Complex systems are increasingly being viewed as distributed information processing systems, particularly in the domains of computational neuroscience, bioinformatics and Artificial Life.$$$This trend has resulted in a strong uptake in the use of (Shannon) information-theoretic measures to analyse the dynamics of complex systems in these fields.$$$We introduce the Java Information Dynamics Toolkit (JIDT): a Google code project which provides a standalone, (GNU GPL v3 licensed) open-source code implementation for empirical estimation of information-theoretic measures from time-series data.$$$While the toolkit provides classic information-theoretic measures (e.g. entropy, mutual information, conditional mutual information), it ultimately focusses on implementing higher-level measures for information dynamics.$$$That is, JIDT focusses on quantifying information storage, transfer and modification, and the dynamics of these operations in space and time.$$$For this purpose, it includes implementations of the transfer entropy and active information storage, their multivariate extensions and local or pointwise variants.$$$JIDT provides implementations for both discrete and continuous-valued data for each measure, including various types of estimator for continuous data (e.g.$$$Gaussian, box-kernel and Kraskov-Stoegbauer-Grassberger) which can be swapped at run-time due to Java's object-oriented polymorphism.$$$Furthermore, while written in Java, the toolkit can be used directly in MATLAB, GNU Octave, Python and other environments.$$$We present the principles behind the code design, and provide several examples to guide users.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES/METHODS METHODS METHODS METHODS METHODS CONCLUSIONS RESULTS
D01977,"This paper presents the release of EmojiNet, the largest machine-readable emoji sense inventory that links Unicode emoji representations to their English meanings extracted from the Web.$$$EmojiNet is a dataset consisting of: (i) 12,904 sense labels over 2,389 emoji, which were extracted from the web and linked to machine-readable sense definitions seen in BabelNet, (ii) context words associated with each emoji sense, which are inferred through word embedding models trained over Google News corpus and a Twitter message corpus for each emoji sense definition, and (iii) recognizing discrepancies in the presentation of emoji on different platforms, specification of the most likely platform-based emoji sense for a selected set of emoji.$$$The dataset is hosted as an open service with a REST API and is available at http://emojinet.knoesis.org/.$$$The development of this dataset, evaluation of its quality, and its applications including emoji sense disambiguation and emoji sense similarity are discussed.",OBJECTIVES METHODS RESULTS CONCLUSIONS
D02549,"In zero-shot learning (ZSL), a classifier is trained to recognize visual classes without any image samples.$$$Instead, it is given semantic information about the class, like a textual description or a set of attributes.$$$Learning from attributes could benefit from explicitly modeling structure of the attribute space.$$$Unfortunately, learning of general structure from empirical samples is hard with typical dataset sizes.$$$Here we describe LAGO, a probabilistic model designed to capture natural soft and-or relations across groups of attributes.$$$We show how this model can be learned end-to-end with a deep attribute-detection model.$$$The soft group structure can be learned from data jointly as part of the model, and can also readily incorporate prior knowledge about groups if available.$$$The soft and-or structure succeeds to capture meaningful and predictive structures, improving the accuracy of zero-shot learning on two of three benchmarks.$$$Finally, LAGO reveals a unified formulation over two ZSL approaches: DAP (Lampert et al., 2009) and ESZSL (Romera-Paredes & Torr, 2015).$$$Interestingly, taking only one singleton group for each attribute, introduces a new soft-relaxation of DAP, that outperforms DAP by ~40.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS METHODS RESULTS CONCLUSIONS RESULTS
D01637,"The paper explores the topic of Facial Action Unit (FAU) detection in the wild.$$$In particular, we are interested in answering the following questions: (1) how useful are residual connections across dense blocks for face analysis?$$$(2) how useful is the information from a network trained for categorical Facial Expression Recognition (FER) for the task of FAU detection?$$$The proposed network (ResiDen) exploits dense blocks along with residual connections and uses auxiliary information from a FER network.$$$The experiments are performed on the EmotionNet and DISFA datasets.$$$The experiments show the usefulness of facial expression information for AU detection.$$$The proposed network achieves state-of-art results on the two databases.$$$Analysis of the results for cross database protocol shows the effectiveness of the network.",BACKGROUND OBJECTIVES OBJECTIVES METHODS METHODS RESULTS RESULTS CONCLUSIONS
D03124,"Classifying single image patches is important in many different applications, such as road detection or scene understanding.$$$In this paper, we present convolutional patch networks, which are convolutional networks learned to distinguish different image patches and which can be used for pixel-wise labeling.$$$We also show how to incorporate spatial information of the patch as an input to the network, which allows for learning spatial priors for certain categories jointly with an appearance model.$$$In particular, we focus on road detection and urban scene understanding, two application areas where we are able to achieve state-of-the-art results on the KITTI as well as on the LabelMeFacade dataset.$$$Furthermore, our paper offers a guideline for people working in the area and desperately wandering through all the painstaking details that render training CNs on image patches extremely difficult.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS CONCLUSIONS
D00241,"Convolutional neural networks (CNNs) show impressive performance for image classification and detection, extending heavily to the medical image domain.$$$Nevertheless, medical experts are sceptical in these predictions as the nonlinear multilayer structure resulting in a classification outcome is not directly graspable.$$$Recently, approaches have been shown which help the user to understand the discriminative regions within an image which are decisive for the CNN to conclude to a certain class.$$$Although these approaches could help to build trust in the CNNs predictions, they are only slightly shown to work with medical image data which often poses a challenge as the decision for a class relies on different lesion areas scattered around the entire image.$$$Using the DiaretDB1 dataset, we show that on retina images different lesion areas fundamental for diabetic retinopathy are detected on an image level with high accuracy, comparable or exceeding supervised methods.$$$On lesion level, we achieve few false positives with high sensitivity, though, the network is solely trained on image-level labels which do not include information about existing lesions.$$$Classifying between diseased and healthy images, we achieve an AUC of 0.954 on the DiaretDB1.",OTHERS BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS
D04157,"Light clients, also known as Simple Payment Verification (SPV) clients, are nodes which only download a small portion of the data in a blockchain, and use indirect means to verify that a given chain is valid.$$$Typically, instead of validating block data, they assume that the chain favoured by the blockchain's consensus algorithm only contains valid blocks, and that the majority of block producers are honest.$$$By allowing such clients to receive fraud proofs generated by fully validating nodes that show that a block violates the protocol rules, and combining this with probabilistic sampling techniques to verify that all of the data in a block actually is available to be downloaded, we can eliminate the honest-majority assumption, and instead make much weaker assumptions about a minimum number of honest nodes that rebroadcast data.$$$Fraud and data availability proofs are key to enabling on-chain scaling of blockchains (e.g. via sharding or bigger blocks) while maintaining a strong assurance that on-chain data is available and valid.$$$We present, implement, and evaluate a novel fraud and data availability proof system.",BACKGROUND BACKGROUND OBJECTIVES/METHODS CONCLUSIONS CONCLUSIONS
D06552,"In this paper, we conduct extensive simulations to understand the properties of the overlay generated by BitTorrent.$$$We start by analyzing how the overlay properties impact the efficiency of BitTorrent.$$$We focus on the average peer set size (i.e., average number of neighbors), the time for a peer to reach its maximum peer set size, and the diameter of the overlay.$$$In particular, we show that the later a peer arrives in a torrent, the longer it takes to reach its maximum peer set size.$$$Then, we evaluate the impact of the maximum peer set size, the maximum number of outgoing connections per peer, and the number of NATed peers on the overlay properties.$$$We show that BitTorrent generates a robust overlay, but that this overlay is not a random graph.$$$In particular, the connectivity of a peer to its neighbors depends on its arriving order in the torrent.$$$We also show that a large number of NATed peers significantly compromise the robustness of the overlay to attacks.$$$Finally, we evaluate the impact of peer exchange on the overlay properties, and we show that it generates a chain-like overlay with a large diameter, which will adversely impact the efficiency of large torrents.",OBJECTIVES/METHODS METHODS METHODS RESULTS METHODS RESULTS RESULTS RESULTS METHODS/RESULTS
D03307,"A prognostic watch of the electric power system (EPS)is framed up, which detects the threat to EPS for a day ahead according to the characteristic times for a day ahead and according to the droop for a day ahead.$$$Therefore, a prognostic analysis of the EPS development for a day ahead is carried out.$$$Also the power grid, the electricity marker state, the grid state and the level of threat for a power grid are found for a day ahead.$$$The accuracy of the built up prognostic watch is evaluated.",OBJECTIVES METHODS RESULTS RESULTS
D00890,"In this paper, we investigate the strength of six different SAT solvers in attacking various obfuscation schemes.$$$Our investigation revealed that Glucose and Lingeling SAT solvers are generally suited for attacking small-to-midsize obfuscated circuits, while the MapleGlucose, if the system is not memory bound, is best suited for attacking mid-to-difficult obfuscation methods.$$$Our experimental result indicates that when dealing with extremely large circuits and very difficult obfuscation problems, the SAT solver may be memory bound, and Lingeling, for having the most memory efficient implementation, is the best-suited solver for such problems.$$$Additionally, our investigation revealed that SAT solver execution times may vary widely across different SAT solvers.$$$Hence, when testing the hardness of an obfuscation method, although the increase in difficulty could be verified by one SAT solver, the pace of increase in difficulty is dependent on the choice of a SAT solver.",OBJECTIVES RESULTS RESULTS/CONCLUSIONS CONCLUSIONS CONCLUSIONS
D02102,Identifying the occurrence of congestion in a Mobile Ad-hoc Network (MANET) is a major task.$$$The inbuilt congestion control techniques of existing Transmission Control Protocol (TCP) designed for wired networks do not handle the unique properties of shared wireless multi-hop link.$$$There are several approaches proposed for detecting and overcoming the congestion in the mobile ad-hoc network.$$$In this paper we present a Modified AD-hoc Transmission Control Protocol (M-ADTCP) method where the receiver detects the probable current network status and transmits this information to the sender as feedback.$$$The sender behavior is altered appropriately.$$$The proposed technique is also compatible with standard TCP.,OBJECTIVES BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS
D01410,"Currently, we are overwhelmed by a deluge of experimental data, and network physics has the potential to become an invaluable method to increase our understanding of large interacting datasets.$$$However, this potential is often unrealized for two reasons: uncovering the hidden community structure of a network, known as community detection, is difficult, and further, even if one has an idea of this community structure, it is not a priori obvious how to efficiently use this information.$$$Here, to address both of these issues, we, first, identify optimal community structure of given networks in terms of modularity by utilizing a recently introduced community detection method.$$$Second, we develop an approach to use this community information to extract hidden information from a network.$$$When applied to a protein-protein interaction network, the proposed method outperforms current state-of-the-art methods that use only the local information of a network.$$$The method is generally applicable to networks from many areas.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS CONCLUSIONS
D02708,"We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling.$$$A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution.$$$In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference.$$$Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.",OBJECTIVES METHODS METHODS RESULTS
D04519,"In this paper, we consider a class of nonlinear regression problems without the assumption of being independent and identically distributed.$$$We propose a correspondent mini-max problem for nonlinear regression and outline a numerical algorithm.$$$Such an algorithm can be applied in regression and machine learning problems, and yield better results than traditional regression and machine learning methods.",OBJECTIVES METHODS RESULTS
D00916,"We present the modeling and characterization of a time-reversal routing dispersion code multiple access (TR-DCMA) system.$$$We show that this system maintains the low complexity advantage of DCMA transceivers while offering dynamic adaptivity for practial communication scenarios.$$$We first derive the mathematical model and explain operation principles of the system, and then characterize its interference, signal to interference ratio, and bit error probability characteristics.",BACKGROUND/OBJECTIVES/METHODS/RESULTS/CONCLUSIONS BACKGROUND/OBJECTIVES/METHODS/RESULTS/CONCLUSIONS BACKGROUND/OBJECTIVES/METHODS/RESULTS/CONCLUSIONS
D05117,"The discriminative power of modern deep learning models for 3D human action recognition is growing ever so potent.$$$In conjunction with the recent resurgence of 3D human action representation with 3D skeletons, the quality and the pace of recent progress have been significant.$$$However, the inner workings of state-of-the-art learning based methods in 3D human action recognition still remain mostly black-box.$$$In this work, we propose to use a new class of models known as Temporal Convolutional Neural Networks (TCN) for 3D human action recognition.$$$Compared to popular LSTM-based Recurrent Neural Network models, given interpretable input such as 3D skeletons, TCN provides us a way to explicitly learn readily interpretable spatio-temporal representations for 3D human action recognition.$$$We provide our strategy in re-designing the TCN with interpretability in mind and how such characteristics of the model is leveraged to construct a powerful 3D activity recognition method.$$$Through this work, we wish to take a step towards a spatio-temporal model that is easier to understand, explain and interpret.$$$The resulting model, Res-TCN, achieves state-of-the-art results on the largest 3D human action recognition dataset, NTU-RGBD.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES OBJECTIVES/METHODS METHODS OBJECTIVES RESULTS/CONCLUSIONS
D01323,"I describe how real quantum annealers may be used to perform local (in state space) searches around specified states, rather than the global searches traditionally implemented in the quantum annealing algorithm.$$$The quantum annealing algorithm is an analogue of simulated annealing, a classical numerical technique which is now obsolete.$$$Hence, I explore strategies to use an annealer in a way which takes advantage of modern classical optimization algorithms, and additionally should be less sensitive to problem mis-specification then the traditional quantum annealing algorithm.",RESULTS/CONCLUSIONS BACKGROUND RESULTS/CONCLUSIONS
D03772,"Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown considerable promise in recent years as a potential tool for improving clinical decision support in medical oncology, particularly those based around the concept of Discovery Radiomics, where radiomic sequencers are discovered through the analysis of medical imaging data.$$$One of the main limitations with current CAD approaches is that it is very difficult to gain insight or rationale as to how decisions are made, thus limiting their utility to clinicians.$$$Methods: In this study, we propose CLEAR-DR, a novel interpretable CAD system based on the notion of CLass-Enhanced Attentive Response Discovery Radiomics for the purpose of clinical decision support for diabetic retinopathy.$$$Results: In addition to disease grading via the discovered deep radiomic sequencer, the CLEAR-DR system also produces a visual interpretation of the decision-making process to provide better insight and understanding into the decision-making process of the system.$$$Conclusion: We demonstrate the effectiveness and utility of the proposed CLEAR-DR system of enhancing the interpretability of diagnostic grading results for the application of diabetic retinopathy grading.$$$Significance: CLEAR-DR can act as a potential powerful tool to address the uninterpretability issue of current CAD systems, thus improving their utility to clinicians.",BACKGROUND BACKGROUND METHODS RESULTS CONCLUSIONS OTHERS
D06107,"In this paper we consider cryptographic applications of the arithmetic on the hyperoctahedral group.$$$On an appropriate subgroup of the latter, we particularly propose to construct public key cryptosystems based on the discrete logarithm.$$$The fact that the group of signed permutations has rich properties provides fast and easy implementation and makes these systems resistant to attacks like the Pohlig-Hellman algorithm.$$$The only negative point is that storing and transmitting permutations need large memory.$$$Using together the hyperoctahedral enumeration system and what is called subexceedant functions, we define a one-to-one correspondance between natural numbers and signed permutations with which we label the message units.",BACKGROUND OBJECTIVES RESULTS CONCLUSIONS METHODS
D04445,"Technical Universities (TUs) exhibit a distinct ranking performance in comparison with other universities.$$$In this paper we identify 137 TUs included in the THE Ranking (2017 edition) and analyse their scores statistically.$$$The results highlight the existence of clusters of TUs showing a general high performance in the Industry Income category and, in many cases, a low performance on Research and Teaching.$$$Finally, the global score weights were simulated, creating several scenarios that confirmed that the majority of TUs (except those with a world-class status) would increase their final scores if industrial income was accounted for at the levels parametrised.",OBJECTIVES/RESULTS METHODS/RESULTS RESULTS RESULTS/CONCLUSIONS
D05704,"Optical backbone networks carry a huge amount of bandwidth and serve as a key enabling technology to provide telecommunication connectivity across the world.$$$Hence, in events of network component (node/link) failures, communication networks may suffer from huge amount of bandwidth loss and service disruptions.$$$Natural disasters such as earthquakes, hurricanes, tornadoes, etc., occur at different places around the world, causing severe communication service disruptions due to network component failures.$$$Most of the previous works on optical network survivability assume that the failures are going to occur in future, and the network is made survivable to ensure connectivity in events of failures.$$$With the advancements in seismology, the predictions of earthquakes are becoming more accurate.$$$Earthquakes have been a major cause of telecommunication service disruption in the past.$$$Hence, the information provided by the meteorological departments and other similar agencies of different countries may be helpful in designing networks that are more robust against earthquakes.$$$In this work, we consider the actual information provided by the Indian meteorological department (IMD) on seismic zones, and earthquakes occurred in the past in India, and propose a scheme to improve the survivability of the existing Indian optical network through minute changes in network topology.$$$Simulations show significant improvement in the network survivability can be achieved using the proposed scheme in events of earthquakes.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS/CONCLUSIONS
D05565,"Did the demise of the Soviet Union in 1991 influence the scientific performance of the researchers in Eastern European countries?$$$Did this historical event affect international collaboration by researchers from the Eastern European countries with those of Western countries?$$$Did it also change international collaboration among researchers from the Eastern European countries?$$$Trying to answer these questions, this study aims to shed light on international collaboration by researchers from the Eastern European countries (Russia, Ukraine, Belarus, Moldova, Bulgaria, the Czech Republic, Hungary, Poland, Romania and Slovakia).$$$The number of publications and normalized citation impact values are compared for these countries based on InCites (Thomson Reuters), from 1981 up to 2011.$$$The international collaboration by researchers affiliated to institutions in Eastern European countries at the time points of 1990, 2000 and 2011 was studied with the help of Pajek and VOSviewer software, based on data from the Science Citation Index (Thomson Reuters).$$$Our results show that the breakdown of the communist regime did not lead, on average, to a huge improvement in the publication performance of the Eastern European countries and that the increase in international co-authorship relations by the researchers affiliated to institutions in these countries was smaller than expected.$$$Most of the Eastern European countries are still subject to changes and are still awaiting their boost in scientific development.",BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES BACKGROUND/OBJECTIVES OBJECTIVES/METHODS METHODS METHODS RESULTS CONCLUSIONS
D01685,"Questions of understanding and quantifying the representation and amount of information in organisms have become a central part of biological research, as they potentially hold the key to fundamental advances.$$$In this paper, we demonstrate the use of information-theoretic tools for the task of identifying segments of biomolecules (DNA or RNA) that are statistically correlated.$$$We develop a precise and reliable methodology, based on the notion of mutual information, for finding and extracting statistical as well as structural dependencies.$$$A simple threshold function is defined, and its use in quantifying the level of significance of dependencies between biological segments is explored.$$$These tools are used in two specific applications.$$$First, for the identification of correlations between different parts of the maize zmSRp32 gene.$$$There, we find significant dependencies between the 5' untranslated region in zmSRp32 and its alternatively spliced exons.$$$This observation may indicate the presence of as-yet unknown alternative splicing mechanisms or structural scaffolds.$$$Second, using data from the FBI's Combined DNA Index System (CODIS), we demonstrate that our approach is particularly well suited for the problem of discovering short tandem repeats, an application of importance in genetic profiling.",BACKGROUND RESULTS RESULTS RESULTS OBJECTIVES RESULTS RESULTS CONCLUSIONS RESULTS
D05042,"The partial information decomposition (PID) is perhaps the leading proposal for resolving information shared between a set of sources and a target into redundant, synergistic, and unique constituents.$$$Unfortunately, the PID framework has been hindered by a lack of a generally agreed-upon, multivariate method of quantifying the constituents.$$$Here, we take a step toward rectifying this by developing a decomposition based on a new method that quantifies unique information.$$$We first develop a broadly applicable method---the dependency decomposition---that delineates how statistical dependencies influence the structure of a joint distribution.$$$The dependency decomposition then allows us to define a measure of the information about a target that can be uniquely attributed to a particular source as the least amount which the source-target statistical dependency can influence the information shared between the sources and the target.$$$The result is the first measure that satisfies the core axioms of the PID framework while not satisfying the Blackwell relation, which depends on a particular interpretation of how the variables are related.$$$This makes a key step forward to a practical PID.",BACKGROUND BACKGROUND OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D03610,"Input validation is the first line of defense against malformed or malicious inputs.$$$It is therefore critical that the validator (which is often part of the parser) is free of bugs.$$$To build dependable input validators, we propose using parser generators for context-free languages.$$$In the context of network protocols, various works have pointed at context-free languages as falling short to specify precisely or concisely common idioms found in protocols.$$$We review those assessments and perform a rigorous, language-theoretic analysis of several common protocol idioms.$$$We then demonstrate the practical value of our findings by developing a modular, robust, and efficient input validator for HTTP relying on context-free grammars and regular expressions.",BACKGROUND BACKGROUND OBJECTIVES BACKGROUND METHODS RESULTS
D03462,"This paper presents a novel hierarchical spatiotemporal orientation representation for spacetime image analysis.$$$It is designed to combine the benefits of the multilayer architecture of ConvNets and a more controlled approach to spacetime analysis.$$$A distinguishing aspect of the approach is that unlike most contemporary convolutional networks no learning is involved; rather, all design decisions are specified analytically with theoretical motivations.$$$This approach makes it possible to understand what information is being extracted at each stage and layer of processing as well as to minimize heuristic choices in design.$$$Another key aspect of the network is its recurrent nature, whereby the output of each layer of processing feeds back to the input.$$$To keep the network size manageable across layers, a novel cross-channel feature pooling is proposed.$$$The multilayer architecture that results systematically reveals hierarchical image structure in terms of multiscale, multiorientation properties of visual spacetime.$$$To illustrate its utility, the network has been applied to the task of dynamic texture recognition.$$$Empirical evaluation on multiple standard datasets shows that it sets a new state-of-the-art.",OBJECTIVES OBJECTIVES/METHODS METHODS OBJECTIVES/METHODS METHODS OBJECTIVES/METHODS RESULTS METHODS RESULTS
D00445,"Recent work have shown that Reed-Muller (RM) codes achieve the erasure channel capacity.$$$However, this performance is obtained with maximum-likelihood decoding which can be costly for practical applications.$$$In this paper, we propose an encoding/decoding scheme for Reed-Muller codes on the packet erasure channel based on Plotkin construction.$$$We present several improvements over the generic decoding.$$$They allow, for a light cost, to compete with maximum-likelihood decoding performance, especially on high-rate codes, while significantly outperforming it in terms of speed.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS CONCLUSIONS
D05945,"The Wasserstein metric or earth mover's distance (EMD) is a useful tool in statistics, machine learning and computer science with many applications to biological or medical imaging, among others.$$$Especially in the light of increasingly complex data, the computation of these distances via optimal transport is often the limiting factor.$$$Inspired by this challenge, a variety of new approaches to optimal transport has been proposed in recent years and along with these new methods comes the need for a meaningful comparison.$$$In this paper, we introduce a benchmark for discrete optimal transport, called DOTmark, which is designed to serve as a neutral collection of problems, where discrete optimal transport methods can be tested, compared to one another, and brought to their limits on large-scale instances.$$$It consists of a variety of grayscale images, in various resolutions and classes, such as several types of randomly generated images, classical test images and real data from microscopy.$$$Along with the DOTmark we present a survey and a performance test for a cross section of established methods ranging from more traditional algorithms, such as the transportation simplex, to recently developed approaches, such as the shielding neighborhood method, and including also a comparison with commercial solvers.",BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES METHODS METHODS METHODS/RESULTS
D06016,"Network pruning is widely used for reducing the heavy computational cost of deep models.$$$A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning.$$$During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy.$$$In this work, we make several surprising observations which contradict common beliefs.$$$For all the six state-of-the-art pruning algorithms we examined, fine-tuning a pruned model only gives comparable or even worse performance than training that model with randomly initialized weights.$$$For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch.$$$Our observations are consistent for a wide variety of pruning algorithms with multiple network architectures, datasets, and tasks.$$$Our results have several implications: 1) training a large, over-parameterized model is not necessary to obtain an efficient final model, 2) learned ""important"" weights of the large model are not necessarily useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited ""important"" weights, is what leads to the efficiency benefit in the final model, which suggests that some pruning algorithms could be seen as performing network architecture search.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS CONCLUSIONS RESULTS CONCLUSIONS
D05407,"Most existing video summarisation methods are based on either supervised or unsupervised learning.$$$In this paper, we propose a reinforcement learning-based weakly supervised method that exploits easy-to-obtain, video-level category labels and encourages summaries to contain category-related information and maintain category recognisability.$$$Specifically, We formulate video summarisation as a sequential decision-making process and train a summarisation network with deep Q-learning (DQSN).$$$A companion classification network is also trained to provide rewards for training the DQSN.$$$With the classification network, we develop a global recognisability reward based on the classification result.$$$Critically, a novel dense ranking-based reward is also proposed in order to cope with the temporally delayed and sparse reward problems for long sequence reinforcement learning.$$$Extensive experiments on two benchmark datasets show that the proposed approach achieves state-of-the-art performance.",BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D06464,"The time domain inter-cell interference coordination techniques specified in LTE Rel.$$$10 standard improves the throughput of picocell-edge users by protecting them from macrocell interference.$$$On the other hand, it also degrades the aggregate capacity in macrocell because the macro base station (MBS) does not transmit data during certain subframes known as almost blank subframes.$$$The MBS data transmission using reduced power subframes was standardized in LTE Rel.$$$11, which can improve the capacity in macrocell while not causing high interference to the nearby picocells.$$$In order to get maximum benefit from the reduced power subframes, setting the key system parameters, such as the amount of power reduction, carries critical importance.$$$Using stochastic geometry, this paper lays down a theoretical foundation for the performance evaluation of heterogeneous networks with reduced power subframes and range expansion bias.$$$The analytic expressions for average capacity and 5th percentile throughput are derived as a function of transmit powers, node densities, and interference coordination parameters in a heterogeneous network scenario, and are validated through Monte Carlo simulations.$$$Joint optimization of range expansion bias, power reduction factor, scheduling thresholds, and duty cycle of reduced power subframes are performed to study the trade-offs between aggregate capacity of a cell and fairness among the users.$$$To validate our analysis, we also compare the stochastic geometry based theoretical results with the real MBS deployment (in the city of London) and the hexagonal-grid model.$$$Our analysis shows that with optimum parameter settings, the LTE Rel.$$$11 with reduced power subframes can provide substantially better performance than the LTE Rel.$$$10 with almost blank subframes, in terms of both aggregate capacity and fairness.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS METHODS/RESULTS CONCLUSIONS CONCLUSIONS CONCLUSIONS
D02023,"Video Quality Assessment (VQA) is a very challenging task due to its highly subjective nature.$$$Moreover, many factors influence VQA.$$$Compression of video content, while necessary for minimising transmission and storage requirements, introduces distortions which can have detrimental effects on the perceived quality.$$$Especially when dealing with modern video coding standards, it is extremely difficult to model the effects of compression due to the unpredictability of encoding on different content types.$$$Moreover, transmission also introduces delays and other distortion types which affect the perceived quality.$$$Therefore, it would be highly beneficial to accurately predict the perceived quality of video to be distributed over modern content distribution platforms, so that specific actions could be undertaken to maximise the Quality of Experience (QoE) of the users.$$$Traditional VQA techniques based on feature extraction and modelling may not be sufficiently accurate.$$$In this paper, a novel Deep Learning (DL) framework is introduced for effectively predicting VQA of video content delivery mechanisms based on end-to-end feature learning.$$$The proposed framework is based on Convolutional Neural Networks, taking into account compression distortion as well as transmission delays.$$$Training and evaluation of the proposed framework are performed on a user annotated VQA dataset specifically created to undertake this work.$$$The experiments show that the proposed methods can lead to high accuracy of the quality estimation, showcasing the potential of using DL in complex VQA scenarios.",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS METHODS METHODS RESULTS CONCLUSIONS
D02970,"Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many applications involve noisy and/or missing data, possibly involving dependence, as well.$$$We study these issues in the context of high-dimensional sparse linear regression, and propose novel estimators for the cases of noisy, missing and/or dependent data.$$$Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently nonconvex, and it is difficult to establish theoretical guarantees on practical algorithms.$$$While our approach also involves optimizing nonconvex programs, we are able to both analyze the statistical error associated with any global optimum, and more surprisingly, to prove that a simple algorithm based on projected gradient descent will converge in polynomial time to a small neighborhood of the set of all global minimizers.$$$On the statistical side, we provide nonasymptotic bounds that hold with high probability for the cases of noisy, missing and/or dependent data.$$$On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm is guaranteed to converge at a geometric rate to a near-global minimizer.$$$We illustrate these theoretical predictions with simulations, showing close agreement with the predicted scalings.",BACKGROUND OBJECTIVES BACKGROUND METHODS RESULTS RESULTS RESULTS
D04295,"Many machine learning problems can be formulated as predicting labels for a pair of objects.$$$Problems of that kind are often referred to as pairwise learning, dyadic prediction or network inference problems.$$$During the last decade kernel methods have played a dominant role in pairwise learning.$$$They still obtain a state-of-the-art predictive performance, but a theoretical analysis of their behavior has been underexplored in the machine learning literature.$$$In this work we review and unify existing kernel-based algorithms that are commonly used in different pairwise learning settings, ranging from matrix filtering to zero-shot learning.$$$To this end, we focus on closed-form efficient instantiations of Kronecker kernel ridge regression.$$$We show that independent task kernel ridge regression, two-step kernel ridge regression and a linear matrix filter arise naturally as a special case of Kronecker kernel ridge regression, implying that all these methods implicitly minimize a squared loss.$$$In addition, we analyze universality, consistency and spectral filtering properties.$$$Our theoretical results provide valuable insights in assessing the advantages and limitations of existing pairwise learning methods.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS RESULTS CONCLUSIONS
D03516,"We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps.$$$The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations.$$$CSRNet is an easy-trained model because of its pure convolutional structure.$$$We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance.$$$In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method.$$$We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset.$$$Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.",BACKGROUND METHODS METHODS RESULTS RESULTS RESULTS RESULTS
D04923,"Photonic systems for high-performance information processing have attracted renewed interest.$$$Neuromorphic silicon photonics has the potential to integrate processing functions that vastly exceed the capabilities of electronics.$$$We report first observations of a recurrent silicon photonic neural network, in which connections are configured by microring weight banks.$$$A mathematical isomorphism between the silicon photonic circuit and a continuous neural network model is demonstrated through dynamical bifurcation analysis.$$$Exploiting this isomorphism, a simulated 24-node silicon photonic neural network is programmed using ""neural compiler"" to solve a differential system emulation task.$$$A 294-fold acceleration against a conventional benchmark is predicted.$$$We also propose and derive power consumption analysis for modulator-class neurons that, as opposed to laser-class neurons, are compatible with silicon photonic platforms.$$$At increased scale, Neuromorphic silicon photonics could access new regimes of ultrafast information processing for radio, control, and scientific computing.",BACKGROUND BACKGROUND RESULTS METHODS/RESULTS RESULTS RESULTS OBJECTIVES CONCLUSIONS
D02658,"Predicting business process behaviour is an important aspect of business process management.$$$Motivated by research in natural language processing, this paper describes an application of deep learning with recurrent neural networks to the problem of predicting the next event in a business process.$$$This is both a novel method in process prediction, which has largely relied on explicit process models, and also a novel application of deep learning methods.$$$The approach is evaluated on two real datasets and our results surpass the state-of-the-art in prediction precision.",BACKGROUND OBJECTIVES OTHERS METHODS/RESULTS
D03996,"Enterprise software systems make complex interactions with other services in their environment.$$$Developing and testing for production-like conditions is therefore a challenging task.$$$Prior approaches include emulations of the dependency services using either explicit modelling or record-and-replay approaches.$$$Models require deep knowledge of the target services while record-and-replay is limited in accuracy.$$$We present a new technique that improves the accuracy of record-and-replay approaches, without requiring prior knowledge of the services.$$$The approach uses multiple sequence alignment to derive message prototypes from recorded system interactions and a scheme to match incoming request messages against message prototypes to generate response messages.$$$We introduce a modified Needleman-Wunsch algorithm for distance calculation during message matching, wildcards in message prototypes for high variability sections, and entropy-based weightings in distance calculations for increased accuracy.$$$Combined, our new approach has shown greater than 99% accuracy for four evaluated enterprise system messaging protocols.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS
D03325,"Enabling ultra fast systems has been widely investigated during recent decades.$$$Although polarization has been deployed from the beginning in satellite communications, nowadays it is being exploited to increase the throughput of satellite links.$$$More precisely, the application of diversity techniques to the polarization domain may provide reliable, robust, and fast satellite communications.$$$Better and more flexible spectrum use is also possible if transmission and reception can take place simultaneously in close or even overlapping frequency bands.$$$In this paper we investigate novel signal processing techniques to increase the throughput of satellite communications in fixed and mobile scenarios.$$$First, we investigate four-dimensional (4D) constellations for the forward link.$$$Second, we focus on the mobile scenario and introduce an adaptive algorithm which selects the optimal tuple of modulation order, coding rate, and MIMO scheme that maximizes the throughput constraint to a maximum packet error rate.$$$Finally, we describe the operation of radio transceivers which cancel actively the self-interference posed by the transmit signal when operating in full-duplex mode.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS
D06822,"In this paper, we study the generation of maximal Poisson-disk sets with varying radii.$$$First, we present a geometric analysis of gaps in such disk sets.$$$This analysis is the basis for maximal and adaptive sampling in Euclidean space and on manifolds.$$$Second, we propose efficient algorithms and data structures to detect gaps and update gaps when disks are inserted, deleted, moved, or have their radius changed.$$$We build on the concepts of the regular triangulation and the power diagram.$$$Third, we will show how our analysis can make a contribution to the state-of-the-art in surface remeshing.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS
D05140,"The min-rank of a digraph was shown by Bar-Yossef et al.$$$(2006) to represent the length of an optimal scalar linear solution of the corresponding instance of the Index Coding with Side Information (ICSI) problem.$$$In this work, the graphs and digraphs of near-extreme min-ranks are characterized.$$$Those graphs and digraphs correspond to the ICSI instances having near-extreme transmission rates when using optimal scalar linear index codes.$$$In particular, it is shown that the decision problem whether a digraph has min-rank two is NP-complete.$$$By contrast, the same question for graphs can be answered in polynomial time.$$$Additionally, a new upper bound on the min-rank of a digraph, the circuit-packing bound, is presented.$$$This bound is often tighter than the previously known bounds.$$$By employing this new bound, we present several families of digraphs whose min-ranks can be found in polynomial time.",BACKGROUND BACKGROUND OBJECTIVES/RESULTS OBJECTIVES/RESULTS RESULTS BACKGROUND RESULTS RESULTS RESULTS
D05443,"We introduce the class of synchronous subsequential relations, a subclass of the synchronous relations which embodies some properties of subsequential relations.$$$If we take relations of this class as forming the possible transitions of an infinite automaton, then most decision problems (apart from membership) still remain undecidable (as they are for synchronous and subsequential rational relations), but on the positive side, they can be approximated in a meaningful way we make precise in this paper.$$$This might make the class useful for some applications, and might serve to establish an intermediate position in the trade-off between issues of expressivity and (un)decidability.",OBJECTIVES RESULTS CONCLUSIONS
D06127,"When trying to maximize the adoption of a behavior in a population connected by a social network, it is common to strategize about where in the network to seed the behavior, often with an element of randomness.$$$Selecting seeds uniformly at random is a basic but compelling strategy in that it distributes seeds broadly throughout the network.$$$A more sophisticated stochastic strategy, one-hop targeting, is to select random network neighbors of random individuals; this exploits a version of the friendship paradox, whereby the friend of a random individual is expected to have more friends than a random individual, with the hope that seeding a behavior at more connected individuals leads to more adoption.$$$Many seeding strategies have been proposed, but empirical evaluations have demanded large field experiments designed specifically for this purpose and have yielded relatively imprecise comparisons of strategies.$$$Here we show how stochastic seeding strategies can be evaluated more efficiently in such experiments, how they can be evaluated ""off-policy"" using existing data arising from experiments designed for other purposes, and how to design more efficient experiments.$$$In particular, we consider contrasts between stochastic seeding strategies and analyze nonparametric estimators adapted from policy evaluation and importance sampling.$$$We use simulations on real networks to show that the proposed estimators and designs can dramatically increase precision while yielding valid inference.$$$We then apply our proposed estimators to two field experiments, one that assigned households to an intensive marketing intervention and one that assigned students to an anti-bullying intervention.",BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES/METHODS OBJECTIVES METHODS/RESULTS/CONCLUSIONS METHODS
D01212,"Aboria is a powerful and flexible C++ library for the implementation of particle-based numerical methods.$$$The particles in such methods can represent actual particles (e.g.$$$Molecular Dynamics) or abstract particles used to discretise a continuous function over a domain (e.g.$$$Radial Basis Functions).$$$Aboria provides a particle container, compatible with the Standard Template Library, spatial search data structures, and a Domain Specific Language to specify non-linear operators on the particle set.$$$This paper gives an overview of Aboria's design, an example of use, and a performance benchmark.",OBJECTIVES OBJECTIVES OBJECTIVES OBJECTIVES METHODS RESULTS
D03658,"The identity of a user is permanently lost if biometric data gets compromised since the biometric information is irreplaceable and irrevocable.$$$To revoke and reissue a new template in place of the compromised biometric template, the idea of cancelable biometrics has been introduced.$$$The concept behind cancelable biometric is to irreversibly transform the original biometric template and perform the comparison in the protected domain.$$$In this paper, a coprime transformation scheme has been proposed to derive a protected fingerprint template.$$$The method divides the fingerprint region into a number of sectors with respect to each minutiae point and identifies the nearest-neighbor minutiae in each sector.$$$Then, ridge features for all neighboring minutiae points are computed and mapped onto co-prime positions of a random matrix to generate the cancelable template.$$$The proposed approach achieves an EER of 1.82, 1.39, 4.02 and 5.77 on DB1, DB2, DB3 and DB4 datasets of the FVC2002 and an EER of 8.70, 7.95, 5.23 and 4.87 on DB1, DB2, DB3 and DB4 datasets of FVC2004 databases, respectively.$$$Experimental evaluations indicate that the method outperforms in comparison to the current state-of-the-art.$$$Moreover, it has been confirmed from the security analysis that the proposed method fulfills the desired characteristics of diversity, revocability, and non-invertibility with a minor performance degradation caused by the transformation.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS CONCLUSIONS
D06114,"As artificial intelligence is increasingly affecting all parts of society and life, there is growing recognition that human interpretability of machine learning models is important.$$$It is often argued that accuracy or other similar generalization performance metrics must be sacrificed in order to gain interpretability.$$$Such arguments, however, fail to acknowledge that the overall decision-making system is composed of two entities: the learned model and a human who fuses together model outputs with his or her own information.$$$As such, the relevant performance criteria should be for the entire system, not just for the machine learning component.$$$In this work, we characterize the performance of such two-node tandem data fusion systems using the theory of distributed detection.$$$In doing so, we work in the population setting and model interpretable learned models as multi-level quantizers.$$$We prove that under our abstraction, the overall system of a human with an interpretable classifier outperforms one with a black box classifier.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES METHODS RESULTS
D04335,"The current work proposes an application of DEA methodology for measurement of technical and allocative efficiency of university research activity.$$$The analysis is based on bibliometric data from the Italian university system for the five year period 2004-2008.$$$Technical and allocative efficiency is measured with input being considered as a university's research staff, classified according to academic rank, and with output considered as the field-standardized impact of the research product realized by these staff.$$$The analysis is applied to all scientific disciplines of the so-called hard sciences, and conducted at subfield level, thus at a greater level of detail than ever before achieved in national-scale research assessments.",OBJECTIVES OTHERS METHODS METHODS
D02761,"Excluding irrelevant features in a pattern recognition task plays an important role in maintaining a simpler machine learning model and optimizing the computational efficiency.$$$Nowadays with the rise of large scale datasets, feature selection is in great demand as it becomes a central issue when facing high-dimensional datasets.$$$The present study provides a new measure of saliency for features by employing a Sensitivity Analysis (SA) technique called the extended Fourier amplitude sensitivity test, and a well-trained Feedforward Neural Network (FNN) model, which ultimately leads to the selection of a promising optimal feature subset.$$$Ideas of the paper are mainly demonstrated based on adopting FNN model for feature selection in classification problems.$$$But in the end, a generalization framework is discussed in order to give insights into the usage in regression problems as well as expressing how other function approximate models can be deployed.$$$Effectiveness of the proposed method is verified by result analysis and data visualization for a series of experiments over several well-known datasets drawn from UCI machine learning repository.",OBJECTIVES BACKGROUND METHODS METHODS RESULTS/CONCLUSIONS RESULTS
D04389,"We present an average case analysis of a variant of dual-pivot quicksort.$$$We show that the used algorithmic partitioning strategy is optimal, i.e., it minimizes the expected number of key comparisons.$$$For the analysis, we calculate the expected number of comparisons exactly as well as asymptotically, in particular, we provide exact expressions for the linear, logarithmic, and constant terms.$$$An essential step is the analysis of zeros of lattice paths in a certain probability model.$$$Along the way a combinatorial identity is proven.",BACKGROUND RESULTS METHODS METHODS RESULTS
D00559,"The analysis of the use of social media for innovative entrepreneurship in the context has received little attention in the literature, especially in the context of Knowledge Intensive Business Services (KIBS).$$$Therefore, this paper focuses on bridging this gap by applying text mining and sentiment analysis techniques to identify the innovative entrepreneurship reflected by these companies in their social media.$$$Finally, we present and analyze the results of our quantitative analysis of 23.483 posts based on eleven Spanish and Italian consultancy KIBS Twitter Usernames and Keywords using data interpretation techniques such as clustering and topic modeling.$$$This paper suggests that there is a significant gap between the perceived potential of social media and the entrepreneurial behaviors at the social context in business-to-business (B2B) companies.",OBJECTIVES METHODS METHODS OTHERS
D01960,"In many real-world applications, data are often collected in the form of stream, and thus the distribution usually changes in nature, which is referred as concept drift in literature.$$$We propose a novel and effective approach to handle concept drift via model reuse, leveraging previous knowledge by reusing models.$$$Each model is associated with a weight representing its reusability towards current data, and the weight is adaptively adjusted according to the model performance.$$$We provide generalization and regret analysis.$$$Experimental results also validate the superiority of our approach on both synthetic and real-world datasets.",BACKGROUND OBJECTIVES/METHODS METHODS RESULTS RESULTS/CONCLUSIONS
D03286,"In this work, we propose a novel sampling method for Design of Experiments.$$$This method allows to sample such input values of the parameters of a computational model for which the constructed surrogate model will have the least possible approximation error.$$$High efficiency of the proposed method is demonstrated by its comparison with other sampling techniques (LHS, Sobol' sequence sampling, and Maxvol sampling) on the problem of least-squares polynomial approximation.$$$Also, numerical experiments for the Lebesgue constant growth for the points sampled by the proposed method are carried out.",BACKGROUND OBJECTIVES RESULTS RESULTS
D03999,"Accurate Traffic Sign Detection (TSD) can help intelligent systems make better decisions according to the traffic regulations.$$$TSD, regarded as a typical small object detection problem in some way, is fundamental in Advanced Driver Assistance Systems (ADAS) and self-driving.$$$However, although deep neural networks have achieved human even superhuman performance on several tasks, due to their own limitations, small object detection is still an open question.$$$In this paper, we proposed a brain-inspired network, named as KB-RANN, to handle this problem.$$$Attention mechanism is an essential function of our brain, we used a novel recurrent attentive neural network to improve the detection accuracy in a fine-grained manner.$$$Further, we combined domain specific knowledge and intuitive knowledge to improve the efficiency.$$$Experimental result shows that our methods achieved better performance than several popular methods widely used in object detection.$$$More significantly, we transplanted our method on our designed embedded system and deployed on our self-driving car successfully.",BACKGROUND BACKGROUND OBJECTIVES METHODS METHODS METHODS RESULTS RESULTS
D04424,"Reinforcement learning has significant applications for multi-agent systems, especially in unknown dynamic environments.$$$However, most multi-agent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multi-agent problems.$$$In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSI) is presented.$$$In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the non-strict Equilibrium Dominating Strategy Profile (non-strict EDSP) or Meta equilibrium for their joint actions.$$$The presented NegoSI algorithm consists of four parts: the equilibrium-based framework for sparse interactions, the negotiation for the equilibrium set, the minimum variance method for selecting one joint action and the knowledge transfer of local Q-values.$$$In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively.$$$To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: steps of each episode (SEE), rewards of each episode (REE) and average runtime (AR).$$$The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm.$$$Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms.",BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS METHODS RESULTS RESULTS RESULTS
D04397,"The use of millimeter wave (mmWave) frequencies for communication will be one of the innovations of the next generation of cellular mobile networks (5G).$$$It will provide unprecedented data rates, but is highly susceptible to rapid channel variations and suffers from severe isotropic pathloss.$$$Highly directional antennas at the transmitter and the receiver will be used to compensate for these shortcomings and achieve sufficient link budget in wide area networks.$$$However, directionality demands precise alignment of the transmitter and the receiver beams, an operation which has important implications for control plane procedures, such as initial access, and may increase the delay of the data transmission.$$$This paper provides a comparison of measurement frameworks for initial access in mmWave cellular networks in terms of detection accuracy, reactiveness and overhead, using parameters recently standardized by the 3GPP and a channel model based on real-world measurements.$$$We show that the best strategy depends on the specific environment in which the nodes are deployed, and provide guidelines to characterize the optimal choice as a function of the system parameters.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS/CONCLUSIONS
D02554,"The increasing demand for higher data rates, better quality of service, fully mobile and connected wireless networks lead the researchers to seek new solutions beyond 4G wireless systems.$$$It is anticipated that 5G wireless networks, which are expected to be introduced around 2020, will achieve ten times higher spectral and energy efficiency than current 4G wireless networks and will support data rates up to 10 Gbps for low mobility users.$$$The ambitious goals set for 5G wireless networks require dramatic changes in the design of different layers for next generation communications systems.$$$Massive multiple-input multiple-output (MIMO) systems, filter bank multi-carrier (FBMC) modulation, relaying technologies, and millimeter-wave communications have been considered as some of the strong candidates for the physical layer design of 5G networks.$$$In this article, we shed light on the potential and implementation of index modulation (IM) techniques for MIMO and multi-carrier communications systems which are expected to be two of the key technologies for 5G systems.$$$Specifically, we focus on two promising applications of IM: spatial modulation (SM) and orthogonal frequency division multiplexing with IM (OFDM-IM), and we discuss the recent advances and future research directions in IM technologies towards spectral and energy-efficient 5G wireless networks.",BACKGROUND OBJECTIVES OBJECTIVES BACKGROUND METHODS CONCLUSIONS
D01252,"Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need.$$$In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks.$$$Our method is trained end-to-end using question-answer pairs as a supervision signal.$$$A question and its paraphrases serve as input to a neural scoring model which assigns higher weights to linguistic expressions most likely to yield correct answers.$$$We evaluate our approach on QA over Freebase and answer sentence selection.$$$Experimental results on three datasets show that our framework consistently improves performance, achieving competitive results despite the use of simple QA models.",OBJECTIVES METHODS METHODS METHODS METHODS RESULTS/CONCLUSIONS
D00223,"Collective learning in economic development has been revealed by recent empirical studies, however, investigations on how to benefit most from its effects remain still lacking.$$$In this paper, we explore the maximization of the collective learning effects using a simple propagation model to study the diversification of industries on real networks built on Brazilian labor data.$$$For the inter-regional learning, we find an optimal strategy that makes a balance between core and periphery industries in the initial activation, considering the core-periphery structure of the industry space--a network representation of the relatedness between industries.$$$For the inter-regional learning, we find an optimal strategy that makes a balance between nearby and distant regions in establishing new spatial connections, considering the spatial structure of the integrated adjacent network that connects all regions.$$$Our findings suggest that the near to by random strategies are likely to make the best use of the collective learning effects in advancing regional economic development practices.",BACKGROUND/OBJECTIVES METHODS RESULTS RESULTS CONCLUSIONS
D03096,"Accounting for undecided and uncertain voters is a challenging issue for predicting election results from public opinion polls.$$$Undecided voters typify the uncertainty of swing voters in polls but are often ignored or allocated to each candidate in a simple, deterministic manner.$$$Historically this may have been adequate because the undecided were comparatively small enough to assume that they do not affect the relative proportions of the decided voters.$$$However, in the presence of high numbers of undecided voters, these static rules may in fact bias election predictions from election poll authors and meta-poll analysts.$$$In this paper, we examine the effect of undecided voters in the 2016 US presidential election to the previous three presidential elections.$$$We show there were a relatively high number of undecided voters over the campaign and on election day, and that the allocation of undecided voters in this election was not consistent with two-party proportional (or even) allocations.$$$We find evidence that static allocation regimes are inadequate for election prediction models and that probabilistic allocations may be superior.$$$We also estimate the bias attributable to polling agencies, often referred to as ""house effects"".",BACKGROUND/OBJECTIVES BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS RESULTS/CONCLUSIONS RESULTS
D01544,"In this paper, we present a method for instance ranking and retrieval at fine-grained level based on the global features extracted from a multi-attribute recognition model which is not dependent on landmarks information or part-based annotations.$$$Further, we make this architecture suitable for mobile-device application by adopting the bilinear CNN to make the multi-attribute recognition model smaller (in terms of the number of parameters).$$$The experiments run on the Dress category of DeepFashion In-Shop Clothes Retrieval and CUB200 datasets show that the results of instance retrieval at fine-grained level are promising for these datasets, specially in terms of texture and color.",OBJECTIVES/METHODS OBJECTIVES RESULTS/CONCLUSIONS
D01310,"Currently, software industries are using different SDLC (software development life cycle) models which are designed for specific purposes.$$$The use of technology is booming in every perspective of life and the software behind the technology plays an enormous role.$$$As the technical complexities are increasing, successful development of software solely depends on the proper management of development processes.$$$So, it is inevitable to introduce improved methodologies in the industry so that modern human centred software applications development can be managed and delivered to the user successfully.$$$So, in this paper, we have explored the facts of different SDLC models and perform their comparative analysis.",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES CONCLUSIONS
D02484,"Deep Neural Networks have been shown to succeed at a range of natural language tasks such as machine translation and text summarization.$$$While tasks on source code (ie, formal languages) have been considered recently, most work in this area does not attempt to capitalize on the unique opportunities offered by its known syntax and structure.$$$In this work, we introduce SmartPaste, a first task that requires to use such information.$$$The task is a variant of the program repair problem that requires to adapt a given (pasted) snippet of code to surrounding, existing source code.$$$As first solutions, we design a set of deep neural models that learn to represent the context of each variable location and variable usage in a data flow-sensitive way.$$$Our evaluation suggests that our models can learn to solve the SmartPaste task in many cases, achieving 58.6% accuracy, while learning meaningful representation of variable usages.",BACKGROUND BACKGROUND OTHERS OTHERS METHODS RESULTS
D04756,"To face future reliability challenges, it is necessary to quantify the risk of error in any part of a computing system.$$$To this goal, the Architectural Vulnerability Factor (AVF) has long been used for chips.$$$However, this metric is used for offline characterisation, which is inappropriate for memory.$$$We survey the literature and formalise one of the metrics used, the Memory Vulnerability Factor, and extend it to take into account false errors.$$$These are reported errors which would have no impact on the program if they were ignored.$$$We measure the False Error Aware MVF (FEA) and related metrics precisely in a cycle-accurate simulator, and compare them with the effects of injecting faults in a program's data, in native parallel runs.$$$Our findings show that MVF and FEA are the only two metrics that are safe to use at runtime, as they both consistently give an upper bound on the probability of incorrect program outcome.$$$FEA gives a tighter bound than MVF, and is the metric that correlates best with the incorrect outcome probability of all considered metrics.",OBJECTIVES BACKGROUND OTHERS METHODS OTHERS METHODS RESULTS RESULTS
D04449,"In the article, an experiment is aimed at clarifying the transfer efficiency of the database in the cloud infrastructure.$$$The system was added to the control unit, which has guided the database search in the local part or in the cloud.$$$It is shown that the time data acquisition remains unchanged as a result of modification.$$$Suggestions have been made about the use of the theory of dynamic systems to hybrid cloud database.$$$The present work is aimed at attracting the attention of spe-cialists in the field of cloud database to the apparatus control theory.$$$The experiment presented in this article allows the use of the description of the known methods for solving important practical problems.",OBJECTIVES METHODS RESULTS METHODS BACKGROUND CONCLUSIONS
D05546,"Automated brain lesions detection is an important and very challenging clinical diagnostic task because the lesions have different sizes, shapes, contrasts, and locations.$$$Deep Learning recently has shown promising progress in many application fields, which motivates us to apply this technology for such important problem.$$$In this paper, we propose a novel and end-to-end trainable approach for brain lesions classification and detection by using deep Convolutional Neural Network (CNN).$$$In order to investigate the applicability, we applied our approach on several brain diseases including high and low-grade glioma tumor, ischemic stroke, Alzheimer diseases, by which the brain Magnetic Resonance Images (MRI) have been applied as an input for the analysis.$$$We proposed a new operating unit which receives features from several projections of a subset units of the bottom layer and computes a normalized l2-norm for next layer.$$$We evaluated the proposed approach on two different CNN architectures and number of popular benchmark datasets.$$$The experimental results demonstrate the superior ability of the proposed approach.",BACKGROUND BACKGROUND OBJECTIVES/METHODS RESULTS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D05228,"Logic programs with aggregates (LPA) are one of the major linguistic extensions to Logic Programming (LP).$$$In this work, we propose a generalization of the notions of unfounded set and well-founded semantics for programs with monotone and antimonotone aggregates (LPAma programs).$$$In particular, we present a new notion of unfounded set for LPAma programs, which is a sound generalization of the original definition for standard (aggregate-free) LP.$$$On this basis, we define a well-founded operator for LPAma programs, the fixpoint of which is called well-founded model (or well-founded semantics) for LPAma programs.$$$The most important properties of unfounded sets and the well-founded semantics for standard LP are retained by this generalization, notably existence and uniqueness of the well-founded model, together with a strong relationship to the answer set semantics for LPAma programs.$$$We show that one of the D-well-founded semantics, defined by Pelov, Denecker, and Bruynooghe for a broader class of aggregates using approximating operators, coincides with the well-founded model as defined in this work on LPAma programs.$$$We also discuss some complexity issues, most importantly we give a formal proof of tractable computation of the well-founded model for LPA programs.$$$Moreover, we prove that for general LPA programs, which may contain aggregates that are neither monotone nor antimonotone, deciding satisfaction of aggregate expressions with respect to partial interpretations is coNP-complete.$$$As a consequence, a well-founded semantics for general LPA programs that allows for tractable computation is unlikely to exist, which justifies the restriction on LPAma programs.$$$Finally, we present a prototype system extending DLV, which supports the well-founded semantics for LPAma programs, at the time of writing the only implemented system that does so.$$$Experiments with this prototype show significant computational advantages of aggregate constructs over equivalent aggregate-free encodings.",BACKGROUND OBJECTIVES RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS RESULTS
D02207,"An open concept of rough evolution and an axiomatic approach to granules was also developed recently by the present author.$$$Subsequently the concepts were used in the formal framework of rough Y-systems (RYS) for developing on granular correspondences by her.$$$These have since been used for a new approach towards comparison of rough algebraic semantics across different semantic domains by way of correspondences that preserve rough evolution and try to avoid contamination.$$$In this research paper, new methods are proposed and a semantics for handling possibly contaminated operations and structured bigness is developed.$$$These would also be of natural interest for relative consistency of one collection of knowledge relative other.",BACKGROUND BACKGROUND METHODS RESULTS CONCLUSIONS
D02622,"Verifying the identity of a person using handwritten signatures is challenging in the presence of skilled forgeries, where a forger has access to a person's signature and deliberately attempt to imitate it.$$$In offline (static) signature verification, the dynamic information of the signature writing process is lost, and it is difficult to design good feature extractors that can distinguish genuine signatures and skilled forgeries.$$$This reflects in a relatively poor performance, with verification errors around 7% in the best systems in the literature.$$$To address both the difficulty of obtaining good features, as well as improve system performance, we propose learning the representations from signature images, in a Writer-Independent format, using Convolutional Neural Networks.$$$In particular, we propose a novel formulation of the problem that includes knowledge of skilled forgeries from a subset of users in the feature learning process, that aims to capture visual cues that distinguish genuine signatures and forgeries regardless of the user.$$$Extensive experiments were conducted on four datasets: GPDS, MCYT, CEDAR and Brazilian PUC-PR datasets.$$$On GPDS-160, we obtained a large improvement in state-of-the-art performance, achieving 1.72% Equal Error Rate, compared to 6.97% in the literature.$$$We also verified that the features generalize beyond the GPDS dataset, surpassing the state-of-the-art performance in the other datasets, without requiring the representation to be fine-tuned to each particular dataset.",BACKGROUND BACKGROUND BACKGROUND OBJECTIVES/METHODS METHODS METHODS RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
D00554,"We focus on robust and efficient iterative solvers for the pressure Poisson equation in incompressible Navier-Stokes problems.$$$Preconditioned Krylov subspace methods are popular for these problems, with BiCGStab and GMRES(m) most frequently used for nonsymmetric systems.$$$BiCGStab is popular because it has cheap iterations, but it may fail for stiff problems, especially early on as the initial guess is far from the solution.$$$Restarted GMRES is better, more robust, in this phase, but restarting may lead to very slow convergence.$$$Therefore, we evaluate the rGCROT method for these systems.$$$This method recycles a selected subspace of the search space (called recycle space) after a restart.$$$This generally improves the convergence drastically compared with GMRES(m).$$$Recycling subspaces is also advantageous for subsequent linear systems, if the matrix changes slowly or is constant.$$$However, rGCROT iterations are still expensive in memory and computation time compared with those of BiCGStab.$$$Hence, we propose a new, hybrid approach that combines the cheap iterations of BiCGStab with the robustness of rGCROT.$$$For the first few time steps the algorithm uses rGCROT and builds an effective recycle space, and then it recycles that space in the rBiCGStab solver.$$$We evaluate rGCROT on a turbulent channel flow problem, and we evaluate both rGCROT and the new, hybrid combination of rGCROT and rBiCGStab on a porous medium flow problem.$$$We see substantial performance gains on both problems.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND BACKGROUND BACKGROUND/METHODS BACKGROUND/METHODS BACKGROUND/METHODS BACKGROUND/METHODS METHODS METHODS RESULTS CONCLUSIONS
D06918,"In many advanced video based applications background modeling is a pre-processing step to eliminate redundant data, for instance in tracking or video surveillance applications.$$$Over the past years background subtraction is usually based on low level or hand-crafted features such as raw color components, gradients, or local binary patterns.$$$The background subtraction algorithms performance suffer in the presence of various challenges such as dynamic backgrounds, photometric variations, camera jitters, and shadows.$$$To handle these challenges for the purpose of accurate background modeling we propose a unified framework based on the algorithm of image inpainting.$$$It is an unsupervised visual feature learning hybrid Generative Adversarial algorithm based on context prediction.$$$We have also presented the solution of random region inpainting by the fusion of center region inpaiting and random region inpainting with the help of poisson blending technique.$$$Furthermore we also evaluated foreground object detection with the fusion of our proposed method and morphological operations.$$$The comparison of our proposed method with 12 state-of-the-art methods shows its stability in the application of background estimation and foreground detection.",BACKGROUND OBJECTIVES OBJECTIVES OBJECTIVES METHODS METHODS RESULTS CONCLUSIONS
D06922,This note explores the relation between the boxicity of undirected graphs and the Ferrers dimension of digraphs.,OBJECTIVES
D01038,"In this paper we present a method for automatically planning optimal paths for a group of robots that satisfy a common high level mission specification.$$$Each robot's motion in the environment is modeled as a weighted transition system.$$$The mission is given as a Linear Temporal Logic formula.$$$In addition, an optimizing proposition must repeatedly be satisfied.$$$The goal is to minimize the maximum time between satisfying instances of the optimizing proposition.$$$Our method is guaranteed to compute an optimal set of robot paths.$$$We utilize a timed automaton representation in order to capture the relative position of the robots in the environment.$$$We then obtain a bisimulation of this timed automaton as a finite transition system that captures the joint behavior of the robots and apply our earlier algorithm for the single robot case to optimize the group motion.$$$We present a simulation of a persistent monitoring task in a road network environment.",OBJECTIVES BACKGROUND BACKGROUND BACKGROUND OBJECTIVES RESULTS/CONCLUSIONS METHODS METHODS RESULTS
D03611,"In the framework of computational complexity and in an effort to define a more natural reduction for problems of equivalence, we investigate the recently introduced kernel reduction, a reduction that operates on each element of a pair independently.$$$This paper details the limitations and uses of kernel reductions.$$$We show that kernel reductions are weaker than many-one reductions and provide conditions under which complete problems exist.$$$Ultimately, the number and size of equivalence classes can dictate the existence of a kernel reduction.$$$We leave unsolved the unconditional existence of a complete problem under polynomial-time kernel reductions for the standard complexity classes.",OBJECTIVES OBJECTIVES RESULTS/CONCLUSIONS CONCLUSIONS OTHERS
D01011,"We report a new model of artificial life called Lenia (from Latin lenis ""smooth""), a two-dimensional cellular automaton with continuous space-time-state and generalized local rule.$$$Computer simulations show that Lenia supports a great diversity of complex autonomous patterns or ""lifeforms"" bearing resemblance to real-world microscopic organisms.$$$More than 400 species in 18 families have been identified, many discovered via interactive evolutionary computation.$$$They differ from other cellular automata patterns in being geometric, metameric, fuzzy, resilient, adaptive, and rule-generic.$$$We present basic observations of the model regarding the properties of space-time and basic settings.$$$We provide a board survey of the lifeforms, categorize them into a hierarchical taxonomy, and map their distribution in the parameter hyperspace.$$$We describe their morphological structures and behavioral dynamics, propose possible mechanisms of their self-propulsion, self-organization and plasticity.$$$Finally, we discuss how the study of Lenia would be related to biology, artificial life, and artificial intelligence.",OBJECTIVES RESULTS RESULTS RESULTS METHODS METHODS METHODS CONCLUSIONS
D05921,"Virtualization is generally adopted in server and desktop environments to provide for fault tolerance, resource management, and energy efficiency.$$$Virtualization enables parallel execution of multiple operating systems (OSs) while sharing the hardware resources.$$$Virtualization was previously not deemed as feasible technology for mobile and embedded devices due to their limited processing and memory resource.$$$However, the enterprises are advocating Bring Your Own Device (BYOD) applications that enable co-existence of heterogeneous OSs on a single mobile device.$$$Moreover, embedded device require virtualization for logical isolation of secure and general purpose OSs on a single device.$$$In this paper, we investigate the processor architectures in the mobile and embedded space while examining their formal visualizability.$$$We also compare the virtualization solutions enabling coexistence of multiple OSs in Multicore Processor System-on-Chip (MPSoC) mobile and embedded systems.$$$We advocate that virtualization is necessary to manage resource in MPSoC designs and to enable BYOD, security, and logical isolation use cases.",BACKGROUND BACKGROUND BACKGROUND BACKGROUND/OBJECTIVES OBJECTIVES METHODS METHODS RESULTS/CONCLUSIONS
D00277,"Numerous institutions and organizations need not only to preserve the material and publications they produce, but also have as their task (although it would be desirable it was an obligation) to publish, disseminate and make publicly available all the results of the research and any other scientific/academic material.$$$The Open Archives Initiative (OAI) and the introduction of Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH), make this task much easier.$$$The main objective of this work is to make a comparative and qualitative study of the data -metadata specifically- contained in the whole set of Argentine repositories listed in the ROAR portal, focusing on the functional perspective of the quality of this metadata.$$$Another objective is to offer an overview of the status of these repositories, in an attempt to detect common failures and errors institutions incur when storing the metadata of the resources contained in these repositories, and thus be able to suggest measures to be able to improve the load and further retrieval processes.$$$It was found that the eight most used Dublin Core fields are: identifier, type, title, date, subject, creator, language and description.$$$Not all repositories fill all the fields, and the lack of normalization, or the excessive use of fields like language, type, format and subject is somewhat striking, and in some cases even alarming",BACKGROUND BACKGROUND OBJECTIVES OBJECTIVES RESULTS/CONCLUSIONS RESULTS/CONCLUSIONS
